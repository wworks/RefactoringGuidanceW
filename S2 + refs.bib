% Encoding: UTF-8

@Article{Urma2015,
  author     = {Urma, Raoul-Gabriel and Mycroft, Alan},
  journal    = {Science of Computer Programming},
  title      = {Source-code queries with graph databases—with application to programming language usage and evolution},
  year       = {2015},
  issn       = {0167-6423},
  month      = jan,
  pages      = {127--134},
  volume     = {97},
  abstract   = {Program querying and analysis tools are of growing importance, and occur in two main variants. Firstly there are source-code query languages which help software engineers to explore a system, or to find code in need of refactoring as coding standards evolve. These also enable language designers to understand the practical uses of language features and idioms over a software corpus. Secondly there are program analysis tools in the style of Coverity which perform deeper program analysis searching for bugs as well as checking adherence to coding standards such as MISRA. The former class are typically implemented on top of relational or deductive databases and make ad-hoc trade-offs between scalability and the amount of source-code detail held—with consequent limitations on the expressiveness of queries. The latter class are more commercially driven and involve more ad-hoc queries over program representations, nonetheless similar pressures encourage user-visible domain-specific languages to specify analyses. We argue that a graph data model and associated query language provides a unifying conceptual model and gives efficient scalable implementation even when storing full source-code detail. It also supports overlays allowing a query DSL to pose queries at a mixture of syntax-tree, type, control-flow-graph or data-flow levels. We describe a prototype source-code query system built on top of Neo4j using its Cypher graph query language; experiments show it scales to multi-million-line programs while also storing full source-code detail.},
  doi        = {10.1016/j.scico.2013.11.010},
  file       = {:Urma2015 - Source Code Queries with Graph Databases—with Application to Programming Language Usage and Evolution (2).pdf:PDF},
  keywords   = {Programming language evolution, Source-code queries and DSLs, Graph databases, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Special {Issue} on {New} {Ideas} and {Emerging} {Results} in {Understanding} {Software}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0167642313002943},
  urldate    = {2021-06-04},
}

@InProceedings{Hajiyev2006,
  author     = {Hajiyev, Elnar and Verbaere, Mathieu and de Moor, Oege},
  booktitle  = {{ECOOP} 2006 – {Object}-{Oriented} {Programming}},
  title      = {{codeQuest}: {Scalable} {Source} {Code} {Queries} with {Datalog}},
  year       = {2006},
  address    = {Berlin, Heidelberg},
  editor     = {Thomas, Dave},
  pages      = {2--27},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Source code querying tools allow programmers to explore relations between different parts of the code base. This paper describes such a tool, named codeQuest. It combines two previous proposals, namely the use of logic programming and database systems.As the query language we use safe Datalog, which was originally introduced in the theory of databases. That provides just the right level of expressiveness; in particular recursion is indispensable for source code queries. Safe Datalog is like Prolog, but all queries are guaranteed to terminate, and there is no need for extra-logical annotations.Our implementation of Datalog maps queries to a relational database system. We are thus able to capitalise on the query optimiser provided by such a system. For recursive queries we implement our own optimisations in the translation from Datalog to SQL. Experiments confirm that this strategy yields an efficient, scalable code querying system.},
  doi        = {10.1007/11785477_2},
  file       = {:Hajiyev2006 - CodeQuest_ Scalable Source Code Queries with Datalog.pdf:PDF;:10.1007_11785477.pdf:PDF},
  isbn       = {9783540357278},
  keywords   = {Database System , Logic Programming , Query Language , Query Optimiser , Call Graph , skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {{codeQuest}},
}

@Article{10.1145/206263.206274,
  author     = {Colket, Currie},
  journal    = {Ada Lett.},
  title      = {Ada Semantic Interface Specification (ASIS): Frequently Asked Questions},
  year       = {1995},
  issn       = {1094-3641},
  month      = jul,
  number     = {4},
  pages      = {50–63},
  volume     = {XV},
  address    = {New York, NY, USA},
  doi        = {10.1145/206263.206274},
  file       = {:C\:/Users/Wernsen/Documents/School/Open Universiteit/Voorbereiden afstuderen/New/Af 10/RQ 1.2/files 2/206263.206274.pdf:PDF;:10.1145_206263.206274 - Ada Semantic Interface Specification (ASIS)_ Frequently Asked Questions.doc:Word},
  issue_date = {July/Aug. 1995},
  keywords   = {skimmed},
  numpages   = {14},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/206263.206274},
}

@InProceedings{10.1145/1512475.1512489,
  author     = {Shonle, Macneil and Griswold, William G. and Lerner, Sorin},
  booktitle  = {Proceedings of the 8th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering},
  title      = {Addressing Common Crosscutting Problems with Arcum},
  year       = {2008},
  address    = {New York, NY, USA},
  pages      = {64–69},
  publisher  = {Association for Computing Machinery},
  series     = {PASTE '08},
  abstract   = {Crosscutting is an inherent part of software development and can typically be managed through modularization: A module's stable properties are defined in an interface while its likely-to-change properties are encapsulated within the module [19]. The crosscutting of the stable properties, such as class and method names, can be mitigated with automated refactoring tools that allow, for example, the interface's elements to be renamed [9, 18]. However, often the crosscutting from design idioms (such as design patterns and coding styles) are so specific to the program's domain that their crosscutting would not likely have been anticipated by the developers of an automated refactoring system.The Arcum plug-in for Eclipse enables programmers to describe the implementation of a crosscutting design idiom as a set of syntactic patterns and semantic constraints. Arcum can process declarations of related implementations and infer the refactoring steps necessary to transform a program from using one implementation to its alternatives. As a result, automating refactoring for domain-specific crosscutting design idioms can be easy and practical. This paper presents a case study of how Arcum was used to mitigate four classic software engineering problems that are exacerbated by crosscutting: library migration, debugging, programmer-defined semantic checking, and architectural enforcement.},
  doi        = {10.1145/1512475.1512489},
  file       = {:C\:/Users/Wernsen/Documents/School/Open Universiteit/Voorbereiden afstuderen/New/Af 10/RQ 1.2/files 2/paste08.pdf:PDF;:10.1145_1512475.1512489 - Addressing Common Crosscutting Problems with Arcum.pdf:PDF},
  isbn       = {9781605583822},
  keywords   = {aspect-oriented programming, design patterns, refactoring, skimmed},
  location   = {Atlanta, Georgia},
  numpages   = {6},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1512475.1512489},
}

@InProceedings{10.1145/1368088.1368092,
  author     = {de Alwis, Brian and Murphy, Gail C.},
  booktitle  = {Proceedings of the 30th International Conference on Software Engineering},
  title      = {Answering Conceptual Queries with Ferret},
  year       = {2008},
  address    = {New York, NY, USA},
  pages      = {21–30},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '08},
  abstract   = {Programmers seek to answer questions as they investigate the functioning of a software system, such as "which execution path is being taken in this case?" Programmers attempt to answer these questions, which we call conceptual queries, using a variety of tools. Each type of tool typically highlights one kind of information about the system, such as static structural information or control-flow information. Unfortunately for the programmer, the tools seldom directly answer the programmer's conceptual queries. Instead, the programmer must piece together results from different tools to determine an answer to the initial query. At best, this process is time consuming and at worst, this process can lead to data overload and disorientation.In this paper, we present a model that supports the integration of different sources of information about a program. This model enables the results of concrete queries in separate tools to be brought together to directly answer many of a programmer's conceptual queries. In addition to presenting this model, we present a tool that implements the model, demonstrate the range of conceptual queries supported by this tool, and present the results of use of the conceptual queries in a small field study.},
  doi        = {10.1145/1368088.1368092},
  file       = {:C\:/Users/Wernsen/Documents/School/Open Universiteit/Voorbereiden afstuderen/New/Af 10/RQ 1.2/files 2/1368088.1368092.pdf:PDF;:10.1145_1368088.1368092 - Answering Conceptual Queries with Ferret.pdf:PDF},
  isbn       = {9781605580791},
  keywords   = {tool integration, software representation models, skimmed},
  location   = {Leipzig, Germany},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/1368088.1368092},
}

@InProceedings{Ujhelyi2014,
  author     = {Ujhelyi, Zoltán and Horváth, Ákos and Varró, Dániel and Csiszár, Norbert István and Szoke, Gábor and Vidács, László and Ferenc, Rudolf},
  booktitle  = {2014 {Software} {Evolution} {Week} - {IEEE} {Conference} on {Software} {Maintenance}, {Reengineering}, and {Reverse} {Engineering} ({CSMR}-{WCRE})},
  title      = {Anti-pattern detection with model queries: {A} comparison of approaches},
  year       = {2014},
  month      = feb,
  pages      = {293--302},
  abstract   = {Program queries play an important role in several software evolution tasks like program comprehension, impact analysis, or the automated identification of anti-patterns for complex refactoring operations. A central artifact of these tasks is the reverse engineered program model built up from the source code (usually an Semantic Graph, ASG), which is traditionally post-processed by dedicated, hand-coded queries. Our paper investigates the use of the popular industrial Eclipse Modeling Framework (EMF) as an underlying representation of program models processed by three general-purpose model query techniques based on native Java code, local-search and incremental evaluation. We provide in-depth comparison of these techniques on the source code of 17 Java projects using queries taken from refactoring operations in different usage profiles. Our results show that general purpose model queries outperform hand-coded queries by 2-3 orders of magnitude, while there is a 5-10 times increase in memory consumption and model load time. In addition, measurement results of usage profiles can be used as guidelines for selecting the appropriate query technologies in concrete scenarios.},
  doi        = {10.1109/CSMR-WCRE.2014.6747181},
  file       = {:C\:/Users/Wernsen/Documents/School/Open Universiteit/Voorbereiden afstuderen/New/Af 10/RQ 1.2/files 2/2498771(1).pdf:PDF;:Ujhelyi2014 - Anti Pattern Detection with Model Queries_ a Comparison of Approaches.pdf:PDF},
  keywords   = {Java, Pattern matching, Load modeling, Abstracts, Semantics, Analytical models, Search problems, skimmed},
  readstatus = {skimmed},
  shorttitle = {Anti-pattern detection with model queries},
}

@InProceedings{10.1145/1529282.1529384,
  author     = {Lazzarini Lemos, Ot\'{a}vio Augusto and Bajracharya, Sushil and Ossher, Joel and Masiero, Paulo Cesar and Lopes, Cristina},
  booktitle  = {Proceedings of the 2009 ACM Symposium on Applied Computing},
  title      = {Applying Test-Driven Code Search to the Reuse of Auxiliary Functionality},
  year       = {2009},
  address    = {New York, NY, USA},
  pages      = {476–482},
  publisher  = {Association for Computing Machinery},
  series     = {SAC '09},
  abstract   = {Software developers spend considerable effort implementing auxiliary functionality used by the main features of a system (e.g. compressing/decompressing files, encryption/decription of data, scaling/rotating images). With the increasing amount of open source code available on the Internet, time and effort can be saved by reusing these utilities through informal practices of code search and reuse. However, when this type of reuse is performed in an ad hoc manner, it can be tedious and error-prone: code results have to be manually inspected and extracted into the workspace. In this paper we introduce the use of test cases as an interface for automating code search and reuse and evaluate its applicability and performance in the reuse of auxiliary functionality. We call our approach Test-Driven Code Search (TDCS). Test cases serve two purposes: (1) they define the behavior of the desired functionality to be searched; and (2) they test the matching results for suitability in the local context. We present CodeGenie, an Eclipse plugin that performs TDCS using a code search engine called Sourcerer. Our evaluation presents evidence of the applicability and good performance of TDCS in the reuse of auxiliary functionality.},
  doi        = {10.1145/1529282.1529384},
  file       = {:1529282.1529384.pdf:PDF},
  isbn       = {9781605581668},
  keywords   = {TDD, source code search, software reuse, test-first, skimmed},
  location   = {Honolulu, Hawaii},
  numpages   = {7},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/1529282.1529384},
}

@Article{Ali2018,
  author     = {Ali, Nour and Baker, Sean and O’Crowley, Ross and Herold, Sebastian and Buckley, Jim},
  journal    = {Empirical Software Engineering},
  title      = {Architecture consistency: {State} of the practice, challenges and requirements},
  year       = {2018},
  issn       = {1573-7616},
  month      = feb,
  number     = {1},
  pages      = {224--258},
  volume     = {23},
  abstract   = {Architecture Consistency (AC) aims to align implemented systems with their intended architectures. Several AC approaches and tools have been proposed and empirically evaluated, suggesting favourable results. In this paper, we empirically examine the state of practice with respect to Architecture Consistency, through interviews with nineteen experienced software engineers. Our goal is to identify 1) any practises that the companies these architects work for, currently undertake to achieve AC; 2) any barriers to undertaking explicit AC approaches in these companies; 3) software development situations where practitioners perceive AC approaches would be useful, and 4) AC tool needs, as perceived by practitioners. We also assess current commercial AC tool offerings in terms of these perceived needs. The study reveals that many practitioners apply informal AC approaches as there are barriers for adopting more formal and explicit approaches. These barriers are: 1) Difficulty in quantifying architectural inconsistency effects, and thus justifying the allocation of resources to fix them to senior management, 2) The near invisibility of architectural inconsistency to customers, 3) Practitioners’ reluctance towards fixing architectural inconsistencies, and 4) Practitioners perception that huge effort is required to map the system to the architecture when using more formal AC approaches and tools. Practitioners still believe that AC would be useful in supporting several of the software development activities such as auditing, evolution and ensuring quality attributes. After reviewing several commercial tools, we posit that AC tool vendors need to work on their ability to support analysis of systems made up of different technologies, that AC tools need to enhance their capabilities with respect to artefacts such as services and meta-data, and to focus more on non-maintainability architectural concerns.},
  doi        = {10.1007/s10664-017-9515-3},
  file       = {:Ali2018 - Architecture Consistency_ State of the Practice, Challenges and Requirements.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {Architecture consistency},
  url        = {https://doi.org/10.1007/s10664-017-9515-3},
  urldate    = {2021-06-04},
}

@Article{10.1145/236338.236380,
  author     = {Sefika, Mohlalefi and Sane, Aamod and Campbell, Roy H.},
  journal    = {SIGPLAN Not.},
  title      = {Architecture-Oriented Visualization},
  year       = {1996},
  issn       = {0362-1340},
  month      = oct,
  number     = {10},
  pages      = {389–405},
  volume     = {31},
  abstract   = {Tracking the changing dynamics of object-oriented frameworks[5], design patterns[7], architectural styles[8], and subsystems during the development and reuse cycle can aid producing complex systems. Unfortunately, current object-oriented programming tools are relatively oblivious to the rich architectural abstractions in a system.This paper shows that architecture-oriented visualization, the graphical presentation of system statics and dynamics in terms of its architectural abstractions, is highly beneficial in designing complex systems. In addition, the paper presents architecture-aware instrumentation, a new technique for building efficient on-line instrumentation to support architectural queries. We demonstrate the effectiveness and performance of the scheme with case studies in the design of the Choices object-oriented operating system.},
  address    = {New York, NY, USA},
  doi        = {10.1145/236338.236380},
  file       = {:236338.236380.pdf:PDF},
  issue_date = {Oct. 1996},
  keywords   = {skimmed},
  numpages   = {17},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/236338.236380},
}

@Article{Fabry2016,
  author     = {Fabry, Johan and De Roover, Coen and Noguera, Carlos and Zschaler, Steffen and Rashid, Awais and Jonckers, Viviane},
  journal    = {Journal of Systems and Software},
  title      = {{AspectJ} code analysis and verification with {GASR}},
  year       = {2016},
  issn       = {0164-1212},
  month      = jul,
  pages      = {528--544},
  volume     = {117},
  abstract   = {Aspect-oriented programming languages extend existing languages with new features for supporting modularization of crosscutting concerns. These features however make existing source code analysis tools unable to reason over this code. Consequently, all code analysis efforts of aspect-oriented code that we are aware of have either built limited analysis tools or were performed manually. Given the significant complexity of building them or manual analysis, a lot of duplication of effort could have been avoided by using a general-purpose tool. To address this, in this paper we present Gasr: a source code analysis tool that reasons over AspectJ source code, which may contain metadata in the form of annotations. GASR provides multiple kinds of analyses that are general enough such that they are reusable, tailorable and can reason over annotations. We demonstrate the use of GASR in two ways: we first automate the recognition of previously identified aspectual source code assumptions. Second, we turn implicit assumptions into explicit assumptions through annotations and automate their verification. In both uses GASR performs detection and verification of aspect assumptions on two well-known case studies that were manually investigated in earlier work. GASR finds already known aspect assumptions and adds instances that had been previously overlooked.},
  doi        = {10.1016/j.jss.2016.04.014},
  file       = {:GASR16.pdf:PDF},
  keywords   = {Aspect oriented programming, Source code analysis, Logic program querying, skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://www.sciencedirect.com/science/article/pii/S0164121216300279},
  urldate    = {2021-06-04},
}

@InProceedings{Fabry2013,
  author     = {Fabry, Johan and De Roover, Coen and Jonckers, Viviane},
  booktitle  = {2013 {IEEE} 13th {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation} ({SCAM})},
  title      = {Aspectual source code analysis with {GASR}},
  year       = {2013},
  month      = sep,
  pages      = {53--62},
  abstract   = {To be able to modularize crosscutting concerns, aspects introduce new programming language features, often in a new language, with a specific syntax. These new features lead to new needs for source code analysis tools, resulting in the requirement for a general-purpose aspectual source code analysis tool. Ignoring this requirement has led to a nontrivial duplication of effort in the aspect-oriented software development community. This is because all code analysis efforts that we are aware of have either built ad-hoc analysis tools or were performed manually. In this paper we present Gasr: a source code analysis tool in the tradition of logic program querying that reasons over AspectJ source code. By hooking into the IDE plugins for AspectJ, Gasr provides a library of predicates that can be used to query aspectual source code. We demonstrate the use of Gasr by automating the recognition of a number of previously identified aspectual source code assumptions. We then detect assumption instances on two well-known case studies that were manually investigated in the earlier work. In addition to finding the already known aspect assumptions, Gasr encounters assumption instances that were overlooked before.},
  doi        = {10.1109/SCAM.2013.6648184},
  file       = {:vub-soft-tr-13-06.pdf:PDF},
  keywords   = {Weaving, Cognition, Software, Libraries, Abstracts, Conferences, Visualization, Aspect Oriented Programming, Logic Program Querying, Aspectual Assumptions, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1321631.1321664,
  author     = {Antkiewicz, Michal and Bartolomei, Thiago Tonelli and Czarnecki, Krzysztof},
  booktitle  = {Proceedings of the Twenty-Second IEEE/ACM International Conference on Automated Software Engineering},
  title      = {Automatic Extraction of Framework-Specific Models from Framework-Based Application Code},
  year       = {2007},
  address    = {New York, NY, USA},
  pages      = {214–223},
  publisher  = {Association for Computing Machinery},
  series     = {ASE '07},
  abstract   = {Framework-specific models represent the design of pplicationcode from the framework viewpoint by showing how framework-provided concepts are implemented in the code. In this paper, we describe an experimental study of the static analyses necessary to automatically retrieve such models from application code. We reverse engineer a number of applications based on three open-source frameworks and evaluate the quality of the retrieved models. The models are expressed using framework-specific modeling languages(FSMLs), each designed for one of the open-source frameworks. For reverse engineering, we use prototype implementations of the three FSMLs. Our results show that for the considered frameworks rather simple code analysesare sufficient for automatically retrieving framework-specific models form a large body of application code with high precision and recall},
  doi        = {10.1145/1321631.1321664},
  file       = {:1321631.1321664.pdf:PDF},
  isbn       = {9781595938824},
  keywords   = {static analysis, framework-specific models, framework-specific modeling languages, reverse engineering, object-oriented frameworks, skimmed},
  location   = {Atlanta, Georgia, USA},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1321631.1321664},
}

@InProceedings{10.1145/2393596.2393604,
  author     = {Augustine, Vinay},
  booktitle  = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
  title      = {Automating Adaptive Maintenance Changes with SrcML and LINQ},
  year       = {2012},
  address    = {New York, NY, USA},
  publisher  = {Association for Computing Machinery},
  series     = {FSE '12},
  abstract   = {Automated program transformation can significantly improve the speed and accuracy of adaptive maintenance tasks. However, developers typically eschew program transformation because of the difficulty in creating, using, and maintaining transformations. They may also be unwilling to learn a new technique that will be used infrequently. We present an approach that leverages programmers' existing knowledge of C# and LINQ to drastically reduce the learning curve associated with program transformation.},
  articleno  = {9},
  doi        = {10.1145/2393596.2393604},
  file       = {:10.1145_2393596.2393604 - Automating Adaptive Maintenance Changes with SrcML and LINQ.pdf:PDF},
  isbn       = {9781450316149},
  keywords   = {program transformation, skimmed},
  location   = {Cary, North Carolina},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/2393596.2393604},
}

@InProceedings{10.1145/1244381.1244398,
  author     = {De Roover, Coen and D'Hondt, Theo and Brichau, Johan and Noguera, Carlos and Duchien, Laurence},
  booktitle  = {Proceedings of the 2007 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation},
  title      = {Behavioral Similarity Matching Using Concrete Source Code Templates in Logic Queries},
  year       = {2007},
  address    = {New York, NY, USA},
  pages      = {92–101},
  publisher  = {Association for Computing Machinery},
  series     = {PEPM '07},
  abstract   = {Program query languages and pattern-detection techniques are an essential part of program analysis and manipulation systems. Queries and patterns permit the identification of the parts of interest in a program's implementation through a representation dedicated to the intent of the system (e.g. call-graphs to detect behavioral flaws, abstract syntax trees for transformations, concrete source code to verify programming conventions, etc). This requires that developers understand and manage all the different representations and techniques in order to detect various patterns of interest. To alleviate this overhead, we present a logic-based language that allows the program's implementation to be queried using concrete source code templates. The queries are matched against a combination of structural and behavioral program representations, including call-graphs, points-to analysis results and abstract syntax trees. The result of our approach is that developers can detect patterns in the queried program using source code excerpts (embedded in logic queries) which act as prototypical samples of the structure and behavior they intend to match.},
  doi        = {10.1145/1244381.1244398},
  file       = {:C\:/Users/Wernsen/Downloads/10.1145_1244381.1244398 - Behavioral Similarity Matching Using Concrete Source Code Templates in Logic Queries.pdf:PDF},
  isbn       = {9781595936202},
  keywords   = {program validation, logic meta programming, source code templates, pattern detection, program analysis, program querying, skimmed},
  location   = {Nice, France},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1244381.1244398},
}

@Article{10.1145/2803171,
  author     = {Dyer, Robert and Nguyen, Hoan Anh and Rajan, Hridesh and Nguyen, Tien N.},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  title      = {Boa: Ultra-Large-Scale Software Repository and Source-Code Mining},
  year       = {2015},
  issn       = {1049-331X},
  month      = dec,
  number     = {1},
  volume     = {25},
  abstract   = {In today's software-centric world, ultra-large-scale software repositories, such as SourceForge, GitHub, and Google Code, are the new library of Alexandria. They contain an enormous corpus of software and related information. Scientists and engineers alike are interested in analyzing this wealth of information. However, systematic extraction and analysis of relevant data from these repositories for testing hypotheses is hard, and best left for mining software repository (MSR) experts! Specifically, mining source code yields significant insights into software development artifacts and processes. Unfortunately, mining source code at a large scale remains a difficult task. Previous approaches had to either limit the scope of the projects studied, limit the scope of the mining task to be more coarse grained, or sacrifice studying the history of the code. In this article we address mining source code: (a) at a very large scale; (b) at a fine-grained level of detail; and (c) with full history information. To address these challenges, we present domain-specific language features for source-code mining in our language and infrastructure called Boa. The goal of Boa is to ease testing MSR-related hypotheses. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also show drastic improvements in scalability.},
  address    = {New York, NY, USA},
  articleno  = {7},
  doi        = {10.1145/2803171},
  file       = {:10.1145_2803171 - Boa_ Ultra Large Scale Software Repository and Source Code Mining.pdf:PDF},
  issue_date = {December 2015},
  keywords   = {lower barrier to entry, domain-specific language, mining software repositories, ease of use, Boa, scalable, skimmed},
  numpages   = {34},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2803171},
}

@InProceedings{10.1145/581339.581381,
  author     = {Michail, Amir},
  booktitle  = {Proceedings of the 24th International Conference on Software Engineering},
  title      = {Browsing and Searching Source Code of Applications Written Using a GUI Framework},
  year       = {2002},
  address    = {New York, NY, USA},
  pages      = {327–337},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '02},
  abstract   = {Nowadays, applications are typically written using an object-oriented GUI framework. In this paper we explore the possibility of using the GUI of such applications to guide browsing and search of their source code. Such a tool would be helpful for software maintenance and reuse, particularly when the application source is unfamiliar. Intuitively, we would expect the task of browsing and searching source code of an application written using a GUI framework to be easier than one that doesn't because the GUI framework imposes a structure on the application. Generally, the GUI framework is in control and makes calls into the application code to handle various events --- thus providing fundamental entry points into the application code, namely the callbacks. Of course, this is a property of frameworks in general but GUI frameworks have one additional advantage: the GUI is visible to the end-user and contains text messages describing what the application can do. Thus we have an explicit connection between an informal specification fragment visible in the GUI and its precise entry point to the implementation in the source. We demonstrate our approach, which takes advantage of this connection, on KDE applications written using the KDE GUI framework.},
  doi        = {10.1145/581339.581381},
  file       = {:10.1145_581339.581381 - Browsing and Searching Source Code of Applications Written Using a GUI Framework.pdf:PDF},
  isbn       = {158113472X},
  keywords   = {skimmed},
  location   = {Orlando, Florida},
  numpages   = {11},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/581339.581381},
}

@InProceedings{DeRoover2014,
  author     = {De Roover, Coen and Stevens, Reinout},
  booktitle  = {2014 {Software} {Evolution} {Week} - {IEEE} {Conference} on {Software} {Maintenance}, {Reengineering}, and {Reverse} {Engineering} ({CSMR}-{WCRE})},
  title      = {Building development tools interactively using the {EKEKO} meta-programming library},
  year       = {2014},
  month      = feb,
  pages      = {429--433},
  abstract   = {EKEKO is a Clojure library for applicative logic meta-programming against an Eclipse workspace. EKEKO has been applied successfully to answering program queries (e.g., “does this bug pattern occur in my code?”), to analyzing project corpora (e.g., “how often does this API usage pattern occur in this corpus?”), and to transforming programs (e.g., “change occurrences of this pattern as follows”) in a declarative manner. These applications rely on a seamless embedding of logic queries in applicative expressions. While the former identify source code of interest, the latter associate error markers with, compute statistics about, or rewrite the identified source code snippets. In this paper, we detail the logic and applicative aspects of the EKEKO library. We also highlight key choices in their implementation. In particular, we demonstrate how a causal connection with the Eclipse infrastructure enables building development tools interactively on the Clojure read-eval-print loop.},
  doi        = {10.1109/CSMR-WCRE.2014.6747211},
  file       = {:DeRoover2014 - Building Development Tools Interactively Using the EKEKO Meta Programming Library.pdf:PDF},
  keywords   = {Libraries, Java, Computational modeling, Buildings, Visualization, Logic programming, Vectors, skimmed},
  readstatus = {skimmed},
}

@Article{Chen1990,
  author     = {Chen, Y.-F. and Nishimoto, M.Y. and Ramamoorthy, C.V.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {The {C} information abstraction system},
  year       = {1990},
  issn       = {1939-3520},
  month      = mar,
  number     = {3},
  pages      = {325--334},
  volume     = {16},
  abstract   = {A system for analyzing program structures is described. The system extracts relational information from C programs according to a conceptual model and stores the information in a database. It is shown how several interesting software tasks can be performed by using the relational views. These tasks include generation of graphical views, subsystem extraction, program layering, dead code elimination and binding analysis.{\textless}{\textgreater}},
  doi        = {10.1109/32.48940},
  file       = {:Chen1990 - The C Information Abstraction System.pdf:PDF},
  keywords   = {Relational databases, Computer science, Software reusability, Software maintenance, Data mining, Software metrics, Computer languages, Software tools, Production, Information analysis, skimmed},
  readstatus = {skimmed},
}

@Article{Bispo2020,
  author     = {Bispo, João and Cardoso, João M. P.},
  journal    = {SoftwareX},
  title      = {Clava: {C}/{C}++ source-to-source compilation using {LARA}},
  year       = {2020},
  issn       = {2352-7110},
  month      = jul,
  pages      = {100565},
  volume     = {12},
  abstract   = {This article presents Clava, a Clang-based source-to-source compiler, that accepts scripts written in LARA, a JavaScript-based DSL with special constructs for code queries, analysis and transformations. Clava improves Clang’s source-to-source capabilities by providing a more convenient and flexible way to analyze, transform and generate C/C++ code, and provides support for building strategies that capture run-time behavior. We present the Clava framework, its main capabilities, and how it can been used. Furthermore, we show that Clava is sufficiently robust to analyze, instrument and test a set of large C/C++ application codes, such as GCC.},
  doi        = {10.1016/j.softx.2020.100565},
  file       = {:Bispo2020 - Clava_ C_C++ Source to Source Compilation Using LARA.pdf:PDF},
  keywords   = {Source-to-source, C/C++, LARA, Compilers, skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {Clava},
  url        = {https://www.sciencedirect.com/science/article/pii/S2352711019302122},
  urldate    = {2021-06-08},
}

@Article{Maekelae2009,
  author     = {Mäkelä, Sami and Leppänen, Ville},
  journal    = {Science of Computer Programming},
  title      = {Client-based cohesion metrics for {Java} programs},
  year       = {2009},
  issn       = {0167-6423},
  month      = mar,
  number     = {5},
  pages      = {355--378},
  volume     = {74},
  abstract   = {One purpose of software metrics is to measure the quality of programs. The results can be for example used to predict maintenance costs or improve code quality. An emerging view is that if software metrics are going to be used to improve quality, they must help in finding code that should be refactored. Often refactoring or applying a design pattern is related to the role of the class to be refactored. In client-based metrics, a project gives the class a context. These metrics measure how a class is used by other classes in the context. We present a new client-based metric LCIC (Lack of Coherence in Clients), which analyses if the class being measured has a coherent set of roles in the program. Interfaces represent the roles of classes. If a class does not have a coherent set of roles, it should be refactored, or a new interface should be defined for the class. We have implemented a tool for measuring the metric LCIC for Java projects in the Eclipse environment. We calculated LCIC values for classes of several open source projects. We compare these results with results of other related metrics, and inspect the measured classes to find out what kind of refactorings are needed. We also analyse the relation of different design patterns and refactorings to our metric. Our experiments reveal the usefulness of client-based metrics to improve the quality of code.},
  doi        = {10.1016/j.scico.2009.01.005},
  file       = {:Maekelae2009 - Client Based Cohesion Metrics for Java Programs.pdf:PDF},
  keywords   = {Metrics, Cohesion, Refactoring, Design patterns, Java, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Special {Issue} on {Principles} and {Practices} of {Programming} in {Java} ({PPPJ} 2007)},
  url        = {https://www.sciencedirect.com/science/article/pii/S0167642309000173},
  urldate    = {2021-06-08},
}

@Article{Holzmann2017,
  author     = {Holzmann, Gerard J.},
  journal    = {Innovations in Systems and Software Engineering},
  title      = {Cobra: a light-weight tool for static and dynamic program analysis},
  year       = {2017},
  issn       = {1614-5054},
  month      = mar,
  number     = {1},
  pages      = {35--49},
  volume     = {13},
  abstract   = {Static source code analysis tools have become indispensable for the development of reliable software applications. The best analyzers can reveal subtle flaws in a code base, but they can also be slow. In part this is due to the collection of detailed information about the possible data and control flow of an application to support the broadest possible range of analyses. For larger code bases it is not unusual that even the best of breed static analyzers can take an hour or more to complete an analysis. In this paper we describe a framework for a much faster, but more light-weight type of static analysis that can support interactive use for standard types of queries. The Cobra tool we designed for this purpose can scale to explore millions of lines of code interactively. The tool is mostly language agnostic, and can therefore easily be configured to resolve even dynamic program analysis queries.},
  doi        = {10.1007/s11334-016-0282-x},
  file       = {:Holzmann2017 - Cobra_ a Light Weight Tool for Static and Dynamic Program Analysis (1).pdf:PDF;:cobra_paper.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {Cobra},
  url        = {https://doi.org/10.1007/s11334-016-0282-x},
  urldate    = {2021-06-08},
}

@InProceedings{Atzeni2017,
  author     = {Atzeni, Mattia and Atzori, Maurizio},
  booktitle  = {The {Semantic} {Web} – {ISWC} 2017},
  title      = {{CodeOntology}: {RDF}-ization of {Source} {Code}},
  year       = {2017},
  address    = {Cham},
  editor     = {d'Amato, Claudia and Fernandez, Miriam and Tamma, Valentina and Lecue, Freddy and Cudré-Mauroux, Philippe and Sequeda, Juan and Lange, Christoph and Heflin, Jeff},
  pages      = {20--28},
  publisher  = {Springer International Publishing},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {In this paper, we leverage advances in the Semantic Web area, including data modeling (RDF), data management and querying (JENA and SPARQL), to develop CodeOntology, a community-shared software framework supporting expressive queries over source code. The project consists of two main contributions: an ontology that provides a formal representation of object-oriented programming languages, and a parser that is able to analyze Java source code and serialize it into RDF triples. The parser has been successfully applied to the source code of OpenJDK 8, gathering a structured dataset consisting of more than 2 million RDF triples. CodeOntology allows to generate Linked Data from any Java project, thereby enabling the execution of highly expressive queries over source code, by means of a powerful language like SPARQL.},
  doi        = {10.1007/978-3-319-68204-4_2},
  file       = {:Atzeni2017 - CodeOntology_ RDF Ization of Source Code (1).pdf:PDF;:271.pdf:PDF},
  isbn       = {9783319682044},
  keywords   = {Ontology , SPARQL , RDF , OWL , Programming languages , skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {{CodeOntology}},
}

@InProceedings{10.1145/1094855.1094884,
  author     = {Hajiyev, Elnar and Verbaere, Mathieu and de Moor, Oege and de Volder, Kris},
  booktitle  = {Companion to the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  title      = {CodeQuest: Querying Source Code with Datalog},
  year       = {2005},
  address    = {New York, NY, USA},
  pages      = {102–103},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '05},
  abstract   = {We describe CodeQuest, a system for querying source code. It combines two previous proposals, namely the use of logic programming and database system. Experiments (on projects ranging from 3KSLOC to 1300KSLOC) confirm that for this application, a query language based on DataLog strikes the right balance between expressiveness and scalability.},
  doi        = {10.1145/1094855.1094884},
  file       = {:10.1145_1094855.1094884 - CodeQuest_ Querying Source Code with Datalog.pdf:PDF},
  isbn       = {1595931937},
  keywords   = {relational databases, DataLog, source code querying, analysis of object-oriented programs, skimmed},
  location   = {San Diego, CA, USA},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1094855.1094884},
}

@InProceedings{Kellens2009,
  author     = {Kellens, Andy and Schutter, Kris De and D'Hondt, Theo and Jorissen, Luc and Passel, Bart Van},
  booktitle  = {2009 13th {European} {Conference} on {Software} {Maintenance} and {Reengineering}},
  title      = {Cognac: {A} {Framework} for {Documenting} and {Verifying} the {Design} of {Cobol} {Systems}},
  year       = {2009},
  month      = mar,
  note       = {ISSN: 1534-5351},
  pages      = {199--208},
  abstract   = {For any non-trivial software project, architectural drift is a well-known problem. Over time, the design rules and guidelines governing the software project are no longer obeyed, resulting in that the software becomes more difficult to maintain. While there exist numerous tools - such as code checkers, architecture and design checkers, and source code query languages - that aid in alleviating this problem none of these approaches are tailored towards supporting one of the main languages still in use today in industry, namely Cobol. In this paper we present Cognac, an extension of the Intensive tool that allows for documenting and verifying design rules in Cobol systems. Next to discussing the architecture of Cognac, we present a validation of our tool on an industrial, large-scale Cobol system.},
  doi        = {10.1109/CSMR.2009.9},
  file       = {:Kellens2009 - Cognac_ a Framework for Documenting and Verifying the Design of Cobol Systems.pdf:PDF},
  issn       = {1534-5351},
  keywords   = {Software maintenance, Gettering, Guidelines, Computer architecture, Large-scale systems, Java, Database languages, System testing, Object oriented modeling, Application software, software evolution, legacy systems, program querying, skimmed},
  readstatus = {skimmed},
  shorttitle = {Cognac},
}

@InProceedings{10.1145/3183440.3183476,
  author     = {Holland, Benjamin and Awadhutkar, Payas and Kothari, Suresh and Tamrawi, Ahmed and Mathews, Jon},
  booktitle  = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
  title      = {COMB: Computing Relevant Program Behaviors},
  year       = {2018},
  address    = {New York, NY, USA},
  pages      = {109–112},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '18},
  abstract   = {The paper presents COMB, a tool to improve accuracy and efficiency of software engineering tasks that hinge on computing all relevant program behaviors. Computing all behaviors and selecting the relevant ones is computationally intractable. COMB uses Projected Control Graph (PCG) abstraction to derive the relevant behaviors directly and efficiently. The PCG is important as the number of behaviors relevant to a task is often significantly smaller than the totality of behaviors.COMB provides extensive capabilities for program comprehension, analysis, and verification. We present a basic case study and a Linux verification study to demonstrate various capabilities of COMB and the addressed challenges. COMB is designed to support multiple programming languages. We demonstrate it for C and Java. Video url: https://youtu.be/YoOJ7avBIdk},
  doi        = {10.1145/3183440.3183476},
  file       = {:10.1145_3183440.3183476 - COMB_ Computing Relevant Program Behaviors.pdf:PDF},
  isbn       = {9781450356633},
  keywords   = {program behaviors, software analysis, software verification, skimmed},
  location   = {Gothenburg, Sweden},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/3183440.3183476},
}

@InProceedings{Alves2011,
  author     = {Alves, Tiago L. and Hage, Jurriaan and Rademaker, Peter},
  booktitle  = {2011 {IEEE} 11th {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation}},
  title      = {A {Comparative} {Study} of {Code} {Query} {Technologies}},
  year       = {2011},
  month      = sep,
  pages      = {145--154},
  abstract   = {When analyzing software systems we face the challenge of how to implement a particular analysis for different programming languages. A solution for this problem is to write a single analysis using a code query language, abstracting from the specificities of languages being analyzed. Over the past ten years many code query technologies have been developed, based on different formalisms. Each technology comes with its own query language and set of features. To determine the state of the art of code querying we compare the languages and tools for seven code query technologies: Grok, Rscript, JRelCal, Semmle Code, JGraLab, CrocoPat and JTransformer. The specification of a package stability metric is used as a running example to compare the languages. The comparison involves twelve criteria, some of which are concerned with properties of the query language (paradigm, types, parametrization, polymorphism, modularity, and libraries), and some of which are concerned with the tool itself (output formats, interactive interface, API support, interchange formats, extraction support, and licensing). We contextualize the criteria in two usage scenarios: interactive and tool integration. We conclude that there is no particularly weak or dominant tool. As important improvement points, we identify the lack of library mechanisms, interchange formats, and possibilities for integration with source code extractors.},
  doi        = {10.1109/SCAM.2011.14},
  file       = {:Alves2011 - A Comparative Study of Code Query Technologies.pdf:PDF},
  keywords   = {Database languages, Measurement, Software, Licenses, Educational institutions, Java, Code query, software analysis, comparative study, Grok, Rscript, JRelCal, SemmleCode, JGraLab, CrocoPat, JTransformer, read},
  readstatus = {read},
}

@InProceedings{10.1145/1297846.1297905,
  author     = {Avgustinov, Pavel and Tibble, Julian and de Moor, Oege},
  booktitle  = {Companion to the 22nd ACM SIGPLAN Conference on Object-Oriented Programming Systems and Applications Companion},
  title      = {A Comparison of Compilation Techniques for Trace Monitors with Free Variables},
  year       = {2007},
  address    = {New York, NY, USA},
  pages      = {821–822},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '07},
  abstract   = {A variety of different designs and optimisation strategies for trace monitoring have been proposed recently. Here, we examine trade-offs in simplicity of implementation and expressiveness of supported patterns, briefly discuss the underlying data structures of two mainstream implementations, and provide a short evaluation of the effectiveness ofmemory optimisations.},
  doi        = {10.1145/1297846.1297905},
  file       = {:10.1145_1297846.1297905 - A Comparison of Compilation Techniques for Trace Monitors with Free Variables.pdf:PDF},
  isbn       = {9781595938657},
  keywords   = {aspect-oriented programming, runtime verification, program monitoring, program analysis, read},
  location   = {Montreal, Quebec, Canada},
  numpages   = {2},
  readstatus = {read},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1297846.1297905},
}

@InProceedings{10.1145/1275672.1275678,
  author     = {Kniesel, G\"{u}nter and Hannemann, Jan and Rho, Tobias},
  booktitle  = {Proceedings of the 3rd Workshop on Linking Aspect Technology and Evolution},
  title      = {A Comparison of Logic-Based Infrastructures for Concern Detection and Extraction},
  year       = {2007},
  address    = {New York, NY, USA},
  pages      = {6–es},
  publisher  = {Association for Computing Machinery},
  series     = {LATE '07},
  abstract   = {In this paper we evaluate logic code analysis and transformation frameworks for their suitability as basic infrastructures for fast detection and extraction of (crosscutting) concerns. Using design patterns as example concerns, we identify desirable properties that an infrastructure should fulfill. We then report our initial results of evaluating candidate systems with respect to these properties. We show how high precision design pattern detectors can be easily formulated as predicates that are evaluated in mere seconds even on the sources of large software systems, such as the Eclipse IDE. Although details still remain to be analyzed further, our current results suggest that the pair JTransformer &amp; CTC is a good candidate for a general infrastructure, combining very good querying performance, scalability and short turn-around times with a seamless integration of querying and transformation capabilities.},
  doi        = {10.1145/1275672.1275678},
  file       = {:10.1145_1275672.1275678 - A Comparison of Logic Based Infrastructures for Concern Detection and Extraction.pdf:PDF},
  isbn       = {9781595936554},
  keywords   = {JQuery, code-quest, JTransformer, CTC, logic code analysis and transformation, design pattern detection, concern mining, read},
  location   = {Vancouver, British Columbia, Canada},
  readstatus = {read},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1275672.1275678},
}

@InProceedings{10.1145/1117696.1117709,
  author     = {Pfeiffer, J.-Hendrik and Sardos, Andonis and Gurd, John R.},
  booktitle  = {Proceedings of the 2005 OOPSLA Workshop on Eclipse Technology EXchange},
  title      = {Complex Code Querying and Navigation for AspectJ},
  year       = {2005},
  address    = {New York, NY, USA},
  pages      = {60–64},
  publisher  = {Association for Computing Machinery},
  series     = {eclipse '05},
  abstract   = {The ever growing size and complexity of software projects demand good IDE support in order to assist the understanding and navigation of source code during implementation and maintenance. In the case of Aspect-oriented programming, additional supporting IDE tools are needed to make aspect-oriented structures explicit. However, existing tools struggle to provide easy-to-use navigation facilities when the size of the source code increases.This paper describes Lost a query and navigation tool for the AspectJ language, its integration with the eclipse IDE, and initial experiences with using the tool. The described tool not only provides features which are novel with respect to current aspect-oriented programming tools but also attempts to overcome deficiencies of existing code querying tools.Additionally, we briefly discuss the implementation of a framework for code querying tools, which was created in order to maintain high flexibility in implementing the code querying tool presented here.},
  doi        = {10.1145/1117696.1117709},
  file       = {:10.1145_1117696.1117709 - Complex Code Querying and Navigation for AspectJ.pdf:PDF},
  isbn       = {1595933425},
  keywords   = {skimmed},
  location   = {San Diego, California},
  numpages   = {5},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1117696.1117709},
}

@InProceedings{Hayes2000,
  author     = {James Hayes and William G. Griswold and Stuart Moskovicsy},
  title      = {Component Design of Retargetable Program Analysis Tools that Reuse Intermediate Representations},
  year       = {2000},
  abstract   = {grams written in varied source languages, since systems that
Interactive program analysis tools are often tailored to one benefit the most from such analysis, especially legacy pro-
particular representation of programs, making adaptation to grams, are often written in multiple or proprietary program-
a new language costly. One way to ease adaptability is to in- ming languages. However, program analysis tools are often
troduce an intermediate abstraction—an adaptation layer— tailored to one particular representation of programs, mak-
between an existing language representation and the pro- ing adaptation to a new language costly (Figure 1a). An in-
gram analysis tool. This adaptation layer translates the tool’s expensive way of achieving adaptability is to introduce an
queries into queries on the particular representation. intermediate abstraction—an adapter component or adapta-},
  file       = {:C\:/Users/Wernsen/Downloads/337180.337221.pdf:PDF},
  keywords   = {language representation interface, because it would impose Retargetability, reuse, software design, program analysis, minimal responsibilities and inconvenience on any represen- software tools. tation (Figure 1b). Only a small number of operations should, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1137983.1138004,
  author     = {Hammouda, Imed and Koskimies, Kai},
  booktitle  = {Proceedings of the 2006 International Workshop on Mining Software Repositories},
  title      = {Concern Based Mining of Heterogeneous Software Repositories},
  year       = {2006},
  address    = {New York, NY, USA},
  pages      = {80–86},
  publisher  = {Association for Computing Machinery},
  series     = {MSR '06},
  abstract   = {In the current trend of software engineering, software systems are viewed as clusters of overlapping structures representing various concerns, covering heterogeneous artifacts like models, code, resource files etc. In those cases, adequate search mechanisms for software repositories should be based on such fragmented nature of software systems, allowing concern-oriented queries on the system data. For this purpose, we propose a conceptual framework for a concern-oriented query language for software repositories. A pattern-based implementation scheme is discussed, exploiting existing tools. The applicability of the approach is studied in the context of an industrial case study.},
  doi        = {10.1145/1137983.1138004},
  file       = {:10.1145_1137983.1138004 - Concern Based Mining of Heterogeneous Software Repositories.pdf:PDF},
  isbn       = {1595933972},
  keywords   = {heterogeneous software repositories, pattern-based search structures, concern-based mining, skimmed},
  location   = {Shanghai, China},
  numpages   = {7},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1137983.1138004},
}

@InProceedings{10.1145/1028664.1028681,
  author     = {Tarr, Peri and Chung, William and Harrison, William and Kruskal, Vincent and Ossher, Harold and Sutton, Stanley M. and Clement, Andrew and Chapman, Matthew and Hawkins, Helen and January, Sian},
  booktitle  = {Companion to the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications},
  title      = {The Concern Manipulation Environment [OOPSLA/GPCE]},
  year       = {2004},
  address    = {New York, NY, USA},
  pages      = {29–30},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '04},
  abstract   = {The Concern Manipulation Environment (CME) aims to provide a set of open, extensible components and a set of tools that promote aspect-oriented software development (AOSD) throughout the software lifecycle. It has two main goals:To provide an open, integrated development environment (IDE) to enable software engineers to use AOSD techniques throughout the software lifecycle, and to allow them to use different AOSD approaches in an integrated manner.To promote the rapid development of new tools supporting AOSD at any stage of the software lifecycle, and to serve as an integrating platform for such tools, facilitating development and experimentation with new AOSD approaches.This demonstration will highlight a number of tools and components that are useful to software developers and to AOSD tool providers and researchers. Tools for software developers include ones that allow developers to identify, model and visualize concerns, aspects and relationships in their software, covering software artifacts of any type, including both code and non-code artifacts, and including latent concerns or aspects that were not separated in the artifacts; that enable flexible queries over software; and that compose/integrate aspects and other concerns. For AOSD tool providers and researchers, the demonstration will describe some of the CME's support for integration of tools and approaches within the environment, highlighting the integration of Java, AspectJ and Ant artifacts within the CME, and how to use the CME's extensible components to create new AOSD tools or prototypes rapidly.},
  doi        = {10.1145/1028664.1028681},
  file       = {:10.1145_1028664.1028681 - The Concern Manipulation Environment [OOPSLA_GPCE].pdf:PDF},
  isbn       = {1581138334},
  keywords   = {software design, integration, extraction, eclipse open source, full-lifecycle software engineering, aspect-oriented software development (AOSD), software query, separation of concern, integrated development environment, concern modeling, software composition, skimmed},
  location   = {Vancouver, BC, CANADA},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1028664.1028681},
}

@InProceedings{10.1145/1083125.1083134,
  author     = {Harrison, William and Ossher, Harold and Sutton, Stanley and Tarr, Peri},
  booktitle  = {Proceedings of the 2005 Workshop on Modeling and Analysis of Concerns in Software},
  title      = {Concern Modeling in the Concern Manipulation Environment},
  year       = {2005},
  address    = {New York, NY, USA},
  pages      = {1–5},
  publisher  = {Association for Computing Machinery},
  series     = {MACS '05},
  abstract   = {The Concern Manipulation Environment (CME) is an AOSD environment in which software is organized and manipulated in terms of concerns. This paper is about ConMan, the CME concern manager, which supports the identification, definition, encapsulation, extraction and composition of concerns. ConMan models software in terms of concerns, relationships, constraints, units, artifacts, and associated information. The concern model is multidimensional and concerns can be defined extensionally and/or intensionally. ConMan is neutral with respect to artifact types and formalisms, and it can be used with both aspect-oriented and non-aspect oriented software and methods. ConMan is intended to serve both as a tool for directly modeling concerns and as a platform for developing alternative concern-modeling approaches.},
  doi        = {10.1145/1083125.1083134},
  file       = {:10.1145_1083125.1083134 - Concern Modeling in the Concern Manipulation Environment.pdf:PDF},
  isbn       = {1595931198},
  keywords   = {skimmed},
  location   = {St. Louis, Missouri},
  numpages   = {5},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1083125.1083134},
}

@InProceedings{10.1145/1065167.1065169,
  author     = {Lam, Monica S. and Whaley, John and Livshits, V. Benjamin and Martin, Michael C. and Avots, Dzintars and Carbin, Michael and Unkel, Christopher},
  booktitle  = {Proceedings of the Twenty-Fourth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
  title      = {Context-Sensitive Program Analysis as Database Queries},
  year       = {2005},
  address    = {New York, NY, USA},
  pages      = {1–12},
  publisher  = {Association for Computing Machinery},
  series     = {PODS '05},
  abstract   = {Program analysis has been increasingly used in software
engineering tasks such as auditing programs for security
vulnerabilities and finding errors in general. Such tools often
require analyses much more sophisticated than those traditionally
used in compiler optimizations. In particular, context-sensitive
pointer alias information is a prerequisite for any sound and
precise analysis that reasons about uses of heap objects in a
program. Context-sensitive analysis is challenging because there
are over 1014 contexts in a typical large program, even
after recursive cycles are collapsed. Moreover, pointers cannot be
resolved in general without analyzing the entire program.This paper presents a new framework, based on the concept of
deductive databases, for context-sensitive program analysis. In
this framework, all program information is stored as relations;
data access and analyses are written as Datalog queries. To handle
the large number of contexts in a program, the database represents
relations with binary decision diagrams (BDDs). The system we have
developed, called bddbddb, automatically translates database
queries into highly optimized BDD programs.Our preliminary experiences suggest that a large class of
analyses involving heap objects can be described succinctly in
Datalog and implemented efficiently with BDDs. To make developing
application-specific analyses easy for programmers, we have also
created a language called PQL that makes a subset of Datalog
queries more intuitive to define. We have used the language to find
many security holes in Web applications.},
  doi        = {10.1145/1065167.1065169},
  file       = {:10.1145_1065167.1065169 - Context Sensitive Program Analysis As Database Queries.pdf:PDF},
  isbn       = {1595930620},
  keywords   = {skimmed},
  location   = {Baltimore, Maryland},
  numpages   = {12},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1065167.1065169},
}

@InProceedings{10.5555/2818754.2818827,
  author     = {Weiss, Cathrin and Rubio-Gonz\'{a}lez, Cindy and Liblit, Ben},
  booktitle  = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
  title      = {Database-Backed Program Analysis for Scalable Error Propagation},
  year       = {2015},
  pages      = {586–597},
  publisher  = {IEEE Press},
  series     = {ICSE '15},
  abstract   = {Software is rapidly increasing in size and complexity. Static analyses must be designed to scale well if they are to be usable with realistic applications, but prior efforts have often been limited by available memory. We propose a database-backed strategy for large program analysis based on graph algorithms, using a Semantic Web database to manage representations of the program under analysis. Our approach is applicable to a variety of interprocedural finite distributive subset (IFDS) dataflow problems; we focus on error propagation as a motivating example. Our implementation analyzes multi-million-line programs quickly and in just a fraction of the memory required by prior approaches. When memory alone is insufficient, our approach falls back on disk using several hybrid configurations tuned to put all available resources to good use.},
  file       = {:10.5555_2818754.2818827 - Database Backed Program Analysis for Scalable Error Propagation.pdf:PDF},
  isbn       = {9781479919345},
  keywords   = {skimmed},
  location   = {Florence, Italy},
  numpages   = {12},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1989323.1989456,
  author     = {Huang, Shan Shan and Green, Todd Jeffrey and Loo, Boon Thau},
  booktitle  = {Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data},
  title      = {Datalog and Emerging Applications: An Interactive Tutorial},
  year       = {2011},
  address    = {New York, NY, USA},
  pages      = {1213–1216},
  publisher  = {Association for Computing Machinery},
  series     = {SIGMOD '11},
  abstract   = {We are witnessing an exciting revival of interest in recursive Datalog queries in a variety of emerging application domains such as data integration, information extraction, networking, program analysis, security, and cloud computing. This tutorial briefly reviews the Datalog language and recursive query processing and optimization techniques, then discusses applications of Datalog in three application domains: data integration, declarative networking, and program analysis. Throughout the tutorial, we use LogicBlox, a commercial Datalog engine for enterprise software systems, to allow the audience to walk through code examples presented in the tutorial.},
  doi        = {10.1145/1989323.1989456},
  file       = {:C\:/Users/Wernsen/Downloads/1989323.1989456.pdf:PDF},
  isbn       = {9781450306614},
  keywords   = {program analysis, declarative networking, data exchange, recursive query processing, datalog, data integration, skimmed},
  location   = {Athens, Greece},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1989323.1989456},
}

@InProceedings{10.1145/3238147.3238211,
  author     = {Sung, Chungha and Lahiri, Shuvendu K. and Enea, Constantin and Wang, Chao},
  booktitle  = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  title      = {Datalog-Based Scalable Semantic Diffing of Concurrent Programs},
  year       = {2018},
  address    = {New York, NY, USA},
  pages      = {656–666},
  publisher  = {Association for Computing Machinery},
  series     = {ASE 2018},
  abstract   = {When an evolving program is modified to address issues related to thread synchronization, there is a need to confirm the change is correct, i.e., it does not introduce unexpected behavior. However, manually comparing two programs to identify the semantic difference is labor intensive and error prone, whereas techniques based on model checking are computationally expensive. To fill the gap, we develop a fast and approximate static analysis for computing synchronization differences of two programs. The method is fast because, instead of relying on heavy-weight model checking techniques, it leverages a polynomial-time Datalog-based program analysis framework to compute differentiating data-flow edges, i.e., edges allowed by one program but not the other. Although approximation is used our method is sufficiently accurate due to careful design of the Datalog inference rules and iterative increase of the required data-flow edges for representing a difference. We have implemented our method and evaluated it on a large number of multithreaded C programs to confirm its ability to produce, often within seconds, the same differences obtained by human; in contrast, prior techniques based on model checking take minutes or even hours and thus can be 10x to 1000x slower.},
  doi        = {10.1145/3238147.3238211},
  file       = {:10.1145_3238147.3238211 - Datalog Based Scalable Semantic Diffing of Concurrent Programs.pdf:PDF},
  isbn       = {9781450359375},
  keywords   = {atomicity, change impact, Concurrency, Datalog, race condition, semantic diffing, static analysis, skimmed},
  location   = {Montpellier, France},
  numpages   = {11},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/3238147.3238211},
}

@InProceedings{10.1145/1328279.1328286,
  author     = {Czyz, Jeffrey K. and Jayaraman, Bharat},
  booktitle  = {Proceedings of the 2007 OOPSLA Workshop on Eclipse Technology EXchange},
  title      = {Declarative and Visual Debugging in Eclipse},
  year       = {2007},
  address    = {New York, NY, USA},
  pages      = {31–35},
  publisher  = {Association for Computing Machinery},
  series     = {eclipse '07},
  abstract   = {We present a declarative and visual debugging environment for Eclipse called JIVE.1 Traditional debugging is procedural in that a programmer must proceed step-by-step and object-by-object in order to uncover the cause of an error. In contrast, we present a declarative approach to debugging consisting of a flexible set of queries over a program's execution history as well as over individual runtime states. This runtime information is depicted in a visual manner during program execution in order to aid the debugging process. The current state of execution is depicted through an enhanced object diagram, and the history of execution is depicted by a sequence diagram. Our methodology makes use of these diagrams as a means of formulating queries and reporting results in a visual manner. It also supports revisiting past runtime states, either through reverse stepping of the program or through queries that report information from past states. Eclipse serves as an ideal framework for implementing JIVE since, like the JIVE architecture, it makes crucial use of the Java Platform Debugging Architecture (JPDA). This paper presents details of the JIVE architecture and its integration into Eclipse.},
  doi        = {10.1145/1328279.1328286},
  file       = {:10.1145_1328279.1328286 - Declarative and Visual Debugging in Eclipse.pdf:PDF},
  isbn       = {9781605580159},
  keywords   = {skimmed},
  location   = {Montreal, Quebec, Canada},
  numpages   = {5},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1328279.1328286},
}

@Article{10.1007/s11219-013-9220-4,
  author     = {Zhang, Tian and Zheng, Xiaomei and Zhang, Yan and Zhao, Jianhua and Li, Xuandong},
  journal    = {Software Quality Journal},
  title      = {A Declarative Approach for Java Code Instrumentation},
  year       = {2015},
  issn       = {0963-9314},
  month      = mar,
  number     = {1},
  pages      = {143–170},
  volume     = {23},
  abstract   = {Source instrumentation plays an important role in dynamic program analysis. However, current instrumentation implementations require programmers to write ad hoc rules that are often too complex to use and maintain. To address this complexity, we divide the task of source instrumentation into two steps: first, the source points are queried, into which code fragments should be planted; secondly, the code fragments including contextual information are generated and planted into source code through the queried points. According to this idea, we present a new method based on declarative code queries, which makes it easier to specify instrumentations using contextual information collected from expressive code queries. The JIns language provided by our method is constructed following an SQL-like style, which is well known and widely used by programmers. We evaluate the method in terms of the reduced complexity of instrumentation specifications for several common instrumentation tasks.},
  address    = {USA},
  doi        = {10.1007/s11219-013-9220-4},
  file       = {:10.1007_s11219-013-9220-4 - A Declarative Approach for Java Code Instrumentation.pdf:PDF},
  issue_date = {March 2015},
  keywords   = {Java, Code instrumentation, Source query, read},
  numpages   = {28},
  publisher  = {Kluwer Academic Publishers},
  readstatus = {read},
  url        = {https://doi.org/10.1007/s11219-013-9220-4},
}

@InProceedings{Stevens2015,
  author     = {Stevens, Reinout},
  booktitle  = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
  title      = {A {Declarative} {Foundation} for {Comprehensive} {History} {Querying}},
  year       = {2015},
  month      = may,
  note       = {ISSN: 1558-1225},
  pages      = {907--910},
  volume     = {2},
  abstract   = {Researchers in the field of Mining Software Repositories perform studies about the evolution of software projects. To this end, they use the version control system storing the changes made to a single software project. Such studies are concerned with the source code characteristics in one particular revision, the commit data for that revision, how the code evolves over time and what concrete, fine-grained changes were applied to the source code between two revisions. Although tools exist to analyse an individual concern, scripts and manual work is required to combine these tools to perform a single experiment. We present a general-purpose history querying tool named QwalKeko that enables expressing these concerns in a single uniform language, and having them detected in a git repository. We have validated our work by means of replication studies as well as through MSR studies of our own.},
  doi        = {10.1109/ICSE.2015.289},
  file       = {:C\:/Users/Wernsen/Downloads/vub-soft-tr-15-05.pdf:PDF},
  issn       = {1558-1225},
  keywords   = {Software, History, Database languages, Libraries, Programming, Medical services, Java, declarative programming, program querying, history querying, mining software repositories, read},
  readstatus = {read},
}

@InProceedings{rohtua,
  author     = {Dyer,Robert and Rajan,Hridesh and Nguyen,Tien},
  title      = {Declarative visitors to ease fine-grained source code mining with full history on billions of AST nodes},
  year       = {2013},
  pages      = {23-32},
  publisher  = {ACM},
  abstract   = {Software repositories contain a vast wealth of information about software development. Mining these repositories has proven useful for detecting patterns in software development, testing hypotheses for new software engineering approaches, etc. Specifically, mining source code has yielded significant insights into software development artifacts and processes. Unfortunately, mining source code at a large-scale remains a difficult task. Previous approaches had to either limit the scope of the projects studied, limit the scope of the mining task to be more coarse-grained, or sacrifice studying the history of the code due to both human and computational scalability issues. In this paper we address the substantial challenges of mining source code: a) at a very large scale; b) at a fine-grained level of detail; and c) with full history information.  To address these challenges, we present domain-specific language features for source code mining. Our language features are inspired by object-oriented visitors and provide a default depth-first traversal strategy along with two expressions for defining custom traversals. We provide an implementation of these features in the Boa infrastructure for software repository mining and describe a code generation strategy into Java code. To show the usability of our domain-specific language features, we reproduced over 40 source code mining tasks from two large-scale previous studies in just 2 person-weeks. The resulting code for these tasks show between 2.0x--4.8x reduction in code size. Finally we perform a small controlled experiment to gain insights into how easily mining tasks written using our language features can be understood, with no prior training. We show a substantial number of tasks (77%) were understood by study participants, in about 3 minutes per task.},
  file       = {:rohtua - Declarative Visitors to Ease Fine Grained Source Code Mining with Full History on Billions of AST Nodes.pdf:PDF},
  isbn       = {9781450323734;1450323731;},
  keywords   = {boa; source code mining; visitor pattern, skimmed},
  language   = {English},
  readstatus = {skimmed},
}

@InProceedings{10.1007/978-3-642-00434-6_17,
  author     = {Basten, H. J. S. and Klint, P.},
  booktitle  = {Software Language Engineering},
  title      = {DeFacto: Language-Parametric Fact Extraction from Source Code},
  year       = {2009},
  address    = {Berlin, Heidelberg},
  editor     = {Ga{\v{s}}evi{\'{c}}, Dragan and L{\"a}mmel, Ralf and Van Wyk, Eric},
  pages      = {265--284},
  publisher  = {Springer Berlin Heidelberg},
  abstract   = {Extracting facts from software source code forms the foundation for any software analysis. Experience shows, however, that extracting facts from programs written in a wide range of programming and application languages is labour-intensive and error-prone. We present DeFacto, a new technique for fact extraction. It amounts to annotating the context-free grammar of a language of interest with fact annotations that describe how to extract elementary facts for language elements such as, for instance, a declaration or use of a variable, a procedure or method call, or control flow statements. Once the elementary facts have been extracted, we use relational techniques to further enrich them and to perform the actual software analysis.},
  file       = {:10.1007_978-3-642-00434-6_17 - DeFacto_ Language Parametric Fact Extraction from Source Code.pdf:PDF},
  isbn       = {978-3-642-00434-6},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1368088.1368142,
  author     = {Eichberg, Michael and Kloppenburg, Sven and Klose, Karl and Mezini, Mira},
  booktitle  = {Proceedings of the 30th International Conference on Software Engineering},
  title      = {Defining and Continuous Checking of Structural Program Dependencies},
  year       = {2008},
  address    = {New York, NY, USA},
  pages      = {391–400},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '08},
  abstract   = {Dependencies between program elements need to be modeled from different perspectives reflecting architectural, design, and implementation level decisions. To avoid erosion of the intended structure of the code, it is necessary to explicitly codify these different perspectives on the permitted dependencies and to detect violations continuously and incrementally as software evolves.We propose an approach that uses declarative queries to group source elements - across programming language module boundaries - into overlapping ensembles. The dependencies between these ensembles are also specified as logic queries. The approach has been integrated into the incremental build process of Eclipse to ensure continuous checking, using an engine for tabled and incremental evaluation of logic queries. Our evaluation shows that our approach is fast enough for day-to-day use along the incremental build process of modern IDEs.},
  doi        = {10.1145/1368088.1368142},
  file       = {:10.1145_1368088.1368142 - Defining and Continuous Checking of Structural Program Dependencies.pdf:PDF},
  isbn       = {9781605580791},
  keywords   = {static analysis, continuous checking, controlling program dependencies, datalog, skimmed},
  location   = {Leipzig, Germany},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1368088.1368142},
}

@Article{Anderson2003,
  author     = {Anderson, P. and Reps, T. and Teitelbaum, T.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {Design and implementation of a fine-grained software inspection tool},
  year       = {2003},
  issn       = {1939-3520},
  month      = aug,
  number     = {8},
  pages      = {721--733},
  volume     = {29},
  abstract   = {Although software inspection has led to improvements in software quality, many software systems continue to be deployed with unacceptable numbers of errors, even when software inspection is part of the development process. The difficulty of manually verifying that the software under inspection conforms to the rules is partly to blame. We describe the design and implementation of a tool designed to help alleviate this problem. The tool provides mechanisms for fine-grained inspection of software by exposing the results of sophisticated whole-program static analysis to the inspector. The tool computes many static-semantic representations of the program, including an accurate call graph and dependence graph. A whole-program pointer analysis is used to make sure that the representation is precise with respect to aliases induced by pointer usage. Views on the dependence graph and related representations are supported. Queries on the dependence graph allow an inspector to answer detailed questions about the semantics of the program. Facilities for openness and extensibility permit the tool to be integrated with many software development processes. The main challenge of the approach is to provide facilities to navigate and manage the enormous complexity of the dependence graph.},
  doi        = {10.1109/TSE.2003.1223646},
  file       = {:C\:/Users/Wernsen/Downloads/anderson2003.pdf:PDF},
  keywords   = {Software tools, Inspection, Software quality, Navigation, Software systems, Software engineering, Best practices, NASA, Performance analysis, Filters, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/372202.372796,
  author     = {Hobatr, Chanika and Malloy, Brian A.},
  booktitle  = {Proceedings of the 2001 ACM Symposium on Applied Computing},
  title      = {The Design of an OCL Query-Based Debugger for C++},
  year       = {2001},
  address    = {New York, NY, USA},
  pages      = {658–662},
  publisher  = {Association for Computing Machinery},
  series     = {SAC '01},
  doi        = {10.1145/372202.372796},
  file       = {:C\:/Users/Wernsen/Downloads/372202.372796.pdf:PDF},
  isbn       = {1581132875},
  keywords   = {OpenC++, meta-class, unified modelling language, code instrumentation, debugging, query, code generation, meta-object protocol, object constraint language, skimmed},
  location   = {Las Vegas, Nevada, USA},
  numpages   = {5},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/372202.372796},
}

@Article{Jarzabek1998,
  author     = {Jarzabek, S.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {Design of flexible static program analyzers with {PQL}},
  year       = {1998},
  issn       = {1939-3520},
  month      = mar,
  number     = {3},
  pages      = {197--215},
  volume     = {24},
  abstract   = {Static program analyzers (SPA) are interactive tools that enhance program understanding during maintenance by answering queries about programs. Depending on the maintenance task in hand, SPAs must process different source programs and answer different types of program queries. Flexibility is, therefore, a desirable property of SPAs. The author describes a program query language, called PQL, that facilitates the design of flexible SPAs. PQL is a conceptual level, source language-independent notation to specify program queries and program views. In PQL, one can query global program design as well as search for detail code patterns. PQL queries are answered automatically by a query evaluation mechanism built into an SPA. Program design models and POL form the core of an SPA conceptual model. He based the SPA's architecture on this conceptual model. By separating the conceptual model from the implementation decisions, one can design SPAs that are customizable to the needs of the maintenance project at hand. Depending on criteria such as efficiency of query evaluation or simplicity of the SPA design, one can implement the same functional specifications of an SPA on a variety of program representations to meet the required criteria. Apart from its role in the design of SPAs, the conceptual model also allows one to rigorously study SPA functionality in the context of the underlying maintenance process and programmer behavior models, in isolation from tool implementation details.},
  doi        = {10.1109/32.667879},
  file       = {:C\:/Users/Wernsen/Downloads/jarzabek1998.pdf:PDF},
  keywords   = {Programming profession, Database languages, Query processing, Context modeling, User interfaces, Computer Society, Reverse engineering, Software maintenance, Cost function, Information systems, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1735935.1735938,
  author     = {Castro, Sergio and Brichau, Johan and Mens, Kim},
  booktitle  = {Proceedings of the International Workshop on Smalltalk Technologies},
  title      = {Diagnosis and Semi-Automatic Correction of Detected Design Inconsistencies in Source Code},
  year       = {2009},
  address    = {New York, NY, USA},
  pages      = {8–17},
  publisher  = {Association for Computing Machinery},
  series     = {IWST '09},
  abstract   = {In order to alleviate design decay, different program design documentation techniques are used for the specification and detection of design inconsistencies in code. However, these design documentation techniques do not always provide support for the diagnosis and (semi-) automatic correction of such inconsistencies. In case they do, corrective solutions are typically targeted to a reduced set of pre-defined inconsistency problems, and they are not easily customizable to new kinds of consistency checks defined by a user. In particular, they cannot infer possible corrective actions to solve new user-defined inconsistency problems. In this paper, we present a technique for the diagnosis and (semi-) automatic correction of inconsistencies in the context of an existing tool for inconsistency management: IntensiVE. Our technique uses logic abductive reasoning to infer solutions to detected user-defined inconsistencies, starting from basic composable corrective actions. A first prototype implementing our technique on top of IntensiVE is shown.},
  doi        = {10.1145/1735935.1735938},
  file       = {:10.1145_1735935.1735938 - Diagnosis and Semi Automatic Correction of Detected Design Inconsistencies in Source Code.pdf:PDF},
  isbn       = {9781605588995},
  keywords   = {Smalltalk, design inconsistency, abductive reasoning, diagnosis, IntensiVE, correction, SOUL, skimmed},
  location   = {Brest, France},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1735935.1735938},
}

@InProceedings{10.1145/1401827.1401837,
  author     = {Gorbovitski, Michael and Rothamel, Tom and Liu, Yanhong A. and Stoller, Scott D.},
  booktitle  = {Proceedings of the 2008 International Workshop on Dynamic Analysis: Held in Conjunction with the ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2008)},
  title      = {Efficient Runtime Invariant Checking: A Framework and Case Study},
  year       = {2008},
  address    = {New York, NY, USA},
  pages      = {43–49},
  publisher  = {Association for Computing Machinery},
  series     = {WODA '08},
  abstract   = {This paper describes a general and powerful framework for efficient runtime invariant checking. The framework supports (1) declarative specification of arbitrary invariants using high-level queries, with easy use of information from any data in the execution, (2) powerful analysis and transformations for automatic generation of instrumentation for efficient incremental checking of invariants, and (3) convenient mechanisms for reporting errors, debugging, and taking preventive or remedial actions, as well as recording history data for use in queries. We demonstrate the advantages and effectiveness of the framework through implementations and case studies with abstract syntax tree transformations, authentication in a SMB client, and the BitTorrent peer-to-peer file distribution protocol.},
  doi        = {10.1145/1401827.1401837},
  file       = {:10.1145_1401827.1401837 - Efficient Runtime Invariant Checking_ a Framework and Case Study.pdf:PDF},
  isbn       = {9781605580548},
  keywords   = {runtime verification, program transformation, incrementalization, alias analysis, skimmed},
  location   = {Seattle, Washington},
  numpages   = {7},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1401827.1401837},
}

@Article{Panchenko2011,
  author     = {Panchenko, Oleksandr and Plattner, Hasso and Zeier, Alexander B.},
  journal    = {Information Systems Frontiers},
  title      = {Efficient storage and fast querying of source code},
  year       = {2011},
  issn       = {1572-9419},
  month      = jul,
  number     = {3},
  pages      = {349--357},
  volume     = {13},
  abstract   = {Enabling fast and detailed insights over large portions of source code is an important task in a global development ecosystem. Numerous data structures have been developed to store source code and to support various structural queries, to help in navigation, evaluation and analysis. Many of these data structures work with tree-based or graph-based representations of source code. The goal of this project is to elaborate a data storage that enables efficient storing and fast querying of structural information. The naive adjacency list method has been enhanced with the use of recent data compression approaches for column-oriented databases to allow no-loss albeit compact storage of fine-grained structural data. The graph indexing has enabled the proposed data model to expeditiously answer fine-grained structural queries. This paper describes the basics of the proposed approach and illustrates its technical feasibility.},
  doi        = {10.1007/s10796-010-9285-6},
  file       = {:Panchenko2011 - Efficient Storage and Fast Querying of Source Code.pdf:PDF;:panchenko2010.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1007/s10796-010-9285-6},
  urldate    = {2021-06-08},
}

@InProceedings{10.1145/2602576.2602590,
  author     = {Olsson, Tobias and Toll, Daniel and Wingkvist, Anna and Ericsson, Morgan},
  booktitle  = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
  title      = {Evaluation of a Static Architectural Conformance Checking Method in a Line of Computer Games},
  year       = {2014},
  address    = {New York, NY, USA},
  pages      = {113–118},
  publisher  = {Association for Computing Machinery},
  series     = {QoSA '14},
  abstract   = {We present an evaluation of a simple method to find architectural problems in a product line of computer games. The method uses dependencies (direct, indirect, or no) to automatically classify types in the implementation to high-level components in the product line architecture. We use a commercially available tool to analyse dependencies in the source code. The automatic classification of types is compared to a manual classification by the developer, and all mismatches are reported. To evaluate the method, we inspect the source code and look for a pre-defined set of architectural problems in all types. We compare the set of types that contained problems to the set of types where the manual and automatic classification disagreed to determine precision and recall. We also investigate what changes are needed to correct the found mismatches by either designing and implementing changes in the source code or refining the automatic classification. Our evaluation shows that the simple method is effective at detecting architectural problems in a product line of four games. The method is lightweight, customisable and easy to implement early in the development cycle.},
  doi        = {10.1145/2602576.2602590},
  file       = {:10.1145_2602576.2602590 - Evaluation of a Static Architectural Conformance Checking Method in a Line of Computer Games.pdf:PDF},
  isbn       = {9781450325769},
  keywords   = {product line architecture, model-view-controller, static conformance checking, MVC, computer game, skimmed},
  location   = {Marcq-en-Bareul, France},
  numpages   = {6},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2602576.2602590},
}

@InProceedings{10.1145/1735935.1735947,
  author     = {G\'{o}mez, Ver\'{o}nica Uquillas and Kellens, Andy and Gybels, Kris and D'Hondt, Theo},
  booktitle  = {Proceedings of the International Workshop on Smalltalk Technologies},
  title      = {Experiments with Pro-Active Declarative Meta-Programming},
  year       = {2009},
  address    = {New York, NY, USA},
  pages      = {68–76},
  publisher  = {Association for Computing Machinery},
  series     = {IWST '09},
  abstract   = {Program querying has become a valuable asset in the programmer's toolbox. Using dedicated querying languages, developers can reason about their source code in order to find errors, refactoring opportunities and so on. Within Smalltalk, the SOUL language has been proposed as one such language that offers a declarative and expressive means to query the source code of object-oriented programs.Ever since its inception, SOUL has been used as the underlying technique for a number of academic software engineering tools. Despite its success, one of the problems of SOUL is that, due to its backward chained implementation, it is less suited as a basis for such pro-active software tools. Using SOUL, a developer has to launch the queries over the system manually, rather than automatically receiving feedback whenever the underlying source code is changed. In this paper we present PARACHUT, an alternative logic query language that is based on forward chaining and temporal logic and that allows developers to express queries over the change history of the system. Furthermore, PARACHUT's data-driven nature makes it possible to provide instant feedback to developers when the source code is changed, thus providing better support for pro-active software tools.},
  doi        = {10.1145/1735935.1735947},
  file       = {:10.1145_1735935.1735947 - Experiments with Pro Active Declarative Meta Programming.pdf:PDF},
  isbn       = {9781605588995},
  keywords   = {source-code history, temporal logic, program querying, skimmed},
  location   = {Brest, France},
  numpages   = {9},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1735935.1735947},
}

@Article{CanovasIzquierdo2014,
  author     = {Cánovas Izquierdo, Javier Luis and García Molina, Jesús},
  journal    = {Software \& Systems Modeling},
  title      = {Extracting models from source code in software modernization},
  year       = {2014},
  issn       = {1619-1374},
  month      = may,
  number     = {2},
  pages      = {713--734},
  volume     = {13},
  abstract   = {Model-driven software modernization is a discipline in which model-driven development (MDD) techniques are used in the modernization of legacy systems. When existing software artifacts are evolved, they must be transformed into models to apply MDD techniques such as model transformations. Since most modernization scenarios (e.g., application migration) involve dealing with code in general-purpose programming languages (GPL), the extraction of models from GPL code is an essential task in a model-based modernization process. This activity could be performed by tools to bridge grammarware and MDD technical spaces, which is normally carried out by dedicated parsers. Grammar-to-Model Transformation Language (Gra2MoL) is a domain-specific language (DSL) tailored to the extraction of models from GPL code. This DSL is actually a text-to-model transformation language which can be applied to any code conforming to a grammar. Gra2MoL aims to reduce the effort needed to implement grammarware-MDD bridges, since building dedicated parsers is a complex and time-consuming task. Like ATL and RubyTL languages, Gra2MoL incorporates the binding concept needed to write mappings between grammar elements and metamodel elements in a simple declarative style. The language also provides a powerful query language which eases the retrieval of scattered information in syntax trees. Moreover, it incorporates extensibility and grammar reuse mechanisms. This paper describes Gra2MoL in detail and includes a case study based on the application of the language in the extraction of models from Delphi code.},
  doi        = {10.1007/s10270-012-0270-z},
  file       = {:CanovasIzquierdo2014 - Extracting Models from Source Code in Software Modernization.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1007/s10270-012-0270-z},
  urldate    = {2021-06-09},
}

@Article{Antkiewicz2008,
  author     = {Antkiewicz, Michal and Tonelli Bartolomei, Thiago and Czarnecki, Krzysztof},
  journal    = {Automated Software Engineering},
  title      = {Fast extraction of high-quality framework-specific models from application code},
  year       = {2008},
  issn       = {1573-7535},
  month      = nov,
  number     = {1},
  pages      = {101},
  volume     = {16},
  abstract   = {Framework-specific models represent the design of application code from the framework viewpoint by showing how framework-provided concepts are instantiated in the code. Retrieving such models quickly and precisely is necessary for practical model-supported software engineering, in which developers use design models for development tasks such as code understanding, verifying framework usage rules, and round-trip engineering. Also, comparing models extracted at different times of the software lifecycle supports software evolution tasks.},
  doi        = {10.1007/s10515-008-0040-x},
  file       = {:Antkiewicz2008 - Fast Extraction of High Quality Framework Specific Models from Application Code.pdf:PDF;:Fast_extraction_of_high-quality_framework-specific.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1007/s10515-008-0040-x},
  urldate    = {2021-06-09},
}

@Article{10.1145/1103845.1094840,
  author     = {Martin, Michael and Livshits, Benjamin and Lam, Monica S.},
  journal    = {SIGPLAN Not.},
  title      = {Finding Application Errors and Security Flaws Using PQL: A Program Query Language},
  year       = {2005},
  issn       = {0362-1340},
  month      = oct,
  number     = {10},
  pages      = {365–383},
  volume     = {40},
  abstract   = {A number of effective error detection tools have been built in recent years to check if a program conforms to certain design rules. An important class of design rules deals with sequences of events asso-ciated with a set of related objects. This paper presents a language called PQL (Program Query Language) that allows programmers to express such questions easily in an application-specific context. A query looks like a code excerpt corresponding to the shortest amount of code that would violate a design rule. Details of the tar-get application's precise implementation are abstracted away. The programmer may also specify actions to perform when a match is found, such as recording relevant information or even correcting an erroneous execution on the fly.We have developed both static and dynamic techniques to find solutions to PQL queries. Our static analyzer finds all potential matches conservatively using a context-sensitive, flow-insensitive, inclusion-based pointer alias analysis. Static results are also use-ful in reducing the number of instrumentation points for dynamic analysis. Our dynamic analyzer instruments the source program to catch all violations precisely as the program runs and to optionally perform user-specified actions.We have implemented the techniques described in this paper and found 206 errors in 6 large real-world open-source Java applica-tions containing a total of nearly 60,000 classes. These errors are important security flaws, resource leaks, and violations of consis-tency invariants. The combination of static and dynamic analysis proves effective at addressing a wide range of debugging and pro-gram comprehension queries. We have found that dynamic analysis is especially suitable for preventing errors such as security vulner-abilities at runtime.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1103845.1094840},
  file       = {:10.1145_1103845.1094840 - Finding Application Errors and Security Flaws Using PQL_ a Program Query Language.pdf:PDF},
  issue_date = {October 2005},
  keywords   = {bug finding, resource leaks, SQL injection, program traces, pattern matching, web applications, skimmed},
  numpages   = {19},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1103845.1094840},
}

@InProceedings{10.1145/1810295.1810343,
  author     = {Deissenboeck, Florian and Heinemann, Lars and Hummel, Benjamin and Juergens, Elmar},
  booktitle  = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
  title      = {Flexible Architecture Conformance Assessment with ConQAT},
  year       = {2010},
  address    = {New York, NY, USA},
  pages      = {247–250},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '10},
  abstract   = {The architecture of software systems is known to decay if no counter-measures are taken. In order to prevent this architectural erosion, the conformance of the actual system architecture to its intended architecture needs to be assessed and controlled; ideally in a continuous manner. To support this, we present the architecture conformance assessment capabilities of our quality analysis framework ConQAT. In contrast to other tools, ConQAT is not limited to the assessment of use-dependencies between software components. Its generic architectural model allows the assessment of various types of dependencies found between different kinds of artifacts. It thereby provides the necessary tool-support for flexible architecture conformance assessment in diverse contexts.},
  doi        = {10.1145/1810295.1810343},
  file       = {:10.1145_1810295.1810343 - Flexible Architecture Conformance Assessment with ConQAT.pdf:PDF},
  isbn       = {9781605587196},
  keywords   = {skimmed},
  location   = {Cape Town, South Africa},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1810295.1810343},
}

@Article{489076,
  author     = {Devanbu, P.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {On "A framework for source code search using program patterns"},
  year       = {1995},
  number     = {12},
  pages      = {1009-1010},
  volume     = {21},
  doi        = {10.1109/32.489076},
  file       = {:489076 - On _A Framework for Source Code Search Using Program Patterns_.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Paul1994,
  author     = {Paul, S. and Prakash, A.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {A framework for source code search using program patterns},
  year       = {1994},
  issn       = {1939-3520},
  month      = jun,
  number     = {6},
  pages      = {463--475},
  volume     = {20},
  abstract   = {For maintainers involved in understanding and reengineering large software, locating source code fragments that match certain patterns is a critical task. Existing solutions to the problem are few, and they either involve manual, painstaking scans of the source code using tools based on regular expressions, or the use of large, integrated software engineering environments that include simple pattern-based query processors in their toolkits. We present a framework in which pattern languages are used to specify interesting code features. The pattern languages are derived by extending the source programming language with pattern-matching symbols. We describe SCRUPLE, a finite state machine-based source code search tool, that efficiently implements this framework. We also present experimental performance results obtained from a SCRUPLE prototype, and the user interface of a source code browser built on top of SCRUPLE.{\textless}{\textgreater}},
  doi        = {10.1109/32.295894},
  file       = {:C\:/Users/Wernsen/Downloads/Paul1994 - A Framework for Source Code Search Using Program Patterns.pdf:PDF},
  keywords   = {Software maintenance, Pattern matching, Design engineering, Programming profession, Software engineering, Computer languages, Software prototyping, Prototypes, User interfaces, Database languages, skimmed},
  readstatus = {skimmed},
}

@Article{10.1145/2211616.2211618,
  author     = {Shonle, Macneil and Griswold, William G. and Lerner, Sorin},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  title      = {A Framework for the Checking and Refactoring of Crosscutting Concepts},
  year       = {2012},
  issn       = {1049-331X},
  month      = jul,
  number     = {3},
  volume     = {21},
  abstract   = {Programmers employ crosscutting concepts, such as design patterns and other programming idioms, when their design ideas cannot be efficiently or effectively modularized in the underlying programming language. As a result, implementations of these crosscutting concepts can be hard to change even when the code is well structured.In this article, we describe Arcum, a system that supports the modular maintenance of crosscutting concepts. Arcum can be used to both check essential constraints of crosscutting concepts and to substitute crosscutting concept implementations with alternative implementations. Arcum is complementary to existing refactoring systems that focus on meaning-preserving program transformations at the programming-language-semantics level, because Arcum focuses on transformations at the conceptual level.We present the underpinnings of the Arcum approach and show how Arcum can be used to address several classical software engineering problems.},
  address    = {New York, NY, USA},
  articleno  = {15},
  doi        = {10.1145/2211616.2211618},
  file       = {:10.1145_2211616.2211618 - A Framework for the Checking and Refactoring of Crosscutting Concepts.pdf:PDF},
  issue_date = {June 2012},
  keywords   = {Design patterns, refactoring, skimmed},
  numpages   = {47},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2211616.2211618},
}

@InProceedings{10.1145/1233833.1233834,
  author     = {Ossher, Harold},
  booktitle  = {Proceedings of the 6th Workshop on Foundations of Aspect-Oriented Languages},
  title      = {Fundamentals of Concern Manipulation},
  year       = {2007},
  address    = {New York, NY, USA},
  pages      = {1–4},
  publisher  = {Association for Computing Machinery},
  series     = {FOAL '07},
  abstract   = {This talks describes a number of principles and key concepts underlying concern manipulation, the use of concerns to aid in a variety of software development tasks. Concern modeling and exploration, query and composition are considered. The principles and concepts guided work on the Concern Manipulation Environment (CME), which provides both prototype tools supporting aspect-oriented software development, and flexible components for use in building such tools.},
  doi        = {10.1145/1233833.1233834},
  file       = {:10.1145_1233833.1233834 - Fundamentals of Concern Manipulation.pdf:PDF},
  isbn       = {9781595936714},
  keywords   = {software decomposition and composition, separation of concerns, aspect-oriented software development, software queries, skimmed},
  location   = {Vancouver, British Columbia, Canada},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1233833.1233834},
}

@Article{Kalleberg2008,
  author     = {Kalleberg, Karl Trygve and Visser, Eelco},
  journal    = {Electronic Notes in Theoretical Computer Science},
  title      = {Fusing a {Transformation} {Language} with an {Open} {Compiler}},
  year       = {2008},
  issn       = {1571-0661},
  month      = apr,
  number     = {2},
  pages      = {21--36},
  volume     = {203},
  abstract   = {Program transformation systems provide powerful analysis and transformation frameworks as well as concise languages for language processing, but instantiating them for every subject language is an arduous task, most often resulting in half-completed frontends. Compilers provide mature frontends with robust parsers and type checkers, but solving language processing problems in general-purpose languages without transformation libraries is tedious. Reusing these frontends with existing transformation systems is therefore attractive. However, for this reuse to be optimal, the functional logic found in the frontend should be exposed to the transformation system – simple data serialization of the abstract syntax tree is not enough, since this fails to expose important compiler functionality, such as import graphs, symbol tables and the type checker. In this paper, we introduce a novel and general technique for combining term-based transformation systems with existing language frontends. The technique is presented in the context of a scriptable analysis and transformation framework for Java built on top of the Eclipse Java compiler. The framework consists of an adapter automatically extracted from the abstract syntax tree of the compiler and an interpreter for the Stratego program transformation language. The adapter allows the Stratego interpreter to rewrite directly on the compiler AST. We illustrate the applicability of our system with scripts written in Stratego that perform framework and library-specific analyses and transformations.},
  doi        = {10.1016/j.entcs.2008.03.042},
  file       = {:Kalleberg2008 - Fusing a Transformation Language with an Open Compiler.pdf:PDF},
  keywords   = {compiler scripting, strategic programming, program transformation, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Proceedings of the {Seventh} {Workshop} on {Language} {Descriptions}, {Tools}, and {Applications} ({LDTA} 2007)},
  url        = {https://www.sciencedirect.com/science/article/pii/S1571066108001473},
  urldate    = {2021-06-09},
}

@InProceedings{10.1145/1639950.1640032,
  author     = {Wegrzynowicz, Patrycja and Stencel, Krzysztof},
  booktitle  = {Proceedings of the 24th ACM SIGPLAN Conference Companion on Object Oriented Programming Systems Languages and Applications},
  title      = {The Good, the Bad, and the Ugly: Three Ways to Use a Semantic Code Query System},
  year       = {2009},
  address    = {New York, NY, USA},
  pages      = {821–822},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '09},
  abstract   = {D-CUBED is a semantic code query system for Java. Its focus is on capturing the semantics of an analyzed program. It provides rich support to investigate the call flow and data flow of a program by using static analysis techniques with the custom model of symbolic instances. The usage scenarios of D-CUBED include: (1) detection of design patterns with the focus on code semantics (the good), (2) discovery of bugs, including a range of security holes (the bad), (3) assessment and improvement of a design by discovering bad design and code practices (the ugly).},
  doi        = {10.1145/1639950.1640032},
  file       = {:10.1145_1639950.1640032 - The Good, the Bad, and the Ugly_ Three Ways to Use a Semantic Code Query System.pdf:PDF},
  isbn       = {9781605587684},
  keywords   = {automatic detection, bugs, semantic code query system, bad practices, design patterns, skimmed},
  location   = {Orlando, Florida, USA},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1639950.1640032},
}

@InProceedings{10.1145/1774088.1774561,
  author     = {Ciraci, Selim and van den Broek, Pim and Aksit, Mehmet},
  booktitle  = {Proceedings of the 2010 ACM Symposium on Applied Computing},
  title      = {Graph-Based Verification of Static Program Constraints},
  year       = {2010},
  address    = {New York, NY, USA},
  pages      = {2265–2272},
  publisher  = {Association for Computing Machinery},
  series     = {SAC '10},
  abstract   = {Software artifacts usually have static program constraints and these constraints should be satisfied in each reuse. In addition to this, the developers are also required to satisfy the coding conventions used by their organization. Because in a complex software system there are too many coding conventions and program constraints to be satisfied, it becomes a cumbersome task to check them all manually. This paper presents a process and tools that allow computer-aided program constraint checking that work on the source code.We developed a modeling language called Source Code Modeling Language (SCML) in which program elements from the source code can be represented. In the process, the source code is converted into SCML models. The constraint detection is realized by graph transformation rules which are also modeled in SCML; the rules detect the violation and extract information from the SCML model of the source code to provide feedback on the location of the problem. The constraint violations can be queried from a querying mechanism that automatically searches the graph for the extracted information. The process has been applied to an industrial software system.},
  doi        = {10.1145/1774088.1774561},
  file       = {:10.1145_1774088.1774561 - Graph Based Verification of Static Program Constraints.pdf:PDF},
  isbn       = {9781605586397},
  keywords   = {graph transformations, program constraints, constraint verification, prolog, skimmed},
  location   = {Sierre, Switzerland},
  numpages   = {8},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1774088.1774561},
}

@InProceedings{10.1145/1836089.1836093,
  author     = {Tekle, K. Tuncay and Gorbovitski, Michael and Liu, Yanhong A.},
  booktitle  = {Proceedings of the 12th International ACM SIGPLAN Symposium on Principles and Practice of Declarative Programming},
  title      = {Graph Queries through Datalog Optimizations},
  year       = {2010},
  address    = {New York, NY, USA},
  pages      = {25–34},
  publisher  = {Association for Computing Machinery},
  series     = {PPDP '10},
  abstract   = {This paper describes the use of a powerful graph query language for querying programs, and a novel combination of transformations for generating efficient implementations of the queries. The language supports graph path expressions that allow convenient use of both vertices and edges of arbitrary kinds as well as additional global and local parameters in graph paths. Our implementation method combines transformation to Datalog, recursion conversion, demand transformation, and specialization, and finally generates efficient analysis programs with precise complexity guarantees. This combination improves an O(VE) time complexity factor using previous methods to O(E), where V and E are the numbers of graph vertices and edges, respectively. We also describe implementations and experiments that confirm the analyzed complexities.},
  doi        = {10.1145/1836089.1836093},
  file       = {:10.1145_1836089.1836093 - Graph Queries through Datalog Optimizations.pdf:PDF},
  isbn       = {9781450301329},
  keywords   = {optimization, program transformation, datalog, graph query languages, complexity analysis, program analysis, demand-driven evaluation, skimmed},
  location   = {Hagenberg, Austria},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1836089.1836093},
}

@InProceedings{10.1145/2717124.2717127,
  author     = {Kartsaklis, Christos and Hernandez, Oscar R.},
  booktitle  = {Proceedings of the 1st Workshop on Programming Language Evolution},
  title      = {HERCULES/PL: The Pattern Language of HERCULES},
  year       = {2014},
  address    = {New York, NY, USA},
  pages      = {5–10},
  publisher  = {Association for Computing Machinery},
  series     = {PLE '14},
  abstract   = {Interrogating the structure of a program for patterns of interest is attractive to the broader spectrum of software engineering. The very approach by which a pattern is constructed remains a concern for the source code mining community. This paper presents a pattern programming model, for the C and Fortran programming languages, using a compiler directives approach. We discuss our specification, called HERCULES/PL, throughout a number of examples and show how different patterns can be constructed, plus some preliminary results.},
  doi        = {10.1145/2717124.2717127},
  file       = {:10.1145_2717124.2717127 - HERCULES_PL_ the Pattern Language of HERCULES.pdf:PDF},
  isbn       = {9781450328876},
  keywords   = {compiler directives, source code mining, pattern construction, skimmed},
  location   = {Uppsala, Sweden},
  numpages   = {6},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2717124.2717127},
}

@InProceedings{10.1109/SUITE.2009.5070019,
  author     = {Panchenko, Oleksandr},
  booktitle  = {Proceedings of the 2009 ICSE Workshop on Search-Driven Development-Users, Infrastructure, Tools and Evaluation},
  title      = {Hybrid Storage for Enabling Fully-Featured Text Search and Fine-Grained Structural Search over Source Code},
  year       = {2009},
  address    = {USA},
  pages      = {37–40},
  publisher  = {IEEE Computer Society},
  series     = {SUITE '09},
  abstract   = {Searching is an important activity in software maintenance. Dedicated data structures have been used to support either textual or structural queries over source code. The goal of this ongoing research is to elaborate a hybrid data storage that enables simultaneous textual and structural search. The naive adjacency list method has been combined with the inverted index approach. The data model has been enhanced with the use of recent data compression approaches for column-oriented databases to allow no-loss albeit compact storage of fine-grained structural data. The graph indexing has enabled the proposed data model to expeditiously answer fine-grained structural queries. This paper describes the basics of the proposed approach and estimates its feasibility.},
  doi        = {10.1109/SUITE.2009.5070019},
  file       = {:10.1109_SUITE.2009.5070019 - Hybrid Storage for Enabling Fully Featured Text Search and Fine Grained Structural Search Over Source Code.pdf:PDF},
  isbn       = {9781424437405},
  keywords   = {skimmed},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1109/SUITE.2009.5070019},
}

@InProceedings{Frey2013,
  author     = {Frey, Tim and Gräf, Matthias},
  booktitle  = {{SOFSEM} 2013: {Theory} and {Practice} of {Computer} {Science}},
  title      = {Hypermodelling {Reporting}: {Towards} {Cockpits} for {Code} {Structure}},
  year       = {2013},
  address    = {Berlin, Heidelberg},
  editor     = {van Emde Boas, Peter and Groen, Frans C. A. and Italiano, Giuseppe F. and Nawrocki, Jerzy and Sack, Harald},
  pages      = {395--407},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Do we have a vendor lock in? How many classes of a framework do we extend in our code? These questions may be asked by software development managers. In order to reveal such facts, a lot of effort is needed. We present the Hypermodelling approach to build software cockpits and use it to reduce the effort. We show a cockpit for the variance of software that is reflecting facts about the inheritance of types. We reveal a schematic cockpit view and evaluate the effort to implement it. Project managers can now use a cockpit to investigate software variances more easily. This also enables easy investigations of dependencies on frameworks. Important indicators about variance can now be investigated at a central spot. This avoids costly, time-consuming and deep investigations in the first place. Further research can reveal additional cockpits for other roles to cover the whole development cycle. Furthermore, the reasonable effort to create such cockpits enables the possibility to create different kinds of cockpits and evaluate or compare the usage of those.},
  doi        = {10.1007/978-3-642-35843-2_34},
  file       = {:Frey2013 - Hypermodelling Reporting_ Towards Cockpits for Code Structure.pdf:PDF;:10.1007_978-3-642-35843-2(1).pdf:PDF},
  isbn       = {9783642358432},
  keywords   = {hypermodelling , software dependency , project controlling , skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {Hypermodelling {Reporting}},
}

@InProceedings{10.1145/2970276.2970329,
  author     = {Asenov, Dimitar and M\"{u}ller, Peter and Vogel, Lukas},
  booktitle  = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
  title      = {The IDE as a Scriptable Information System},
  year       = {2016},
  address    = {New York, NY, USA},
  pages      = {444–449},
  publisher  = {Association for Computing Machinery},
  series     = {ASE 2016},
  abstract   = {Software engineering is extremely information-intensive. Every day developers work with source code, version repositories, issue trackers, documentation, web-based and other information resources. However, three key aspects of information work lack good support: (i) combining information from different sources; (ii) flexibly presenting collected information to enable easier comprehension; and (iii) automatically acting on collected information, for example to perform a refactoring. Poor support for these activities makes many common development tasks time-consuming and error-prone. We propose an approach that directly addresses these three issues by integrating a flexible query mechanism into the development environment. Our approach enables diverse ways to process and visualize information and can be extended via scripts. We demonstrate how an implementation of the approach can be used to rapidly write queries that meet a wide range of information needs.},
  doi        = {10.1145/2970276.2970329},
  file       = {:C\:/Users/Wernsen/Downloads/2970276.2970329.pdf:PDF},
  isbn       = {9781450338455},
  keywords   = {software visualization, refactoring, code queries, skimmed},
  location   = {Singapore, Singapore},
  numpages   = {6},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2970276.2970329},
}

@InProceedings{10.1145/1297846.1297936,
  author     = {Verbaere, Mathieu and Hajiyev, Elnar and De Moor, Oege},
  booktitle  = {Companion to the 22nd ACM SIGPLAN Conference on Object-Oriented Programming Systems and Applications Companion},
  title      = {Improve Software Quality with SemmleCode: An Eclipse Plugin for Semantic Code Search},
  year       = {2007},
  address    = {New York, NY, USA},
  pages      = {880–881},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '07},
  abstract   = {Navigate code, find bugs, compute metrics, check style rules, and enforce coding conventions in Eclipse with SemmleCode. SemmleCode is a new free Eclipse plugin that allows you to phrase these tasks as queries over the codebase - it thus takes the search facilities in Eclipse to a whole new level. A large library of queries for common operations is provided, including metrics and Java EE style rules. Query results can be displayed as a tree view, a table view, in the problem view, as charts or graphs, all with links to the source code.},
  doi        = {10.1145/1297846.1297936},
  file       = {:10.1145_1297846.1297936 - Improve Software Quality with SemmleCode_ an Eclipse Plugin for Semantic Code Search.pdf:PDF},
  isbn       = {9781595938657},
  keywords   = {object-oriented query language, metrics, coding conventions, code queries, code search, style rules, skimmed},
  location   = {Montreal, Quebec, Canada},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1297846.1297936},
}

@Article{Marin2009,
  author     = {Marin, Marius and van Deursen, Arie and Moonen, Leon and van der Rijst, Robin},
  journal    = {Automated Software Engineering},
  title      = {An integrated crosscutting concern migration strategy and its semi-automated application to {JHotDraw}},
  year       = {2009},
  issn       = {1573-7535},
  month      = jun,
  number     = {2},
  pages      = {323--356},
  volume     = {16},
  abstract   = {In this paper we propose a systematic strategy for migrating crosscutting concerns in existing object-oriented systems to aspect-oriented programming solutions. The proposed strategy consists of four steps: mining, exploration, documentation and refactoring of crosscutting concerns. We discuss in detail a new approach to refactoring to aspect-oriented programming that is fully integrated with our strategy, and apply the whole strategy to an object-oriented system, namely the JHotDraw framework.},
  doi        = {10.1007/s10515-009-0051-2},
  file       = {:Marin2009 - An Integrated Crosscutting Concern Migration Strategy and Its Semi Automated Application to JHotDraw.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1007/s10515-009-0051-2},
  urldate    = {2021-06-09},
}

@InProceedings{10.1145/1409720.1409732,
  author     = {Telea, Alexandru and Voinea, Lucian},
  booktitle  = {Proceedings of the 4th ACM Symposium on Software Visualization},
  title      = {An Interactive Reverse Engineering Environment for Large-Scale C++ Code},
  year       = {2008},
  address    = {New York, NY, USA},
  pages      = {67–76},
  publisher  = {Association for Computing Machinery},
  series     = {SoftVis '08},
  abstract   = {Few toolsets for reverse-engineering and understanding of C++ code provide parsing and fact extraction, querying, analysis and code metrics, navigation, and visualization of source-code-level facts in a way which is as easy-to-use as integrated development environments (IDEs) are for forward engineering. We present an interactive reverse-engineering environment (IRE) for C and C++ which allows to set up the fact extraction process, apply user-written queries and metrics, and visualize combined query results, metrics, code text, and code structure. Our IRE tightly couples a fast, tolerant C++ fact extractor, an open query system, and several scalable dense-pixel visualizations in a novel way, offering an easy way to analyze and examine large code bases. We illustrate our IRE with several examples, focusing on the added value of the integrated, visual reverse-engineering approach.},
  doi        = {10.1145/1409720.1409732},
  file       = {:10.1145_1409720.1409732 - An Interactive Reverse Engineering Environment for Large Scale C++ Code.pdf:PDF},
  isbn       = {9781605581125},
  keywords   = {skimmed},
  location   = {Ammersee, Germany},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1409720.1409732},
}

@InProceedings{10.1145/1013963.1013964,
  author     = {Beyer, Dirk and Chlipala, Adam J. and Henzinger, Thomas A. and Jhala, Ranjit and Majumdar, Rupak},
  booktitle  = {Proceedings of the 6th ACM SIGPLAN International Conference on Principles and Practice of Declarative Programming},
  title      = {Invited Talk: The Blast Query Language for Software Verification},
  year       = {2004},
  address    = {New York, NY, USA},
  pages      = {1–2},
  publisher  = {Association for Computing Machinery},
  series     = {PPDP '04},
  abstract   = {Blast is an automatic verification tool for checking temporal safety properties of C programs. Blast is based on lazy predicate abstraction driven by interpolation-based predicate discovery. The Blast specification language specifies program properties at two levels of precision. At the lower level, monitor automata are used to specify temporal safety properties of program executions (traces). At the higher level, relational reachability queries over program locations are used to combine lower-level trace properties. The two-level specification language can be used to break down a verification task into several independent calls of the model-checking engine. In this way, each call to the model checker may have to analyze only part of the program, or part of the specification, and may thus succeed in a reduction of the number of predicates needed for the analysis. In addition, the two-level specification language provides a means for structuring and maintaining specifications.},
  doi        = {10.1145/1013963.1013964},
  file       = {:10.1145_1013963.1013964 - Invited Talk_ the Blast Query Language for Software Verification.pdf:PDF},
  isbn       = {1581138199},
  keywords   = {software verification, software specification, skimmed},
  location   = {Verona, Italy},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1013963.1013964},
}

@InProceedings{10.1145/2856636.2856645,
  author     = {Saxena, Amitabh and Soundrapandian, Pradeepkumar Duraisamy and Sharma, Vibhu Saujanya and Kaulgud, Vikrant},
  booktitle  = {Proceedings of the 9th India Software Engineering Conference},
  title      = {JDQL: A Framework for Java Static Analysis},
  year       = {2016},
  address    = {New York, NY, USA},
  pages      = {136–140},
  publisher  = {Association for Computing Machinery},
  series     = {ISEC '16},
  abstract   = {In recent years, Datalog has been used in static analysis to detect bugs and security vulnerabilities. We present a Java Static Analysis framework based on Datalog called JDQL, which allows us to find buggy code.},
  doi        = {10.1145/2856636.2856645},
  file       = {:10.1145_2856636.2856645 - JDQL_ a Framework for Java Static Analysis.pdf:PDF},
  isbn       = {9781450340182},
  keywords   = {skimmed},
  location   = {Goa, India},
  numpages   = {5},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2856636.2856645},
}

@Article{10.1145/1167515.1167481,
  author     = {Cohen, Tal and Gil, Joseph (Yossi) and Maman, Itay},
  journal    = {SIGPLAN Not.},
  title      = {JTL: The Java Tools Language},
  year       = {2006},
  issn       = {0362-1340},
  month      = oct,
  number     = {10},
  pages      = {89–108},
  volume     = {41},
  abstract   = {We present an overview of JTL (the Java Tools Language, pronounced "Gee-tel"), a novel language for querying JAVA [8] programs. JTL was designed to serve the development of source code software tools for JAVA, and as a small language which to aid programming language extensions to JAVA. Applications include definition of pointcuts for aspect-oriented programming, fixing type constraints for generic programming, specification of encapsulation policies, definition of micro-patterns, etc. We argue that the JTL expression of each of these is systematic, concise, intuitive and general.JTL relies on a simply-typed relational database for program representation, rather than an abstract syntax tree. The underlying semantics of the language is restricted to queries formulated in First Order Predicate Logic augmented with transitive closure (FOPL).Special effort was taken to ensure terse, yet readable expression of logical conditions. The JTL pattern <b>public abstract class</b>, for example, matches all abstract classes which are publicly accessible, while <b>class</b> (<b>public</b> clone();) matches all classes in which method clone is public. To this end, JTL relies on a DATALOG-like syntax and semantics, enriched with quantifiers and pattern matching which all but entirely eliminate the need for recursive calls.JTL's query analyzer gives special attention to the fragility of the "closed world assumption" in examining JAVA software, and determines whether a query relies on such an assumption.The performance of the JTL interpreter is comparable to that of JQuery after it generated its database cache, and at least an order of magnitude faster when the cache has to be rebuilt.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1167515.1167481},
  file       = {:10.1145_1167515.1167481 - JTL_ the Java Tools Language.pdf:PDF},
  issue_date = {October 2006},
  keywords   = {reverse engineering, declarative programming, skimmed},
  numpages   = {20},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1167515.1167481},
}

@InProceedings{10.1145/1134285.1134311,
  author     = {Verbaere, Mathieu and Ettinger, Ran and de Moor, Oege},
  booktitle  = {Proceedings of the 28th International Conference on Software Engineering},
  title      = {JunGL: A Scripting Language for Refactoring},
  year       = {2006},
  address    = {New York, NY, USA},
  pages      = {172–181},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '06},
  abstract   = {Refactorings are behaviour-preserving program transformations, typically for improving the structure of existing code. A few of these transformations have been mechanised in interactive development environments. Many more refactorings have been proposed, and it would be desirable for programmers to script their own refactorings. Implementing such source-to-source transformations, however, is quite complex: even the most sophisticated development environments contain significant bugs in their refactoring tools.We present a domain-specific language for refactoring, named JunGL. It manipulates a graph representation of the program: all information about the program, including ASTs for its compilation units, variable binding, control flow and so on is represented in a uniform graph format. The language is a hybrid of a functional language (in the style of ML) and a logic query language (akin to Datalog). JunGL furthermore has a notion of demand-driven evaluation for constructing computed information in the graph, such as control flow edges. Borrowing from earlier work on the specification of compiler optimisations, JunGL uses so-called `path queries' to express dataflow properties.We motivate the design of JunGL via a number of non-trivial refactorings, and describe its implementation on the.NET platform.},
  doi        = {10.1145/1134285.1134311},
  file       = {:10.1145_1134285.1134311 - JunGL_ a Scripting Language for Refactoring.pdf:PDF},
  isbn       = {1595933751},
  keywords   = {source code transformation, refactoring, scripting language, language workbenches, skimmed},
  location   = {Shanghai, China},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1134285.1134311},
}

@Article{10.1145/1837852.1621617,
  author     = {Liu, Yanhong A. and Gorbovitski, Michael and Stoller, Scott D.},
  journal    = {SIGPLAN Not.},
  title      = {A Language and Framework for Invariant-Driven Transformations},
  year       = {2009},
  issn       = {0362-1340},
  month      = oct,
  number     = {2},
  pages      = {55–64},
  volume     = {45},
  abstract   = {This paper describes a language and framework that allow coordinated transformations driven by invariants to be specified declaratively, as invariant rules, and applied automatically. The framework supports incremental maintenance of invariants for program design and optimization, as well as general transformations for instrumentation, refactoring, and other purposes. This paper also describes our implementations for transforming Python and C programs and experiments with successful applications of the systems in generating efficient implementations from clear and modular specifications, in instrumenting programs for runtime verification, profiling, and debugging, and in code refactoring.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1837852.1621617},
  file       = {:10.1145_1837852.1621617 - A Language and Framework for Invariant Driven Transformations.pdf:PDF},
  issue_date = {February 2010},
  keywords   = {runtime invariant checking, incremental maintenance, program optimization, program transformation, invariants, skimmed},
  numpages   = {10},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1837852.1621617},
}

@Article{Rasool2017,
  author     = {Rasool, Ghulam and Arshad, Zeeshan},
  journal    = {Arabian Journal for Science and Engineering},
  title      = {A {Lightweight} {Approach} for {Detection} of {Code} {Smells}},
  year       = {2017},
  issn       = {2191-4281},
  month      = feb,
  number     = {2},
  pages      = {483--506},
  volume     = {42},
  abstract   = {The accurate removal of code smells from source code supports activities such as refactoring, maintenance, examining code quality etc. A large number of techniques and tools are presented for the specification and detection of code smells from source code in the last decade, but they still lack accuracy and flexibility due to different interpretations of code smell definitions. Most techniques target just detection of few code smells and render different results on the same examined systems due to different informal definitions and threshold values of metrics used for detecting code smells. We present a flexible and lightweight approach based on multiple searching techniques for the detection and visualization of all 22 code smells from source code of multiple languages. Our approach is lightweight and flexible due to application of SQL queries on intermediate repository and use of regular expressions on selected source code constructs. The concept of approach is validated by performing experiments on eight publicly available open source software projects developed using Java and C\# programming languages, and results are compared with existing approaches. The accuracy of presented approach varies from 86–97 \% on the eight selected software projects.},
  doi        = {10.1007/s13369-016-2238-8},
  file       = {:Rasool2017 - A Lightweight Approach for Detection of Code Smells.pdf:PDF;:rasool2016.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1007/s13369-016-2238-8},
  urldate    = {2021-06-09},
}

@Article{Canfora1992,
  author     = {Canfora, G. and Cimitile, A. and de Carlini, U.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {A logic-based approach to reverse engineering tools production},
  year       = {1992},
  issn       = {1939-3520},
  month      = dec,
  number     = {12},
  pages      = {1053--1064},
  volume     = {18},
  abstract   = {Difficulties arising in the use of documents produced by reverse engineering tools are analyzed. With reference to intermodular data flow analysis for Pascal software systems, an interactive and evolutionary tool is proposed. The tool is based on the production of intermodular data flow information by static analysis of code, its representation in a Prolog program dictionary, and a Prolog abstractor that allows the specific queries to be answered.{\textless}{\textgreater}},
  doi        = {10.1109/32.184760},
  file       = {:Canfora1992 - A Logic Based Approach to Reverse Engineering Tools Production.pdf:PDF},
  keywords   = {Reverse engineering, Production, Data analysis, Software systems, Information analysis, Dictionaries, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1960518.1960522,
  author     = {Mitschke, Ralf and Sewe, Andreas and Mezini, Mira},
  booktitle  = {Proceedings of the 1st Workshop on Modularity in Systems Software},
  title      = {Magic for the Masses: Safer High-Level Low-Level Programming through Customizable Static Analyses},
  year       = {2011},
  address    = {New York, NY, USA},
  pages      = {13–17},
  publisher  = {Association for Computing Machinery},
  series     = {MISS '11},
  abstract   = {Writing high-performance virtual machines in a high-level language requires an escape-hatch, such that unavoidable low-level tasks can be performed efficiently. To this end, the org.vmmagic framework used by Jikes RVM and other VMs makes it possible to extend the Java language with the needed low-level facilities. For these facilities and the constraints they impose, though, tool support is almost nonexistent, making it difficult for implementers not to violate the additional constraints imposed by the language extension. We thus propose an declarative approach based on customizable static analyses to make specification and checking of these constraints easily accessible to implementers},
  doi        = {10.1145/1960518.1960522},
  file       = {:10.1145_1960518.1960522 - Magic for the Masses_ Safer High Level Low Level Programming through Customizable Static Analyses.pdf:PDF},
  isbn       = {9781450306478},
  keywords   = {static analyses, Java annotations, Jikes RVM, high-level low-level programming, skimmed},
  location   = {Porto de Galinhas, Brazil},
  numpages   = {5},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1960518.1960522},
}

@Article{Oman1990,
  author     = {Oman, P. and Novobilski, A. and Rajlich, V. and Harband, J. and McCabe, T. and Cross, J. and Vanek, L. and Davis, L. and Gallagher, K. and Wilde, N.},
  journal    = {IEEE Software},
  title      = {Maintenance tools},
  year       = {1990},
  issn       = {1937-4194},
  month      = may,
  number     = {3},
  pages      = {59--65},
  volume     = {7},
  abstract   = {After a brief overview, eight tools to help the maintenance programmer analyze and understand code are described in separate presentations. All of them are code-visualization tools. However, while all these tools show how a program is structured, they use different means to achieve different ends. The tools covered are: Objective-C Browser; Vifor; Seela; Battle Map; Act; Grasp/Ada; Expert Dataflow and Static Analysis; Surgeon's Assistant; and Dependency Analysis Tool Set;.{\textless}{\textgreater}},
  doi        = {10.1109/52.55229},
  file       = {:C\:/Users/Wernsen/Downloads/oman1990.pdf:PDF},
  keywords   = {Sun, Workstations, Costs, Skeleton, Displays, Graphical user interfaces, Debugging, Software tools, Visual databases, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1858996.1859091,
  author     = {Wang, Xiaoyin and Lo, David and Cheng, Jiefeng and Zhang, Lu and Mei, Hong and Yu, Jeffrey Xu},
  booktitle  = {Proceedings of the IEEE/ACM International Conference on Automated Software Engineering},
  title      = {Matching Dependence-Related Queries in the System Dependence Graph},
  year       = {2010},
  address    = {New York, NY, USA},
  pages      = {457–466},
  publisher  = {Association for Computing Machinery},
  series     = {ASE '10},
  abstract   = {In software maintenance and evolution, it is common that developers want to apply a change to a number of similar places. Due to the size and complexity of the code base, it is challenging for developers to locate all the places that need the change. A main challenge in locating the places that need the change is that, these places share certain common dependence conditions but existing code searching techniques can hardly handle dependence relations satisfactorily. In this paper, we propose a technique that enables developers to make queries involving dependence conditions and textual conditions on the system dependence graph of the program. We carried out an empirical evaluation on four searching tasks taken from the development history of two real-world projects. The results of our evaluation indicate that, compared with code-clone detection, our technique is able to locate many required code elements that code-clone detection cannot locate, and compared with text search, our technique is able to effectively reduce false positives without losing any required code elements.},
  doi        = {10.1145/1858996.1859091},
  file       = {:C\:/Users/Wernsen/Downloads/1858996.1859091.pdf:PDF},
  isbn       = {9781450301169},
  keywords   = {code search, graph indexing, system dependence graph, skimmed},
  location   = {Antwerp, Belgium},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1858996.1859091},
}

@InProceedings{10.1145/1791194.1791200,
  author     = {Williams, Daniel and Davidson, Jack W.},
  booktitle  = {Proceedings of the Workshop on Binary Instrumentation and Applications},
  title      = {Metaman: System-Wide Metadata Management},
  year       = {2009},
  address    = {New York, NY, USA},
  pages      = {34–42},
  publisher  = {Association for Computing Machinery},
  series     = {WBIA '09},
  abstract   = {Understanding how programs are created and how they behave at run time is vital to building secure and efficient programs. Typically program information generated when building and linking a program is not available to run-time instrumentation tools that are used to better understand and improve program behavior. This paper presents the structure of Metaman, the metadata manager, a tool for building instrumented systems that spans the entire build and run-time system. Metaman collects available metadata about the program from build tools as well as run-time tools as XML, and then makes the metadata available to the rest of the tools in the toolchain. In order for Metaman to be practical, it must be easy for each tool to send and receive metadata from Metaman. This paper discusses: (1) the process of adapting of build-time tools such as compilers, assemblers and static analyzers for use with Metaman, (2) the process of adapting the build system to assist in correctly coordinating novel metadata, and finally (3) the process of integrating run-time development tools such as debuggers, instrumentation tools, and software dynamic translators (SDT) for use with Metaman.},
  doi        = {10.1145/1791194.1791200},
  file       = {:10.1145_1791194.1791200 - Metaman_ System Wide Metadata Management.pdf:PDF},
  isbn       = {9781605587936},
  keywords   = {metadata, software dynamic translation, software toolchain, run-time systems, skimmed},
  location   = {New York, New York, USA},
  numpages   = {9},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1791194.1791200},
}

@InProceedings{10.1145/1176617.1176632,
  author     = {Kurtev, Ivan and B\'{e}zivin, Jean and Jouault, Fr\'{e}d\'{e}ric and Valduriez, Patrick},
  booktitle  = {Companion to the 21st ACM SIGPLAN Symposium on Object-Oriented Programming Systems, Languages, and Applications},
  title      = {Model-Based DSL Frameworks},
  year       = {2006},
  address    = {New York, NY, USA},
  pages      = {602–616},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '06},
  abstract   = {More than five years ago, the OMG proposed the Model Driven Architecture (MDA™) approach to deal with the separation of platform dependent and independent aspects in information systems. Since then, the initial idea of MDA evolved and Model Driven Engineering (MDE) is being increasingly promoted to handle separation and combination of various kinds of concerns in software or data engineering. MDE is more general than the set of standards and practices recommended by the OMG's MDA proposal. In MDE the concept of model designates not only OMG models but a lot of other artifacts like XML documents, Java programs, RDBMS data, etc. Today we observe another evolutionary step. A convergence between MDE and DSL (Domain Specific Language) engineering is rapidly appearing. In the same way as MDE is a generalization of MDA, the DSL engineering may be viewed as a generalization of MDE. One of the goals of this paper is to explore the potential of this important evolution of engineering practices. In order to anchor the discussion on practical grounds, we present a set of typical problems that could be solved by classical (object-oriented and others), MDE, or DSL-based techniques. Solutions to these problems will be based on current platforms (EMF, AMMA, GME, etc.). This paper illustrates how powerful model-based frameworks, allowing to use and build a variety of DSLs, may help to solve complex problems in a more efficient way.},
  doi        = {10.1145/1176617.1176632},
  file       = {:10.1145_1176617.1176632 - Model Based DSL Frameworks.pdf:PDF},
  isbn       = {159593491X},
  keywords   = {tool-based approaches, model-driven engineering, MDA, DSL engineering, skimmed},
  location   = {Portland, Oregon, USA},
  numpages   = {15},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1176617.1176632},
}

@InProceedings{10.1145/1117696.1117711,
  author     = {Majid, Imran and Robillard, Martin P.},
  booktitle  = {Proceedings of the 2005 OOPSLA Workshop on Eclipse Technology EXchange},
  title      = {NaCIN: An Eclipse Plug-in for Program Navigation-Based Concern Inference},
  year       = {2005},
  address    = {New York, NY, USA},
  pages      = {70–74},
  publisher  = {Association for Computing Machinery},
  series     = {eclipse '05},
  abstract   = {In this paper we describe NaCIN, an Eclipse plug-in that records a developer's code navigation activity and produces sets of elements potentially implementing different concerns relevant to the current task. It performs an analysis of the navigation paths and structural dependencies of the recorded elements and clusters the results in groups potentially associated with high level concepts. NaCIN partially automates the process of relating source code with high-level abstractions and enables knowledge about the implementation of different concerns to be reused in future investigations. We present the architecture and a preliminary assessment of NaCIN.},
  doi        = {10.1145/1117696.1117711},
  file       = {:10.1145_1117696.1117711 - NaCIN_ an Eclipse Plug in for Program Navigation Based Concern Inference.pdf:PDF},
  isbn       = {1595933425},
  keywords   = {concern inference, program navigation, concern modeling, program investigation, skimmed},
  location   = {San Diego, California},
  numpages   = {5},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1117696.1117711},
}

@InProceedings{10.1145/643603.643622,
  author     = {Janzen, Doug and De Volder, Kris},
  booktitle  = {Proceedings of the 2nd International Conference on Aspect-Oriented Software Development},
  title      = {Navigating and Querying Code without Getting Lost},
  year       = {2003},
  address    = {New York, NY, USA},
  pages      = {178–187},
  publisher  = {Association for Computing Machinery},
  series     = {AOSD '03},
  abstract   = {A development task related to a crosscutting concern is challenging because a developer can easily get lost when exploring scattered elements of code and the complex tangle of relationships between them. In this paper we present a source browsing tool that improves the developer's ability to work with crosscutting concerns by providing better support for exploring code. Our tool helps the developer to remain oriented while exploring and navigating across a code base. The cognitive burden placed on a developer is reduced by avoiding disorienting view switches and by providing an explicit representation of the exploration process in terms of exploration paths. While our tool is generally useful, good navigation support is particularly important when exploring crosscutting concerns.},
  doi        = {10.1145/643603.643622},
  file       = {:10.1145_643603.643622 - Navigating and Querying Code without Getting Lost.pdf:PDF;:3275219.3275221.pdf:PDF},
  isbn       = {1581136609},
  keywords   = {skimmed},
  location   = {Boston, Massachusetts},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/643603.643622},
}

@InProceedings{10.1145/1595808.1595827,
  author     = {P\'{e}rez, Javier and Crespo, Yania},
  booktitle  = {Proceedings of the Joint International and Annual ERCIM Workshops on Principles of Software Evolution (IWPSE) and Software Evolution (Evol) Workshops},
  title      = {Perspectives on Automated Correction of Bad Smells},
  year       = {2009},
  address    = {New York, NY, USA},
  pages      = {99–108},
  publisher  = {Association for Computing Machinery},
  series     = {IWPSE-Evol '09},
  abstract   = {Keeping a software system conformant with a desired architecture and consistent with good design principles is a recurring task during the software evolution process. Deviations from good design principles can manifest in the form of bad smells: problems in the system's structure that can negatively affect software quality factors.Many authors have worked in identifying bad smells and in removing them with refactorings: tools have been built to suggest refactorings; successful approaches to detect bad smells have been developed, etc.. We present a comprehensive and historical review on this subject, in order to model the current state of the art and to identify the open challenges, current trends and research opportunities.We also propose a technique based on automated planning, aimed at taking one step forward in the automatic improvement of a system's structure. This proposal will allow computing complex refactoring sequences which can be directed to the achievement of a certain objective, such as the correction of bad smells.},
  doi        = {10.1145/1595808.1595827},
  file       = {:10.1145_1595808.1595827 - Perspectives on Automated Correction of Bad Smells.pdf:PDF},
  isbn       = {9781605586786},
  keywords   = {refactoring, bad smells, automated planning, skimmed},
  location   = {Amsterdam, The Netherlands},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1595808.1595827},
}

@InProceedings{Jarzabek1995,
  author     = {Jarzabek, Stan},
  booktitle  = {Software {Engineering} — {ESEC} '95},
  title      = {{PQL}: {A} language for specifying abstract program views},
  year       = {1995},
  address    = {Berlin, Heidelberg},
  editor     = {Schäfer, Wilhelm and Botella, Pere},
  pages      = {324--342},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {A program query language, PQL for short, described in this paper is a source language-independent notation to specify program queries and program views. We use PQL as an interface to Static Program Analyzers (SPA), interactive tools that enhance program understanding by answering queries about programs. In PQL, we can query on global program design as well as search for detail code patterns. Program queries and patterns supported by other notations described in literature and those supported by commercial tools known to the author, can be written simply and naturally in PQL. Program modeling and PQL notations described in the paper form a basis for an SPA generation system. These notations also allow us to rigorously study tool capabilities in the context of underlying software maintenance process and programmer's behavior models.},
  doi        = {10.1007/3-540-60406-5_23},
  file       = {:Jarzabek1995 - PQL_ a Language for Specifying Abstract Program Views.pdf:PDF;:f37a9d5b833ca67e4817aec9c7b1e2a6.pdf:PDF},
  isbn       = {9783540455523},
  keywords   = {Reverse Engineering , Syntax Tree , Program Information , Grammar Rule , Code Pattern , skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {{PQL}},
}

@InProceedings{Panchenko2011a,
  author     = {Panchenko, Oleksandr and Karstens, Jan and Plattner, Hasso and Zeier, Alexander},
  booktitle  = {2011 {IEEE} 19th {International} {Conference} on {Program} {Comprehension}},
  title      = {Precise and {Scalable} {Querying} of {Syntactical} {Source} {Code} {Patterns} {Using} {Sample} {Code} {Snippets} and a {Database}},
  year       = {2011},
  month      = jun,
  note       = {ISSN: 1092-8138},
  pages      = {41--50},
  abstract   = {While analyzing a log file of a text-based source code search engine we discovered that developers search for fine-grained syntactical patterns in 36\% of queries. Currently, to cope with queries of this kind developers need to use regular expressions, to add redundant terms to the query or to combine searching with other tools provided by the development environment. To improve the expressiveness of the queries, these can be formulated as tree patterns of abstract syntax trees. These search patterns can be expressed by using query languages, such as XPath. However, developers usually do not work with either XPath or with AST. To shield developers from the complexity of query formulation we propose using sample code snippets as queries. The novelty of our approach is the combination of a query language that is very close to the surface programming language and a special database technology to store a large amount of abstract syntax trees. The advantage of this approach over existing source code query languages and search engines is the performance of both query formulation and query execution. This paper describes the technical details of the method and illustrates the value of this approach with performance measures and an industrial controlled experiment. All developers were able to complete the tasks of the experiment faster and more accurately by using our tool (ACS) than by using a text-based search engine. The number of false positives in the result lists was significantly decreased.},
  doi        = {10.1109/ICPC.2011.31},
  file       = {:C\:/Users/Wernsen/Downloads/Panchenko2011a - Precise and Scalable Querying of Syntactical Source Code Patterns Using Sample Code Snippets and a Database.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Databases, Database languages, Search engines, XML, Data models, Syntactics, source code search, query-by-example, source code query language, abstract syntax trees, XPath, database, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1119655.1119677,
  author     = {Coelho, Wesley and Murphy, Gail C.},
  booktitle  = {Proceedings of the 5th International Conference on Aspect-Oriented Software Development},
  title      = {Presenting Crosscutting Structure with Active Models},
  year       = {2006},
  address    = {New York, NY, USA},
  pages      = {158–168},
  publisher  = {Association for Computing Machinery},
  series     = {AOSD '06},
  abstract   = {When modifying or debugging a software system, among other tasks, developers must often understand and manipulate source code that crosscuts the system's structure. These tasks are made more difficult by limitations of the two approaches currently used to present details of crosscutting structure: tree views and structural diagrams. Tree views force the developer to manually synthesize information from multiple views; structure diagrams quickly suffer from graphical complexity. We introduce an active model as a means of presenting the right information about crosscutting structure to a developer at the right time. An active model is produced as a result of three automated operations---projection, expansion, and abstraction. Combined with particular user interaction features during display, these operations enable a view of the model to be presented to the developer without suffering from the complexity of existing approaches. We have implemented an active model tool, called ActiveAspect, for presenting crosscutting structure described by AspectJ aspects. We report on the results of a case study in which the tool was used effectively by two subjects to implement a modification task to a non-trivial AspectJ system.},
  doi        = {10.1145/1119655.1119677},
  file       = {:10.1145_1119655.1119677 - Presenting Crosscutting Structure with Active Models.pdf:PDF},
  isbn       = {159593300X},
  keywords   = {structure presentation, aspect-oriented programming, AspectJ, program structure, design views, skimmed},
  location   = {Bonn, Germany},
  numpages   = {11},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1119655.1119677},
}

@InProceedings{Noguera2011,
  author     = {Noguera, Carlos and Roover, Coen De and Kellens, Andy and Jonckers, Viviane},
  booktitle  = {2011 27th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})},
  title      = {Program querying with a {SOUL}: {The} {BARISTA} tool suite},
  year       = {2011},
  month      = sep,
  note       = {ISSN: 1063-6773},
  pages      = {582--585},
  abstract   = {Extracting information from the source code of a program is an important step in the way to program understanding, manipulation, development and maintenance. To this end, logic-based query languages provide a declarative manner in which to identify program elements of interest. In this paper we present BARISTA, a tool-suite for querying Java programs based on the Smalltalk Open Unification Language (SOUL). BARISTA offers programmers an advanced IDE to write queries and navigate their results. Tool builders can benefit from SOUL querying facilities by exploiting the on demand code querying and query scheduling services offered by BARISTA.},
  doi        = {10.1109/ICSM.2011.6080835},
  file       = {:Noguera2011 - Program Querying with a SOUL_ the BARISTA Tool Suite.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {Java, Unified modeling language, Syntactics, Database languages, Semantics, Libraries, Engines, skimmed},
  readstatus = {skimmed},
  shorttitle = {Program querying with a {SOUL}},
}

@Article{Paul1996,
  author     = {Paul, S. and Prakash, A.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {A query algebra for program databases},
  year       = {1996},
  issn       = {1939-3520},
  month      = mar,
  number     = {3},
  pages      = {202--217},
  volume     = {22},
  abstract   = {Querying source code is an essential aspect of a variety of software engineering tasks such as program understanding, reverse engineering, program structure analysis and program flow analysis. In this paper, we present and demonstrate the use of an algebraic source code query technique that blends expressive power with query compactness. The query framework of Source Code Algebra (SCA) permits users to express complex source code queries and views as algebraic expressions. Queries are expressed on an extensible, object-oriented database that stores program source code. The SCA algebraic approach offers multiple benefits such as an applicative query language, high expressive power, seamless handling of structural and flow information, clean formalism and potential for query optimization. We present a case study where SCA expressions are used to query a program in terms of program organization, resource flow, control flow, metrics and syntactic structure. Our experience with an SCA-based prototype query processor indicates that an algebraic approach to source code queries combines the benefits of expressive power and compact query formulation.},
  doi        = {10.1109/32.489080},
  file       = {:C\:/Users/Wernsen/Downloads/Paul1996 - A Query Algebra for Program Databases.pdf:PDF},
  keywords   = {Algebra, Data mining, Software maintenance, Reverse engineering, Database languages, Software systems, Humans, Software tools, Software engineering, Documentation, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Kellens2011,
  author     = {Kellens, Andy and De Roover, Coen and Noguera, Carlos and Stevens, Reinout and Jonckers, Viviane},
  booktitle  = {2011 18th {Working} {Conference} on {Reverse} {Engineering}},
  title      = {Reasoning over the {Evolution} of {Source} {Code} {Using} {Quantified} {Regular} {Path} {Expressions}},
  year       = {2011},
  month      = oct,
  note       = {ISSN: 2375-5369},
  pages      = {389--393},
  abstract   = {Version control systems (VCS) have become indispensable to develop software. Next to their immediate advantages, they also offer information about the evolution of software and its development process. Despite this wealth of information, it has only been leveraged by tools that are dedicated to a specific software engineering task such as predicting bugs or identifying hotspots. General-purpose tool support for reasoning about the information contained in a version control system is limited. In this paper, we introduce the logic-based program query language ABSINTHE. It supports querying versioned software systems using logic queries in which quantified regular path expressions are embedded. These expressions lend themselves to specifying the properties that each individual version in a sequence of successive software versions ought to exhibit.},
  doi        = {10.1109/WCRE.2011.54},
  file       = {:Kellens2011 - Reasoning Over the Evolution of Source Code Using Quantified Regular Path Expressions.pdf:PDF},
  issn       = {2375-5369},
  keywords   = {History, Software, Cognition, Libraries, Control systems, Database languages, Software engineering, skimmed},
  readstatus = {skimmed},
}

@Article{10.1145/1082983.1083248,
  author     = {Bowdidge, Robert W.},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Refactoring Gcc Using Structure Field Access Traces and Concept Analysis},
  year       = {2005},
  issn       = {0163-5948},
  month      = may,
  number     = {4},
  pages      = {1–7},
  volume     = {30},
  abstract   = {Refactoring usually involves statically analyzing source code to understand which transformations safely preserve execution behavior of the program. However, static analysis may not scale well for large programs when analysis results are too general, when tools for analyzing the source code are unwieldy, or when the tools simply do not exist. In such cases, it can be simpler to analyze the program at runtime to gather answers needed for safe code changes. I show how dynamic data can guide refactoring of a single data structure into a hierarchy of classes. Specifically, I show how I refactored the gcc compiler to cut its use of heap memory. In order to partition the declaration data structure into more efficiently-sized parts, I used data structure field access traces to automatically identify how the data structure might be refactored. I also identified other potential refactorings of the data structure using concept analysis. These results then guided by-hand modifications to the compiler. I finally evaluated what size test cases would be needed to gather adequate information to correctly perform the refactoring. The case study showed the refactoring could be performed with the dynamic information, but without traces from an exhaustive set of test cases, some fields would be moved incorrectly.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1082983.1083248},
  file       = {:10.1145_1082983.1083248 - Refactoring Gcc Using Structure Field Access Traces and Concept Analysis.pdf:PDF},
  issue_date = {July 2005},
  keywords   = {meaning-preserving restructuring, case study, gcc, skimmed},
  numpages   = {7},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1082983.1083248},
}

@Article{10.1145/1086649.1086664,
  author     = {Afrati, Foto},
  journal    = {SIGACT News},
  title      = {Report on PODS 2005},
  year       = {2005},
  issn       = {0163-5700},
  month      = sep,
  number     = {3},
  pages      = {39–40},
  volume     = {36},
  address    = {New York, NY, USA},
  doi        = {10.1145/1086649.1086664},
  file       = {:10.1145_1086649.1086664 - Report on PODS 2005.pdf:PDF},
  issue_date = {September 2005},
  keywords   = {skimmed},
  numpages   = {2},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1086649.1086664},
}

@InProceedings{10.1145/225014.225046,
  author     = {Quilici, Alex},
  booktitle  = {Proceedings of the 17th International Conference on Software Engineering},
  title      = {Reverse Engineering of Legacy Systems: A Path toward Success},
  year       = {1995},
  address    = {New York, NY, USA},
  pages      = {333–336},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '95},
  doi        = {10.1145/225014.225046},
  file       = {:10.1145_225014.225046 - Reverse Engineering of Legacy Systems_ a Path toward Success.pdf:PDF},
  isbn       = {0897917081},
  keywords   = {skimmed},
  location   = {Seattle, Washington, USA},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/225014.225046},
}

@InProceedings{10.1145/225014.225032,
  author     = {Harris, David R. and Reubenstein, Howard B. and Yeh, Alexander S.},
  booktitle  = {Proceedings of the 17th International Conference on Software Engineering},
  title      = {Reverse Engineering to the Architectural Level},
  year       = {1995},
  address    = {New York, NY, USA},
  pages      = {186–195},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '95},
  doi        = {10.1145/225014.225032},
  file       = {:10.1145_225014.225032 - Reverse Engineering to the Architectural Level.pdf:PDF},
  isbn       = {0897917081},
  keywords   = {skimmed},
  location   = {Seattle, Washington, USA},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/225014.225032},
}

@InProceedings{10.1145/1297846.1297949,
  author     = {Antkiewicz, Michal},
  booktitle  = {Companion to the 22nd ACM SIGPLAN Conference on Object-Oriented Programming Systems and Applications Companion},
  title      = {Round-Trip Engineering Using Framework-Specific Modeling Languages},
  year       = {2007},
  address    = {New York, NY, USA},
  pages      = {927–928},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '07},
  abstract   = {This research explores the synergies between object-oriented application frameworks and model-driven engineering. We propose Framework-Specific Modeling Languages (FSMLs) which are domain-specific modeling languages designed for areas of concern to object-oriented frameworks. A framework-specific model expressed using an FSML describes how an application built on top of a framework is using the framework. The semantics of FSMLs can be precisely defined based on framework-completion knowledge: the prescribed steps and rules of writing the framework-completion code for the given framework. The mapping between the abstract syntax of an FSML and its base framework's API enables automatic forward-, reverse-, and round-trip engineering of thecompletion code.},
  doi        = {10.1145/1297846.1297949},
  file       = {:10.1145_1297846.1297949 - Round Trip Engineering Using Framework Specific Modeling Languages.pdf:PDF},
  isbn       = {9781595938657},
  keywords   = {framework-specific modeling, FSMLs, object-oriented frameworks, round-trip engineering, framework-specific modeling languages, skimmed},
  location   = {Montreal, Quebec, Canada},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1297846.1297949},
}

@InProceedings{10.1145/1176617.1176656,
  author     = {Verbaere, Mathieu and Payement, Arnaud and de Moor, Oege},
  booktitle  = {Companion to the 21st ACM SIGPLAN Symposium on Object-Oriented Programming Systems, Languages, and Applications},
  title      = {Scripting Refactorings with JunGL},
  year       = {2006},
  address    = {New York, NY, USA},
  pages      = {651–652},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '06},
  abstract   = {We describe JunGL, a language to script refactoring transformations. It manipulates a graph representation of the program, including extensible semantic information such as variable binding and dataflow. JunGL enables the full automation of complex refactorings: finding program elements of interest, checking preconditions and performing the transformation itself.},
  doi        = {10.1145/1176617.1176656},
  file       = {:10.1145_1176617.1176656 - Scripting Refactorings with JunGL.pdf:PDF},
  isbn       = {159593491X},
  keywords   = {refactoring, scripting language, language workbenches, source code transformation, skimmed},
  location   = {Portland, Oregon, USA},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1176617.1176656},
}

@InProceedings{10.1145/1328408.1328410,
  author     = {Lam, Monica S. and Martin, Michael and Livshits, Benjamin and Whaley, John},
  booktitle  = {Proceedings of the 2008 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation},
  title      = {Securing Web Applications with Static and Dynamic Information Flow Tracking},
  year       = {2008},
  address    = {New York, NY, USA},
  pages      = {3–12},
  publisher  = {Association for Computing Machinery},
  series     = {PEPM '08},
  abstract   = {SQL injection and cross-site scripting are two of the most common security vulnerabilities that plague web applications today. These and many others result from having unchecked data input reach security-sensitive operations. This paper describes a language called PQL (Program Query Language) that allows users to declare to specify information flow patterns succinctly and declaratively. We have developed a static context-sensitive, but flow-insensitive information flow tracking analysis that can be used to find all the vulnerabilities in a program. In the event that the analysis generates too many warnings, the result can be used to drive a model-checking system to analyze more precisely. Model checking is also used to automatically generate the input vectors that expose the vulnerability. Any remaining behavior these static analyses have not isolated may be checked dynamically. The results of the static analyses may be used to optimize these dynamic checks.Our experimental results indicate the language is expressive enough for describing a large number of vulnerabilities succinctly. We have analyzed over nine applications, detecting 30 serious security vulnerabilities. We were also able to automatically recover from attacks as they occurred using the dynamic checker.},
  doi        = {10.1145/1328408.1328410},
  file       = {:10.1145_1328408.1328410 - Securing Web Applications with Static and Dynamic Information Flow Tracking.pdf:PDF},
  isbn       = {9781595939777},
  keywords   = {cross-site scripting, web applications, SQL injection, model checking, dynamic analysis, static analysis, pattern matching, skimmed},
  location   = {San Francisco, California, USA},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1328408.1328410},
}

@Article{10.1145/1190215.1190221,
  author     = {Avgustinov, Pavel and Hajiyev, Elnar and Ongkingco, Neil and de Moor, Oege and Sereni, Damien and Tibble, Julian and Verbaere, Mathieu},
  journal    = {SIGPLAN Not.},
  title      = {Semantics of Static Pointcuts in AspectJ},
  year       = {2007},
  issn       = {0362-1340},
  month      = jan,
  number     = {1},
  pages      = {11–23},
  volume     = {42},
  abstract   = {In aspect-oriented programming, one can intercept events by writing patterns called pointcuts. The pointcut language of the most popular aspect-oriented programming language, AspectJ, allows the expression of highly complex properties of the static program structure.We present the first rigorous semantics of the AspectJ pointcut language, by translating static patterns into safe ( i.e. range-restricted and stratified) Datalog queries. Safe Datalog is a logic language like Prolog, but it does not have data structures; consequently it has a straightforward least fixpoint semantics and all queries terminate.The translation from pointcuts to safe Datalog consists of a set of simple conditional rewrite rules, implemented using the Stratego system. The resulting queries are themselves executable with the CodeQuest system. We present experiments indicating that direct execution of our semantics is not prohibitively expensive.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1190215.1190221},
  file       = {:10.1145_1190215.1190221 - Semantics of Static Pointcuts in AspectJ.pdf:PDF},
  issue_date = {January 2007},
  keywords   = {datalog, logic programming, aspect-oriented programming, pointcuts, term rewriting, skimmed},
  numpages   = {13},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1190215.1190221},
}

@Article{Meister2010,
  author     = {Meister, Jeffrey A. and Foster, Jeffrey S. and Hicks, Michael},
  journal    = {Software: Practice and Experience},
  title      = {Serializing {C} intermediate representations for efficient and portable parsing},
  year       = {2010},
  issn       = {1097-024X},
  number     = {3},
  pages      = {225--238},
  volume     = {40},
  abstract   = {C static analysis tools often use intermediate representations (IRs) that organize program data in a simple, well-structured manner. However, the C parsers that create IRs are slow, and because they are difficult to write, only a few implementations exist, limiting the languages in which a C static analysis can be written. To solve these problems, we investigate two language-independent, on-disk representations of C IRs: one using XML and the other using an Internet standard binary encoding called eXternal Data Representation (XDR). We benchmark the parsing speeds of both options, finding the XML to be about a factor of 2 slower than parsing C and the XDR over 6 times faster. Furthermore, we show that the XML files are far too large at 19 times the size of C source code, whereas XDR is only 2.2 times the C size. We also demonstrate the portability of our XDR system by presenting a C source code querying tool in Ruby. Our solution and the insights we gained from building it will be useful to analysis authors and other clients of C IRs. We have made our software freely available for download at http://www.cs.umd.edu/projects/PL/scil/. Copyright © 2010 John Wiley\&Sons, Ltd.},
  copyright  = {Copyright © 2010 John Wiley \& Sons, Ltd.},
  doi        = {10.1002/spe.954},
  file       = {:Meister2010 - Serializing C Intermediate Representations for Efficient and Portable Parsing.pdf:PDF;:scil(1).pdf:PDF},
  keywords   = {C, static analysis, intermediate representations, parsing, XML, XDR, skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.954},
  urldate    = {2021-06-10},
}

@InProceedings{10.1145/944868.944911,
  author     = {Nguyen, Tien N. and Munson, Ethan V.},
  booktitle  = {Proceedings of the 21st Annual International Conference on Documentation},
  title      = {The Software Concordance: A New Software Document Management Environment},
  year       = {2003},
  address    = {New York, NY, USA},
  pages      = {198–205},
  publisher  = {Association for Computing Machinery},
  series     = {SIGDOC '03},
  abstract   = {In this paper, we describe the efforts of Juniper Networks to implement a Feature Guide documentation manual and discuss the usability merits of this documentation method.},
  doi        = {10.1145/944868.944911},
  file       = {:10.1145_944868.944911 - The Software Concordance_ a New Software Document Management Environment.pdf:PDF},
  isbn       = {158113696X},
  keywords   = {hypermedia, documentation, software engineering, skimmed},
  location   = {San Francisco, CA, USA},
  numpages   = {8},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/944868.944911},
}

@InProceedings{10.1109/ICSE.2007.78,
  author     = {Marin, Marius and Moonen, Leon and van Deursen, Arie},
  booktitle  = {Proceedings of the 29th International Conference on Software Engineering},
  title      = {SoQueT: Query-Based Documentation of Crosscutting Concerns},
  year       = {2007},
  address    = {USA},
  pages      = {758–761},
  publisher  = {IEEE Computer Society},
  series     = {ICSE '07},
  abstract   = {Understanding crosscutting concerns is difficult because their underlying relations remain hidden in a class-based decomposition of a system. Based on an extensive investigation of crosscutting concerns in existing systems and literature, we identified a number of typical implementation idioms and relations that allow us to group such concerns around socalled .sorts.. In this paper, we present SOQUET, a tool that uses sorts to support the consistent description and documentation of crosscutting relations using pre-defined, sort-specific query templates.},
  doi        = {10.1109/ICSE.2007.78},
  file       = {:10.1109_ICSE.2007.78 - SoQueT_ Query Based Documentation of Crosscutting Concerns.pdf:PDF},
  isbn       = {0769528287},
  keywords   = {skimmed},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1109/ICSE.2007.78},
}

@InProceedings{10.1145/2093157.2093168,
  author     = {De Roover, Coen and Noguera, Carlos and Kellens, Andy and Jonckers, Vivane},
  booktitle  = {Proceedings of the 9th International Conference on Principles and Practice of Programming in Java},
  title      = {The SOUL Tool Suite for Querying Programs in Symbiosis with Eclipse},
  year       = {2011},
  address    = {New York, NY, USA},
  pages      = {71–80},
  publisher  = {Association for Computing Machinery},
  series     = {PPPJ '11},
  abstract   = {Program queries can answer important software engineering questions that range from "which expressions are cast to this type?" over "does my program attempt to read from a closed file?" to "does my code follow the prescribed design?". In this paper, we present a comprehensive tool suite for querying Java programs. It consists of the logic program query language Soul, the Cava library of predicates for quantifying over an Eclipse workspace and the Eclipse plugin Barista for launching queries and inspecting their results. Barista allows other Eclipse plugins to peruse program query results which is facilitated by the symbiosis of Soul with Java -- setting Soul apart from other program query languages. This symbiosis enables the Cava library to forego the predominant transcription to logic facts of the queried program. Instead, the library queries the actual AST nodes used by Eclipse itself, making it trivial for any Eclipse plugin to find the AST nodes that correspond to a query result. Moreover, such plugins do not have to worry about having queried stale program information. We illustrate the extensibility of our suite by implementing a tool for co-evolving source code and annotations using program queries.},
  doi        = {10.1145/2093157.2093168},
  file       = {:C\:/Users/Wernsen/Downloads/10.1145_2093157.2093168 - The SOUL Tool Suite for Querying Programs in Symbiosis with Eclipse.pdf:PDF},
  isbn       = {9781450309356},
  keywords   = {program analysis, program queries, integrated development environments, logic programming, software engineering tools, skimmed},
  location   = {Kongens Lyngby, Denmark},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2093157.2093168},
}

@Article{article,
  author     = {Tóth, Melinda and Bozó, István and Koszegi, Judit and Horváth, Zoltán},
  journal    = {Acta Electrotechnica et Informatica},
  title      = {Static Analysis Based Support for Program Comprehension in Erlang},
  year       = {2011},
  month      = {10},
  volume     = {11},
  doi        = {10.2478/v10198-011-0022-y},
  file       = {:article - Static Analysis Based Support for Program Comprehension in Erlang.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{10.1145/1006142.1006182,
  author     = {Tischler, Ron and Schaufler, Robin and Payne, Charlotte},
  journal    = {SIGPLAN Not.},
  title      = {Static Analysis of Programs as an Aid to Debugging},
  year       = {1983},
  issn       = {0362-1340},
  month      = mar,
  number     = {8},
  pages      = {155–158},
  volume     = {18},
  abstract   = {This paper describes how MAP, a tool for understanding software, combines static analysis, some dynamic features, and an interactive presentation to aid programmers in debugging. Static analysis of the sort produced in optimizing compilers could provide programmers with useful information that they cannot get from dynamic debuggers. The challenge for designers of static analysis tools is to present the information in a useful form.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1006142.1006182},
  file       = {:10.1145_1006142.1006182 - Static Analysis of Programs As an Aid to Debugging.pdf:PDF},
  issue_date = {August 1983},
  keywords   = {skimmed},
  numpages   = {4},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1006142.1006182},
}

@InProceedings{10.1145/1218563.1218571,
  author     = {Morgan, Clint and De Volder, Kris and Wohlstadter, Eric},
  booktitle  = {Proceedings of the 6th International Conference on Aspect-Oriented Software Development},
  title      = {A Static Aspect Language for Checking Design Rules},
  year       = {2007},
  address    = {New York, NY, USA},
  pages      = {63–72},
  publisher  = {Association for Computing Machinery},
  series     = {AOSD '07},
  abstract   = {Design rules express constraints on the behavior and structure of a program. These rules can help ensure that a program follows a set of established practices, and avoids certain classes of errors.Design rules often crosscut program structure and enforcing them is emerging as an important application domain for Aspect Oriented Programming. For many interesting design rules, current general purpose AOP languages lack the expressiveness to characterize them statically and enforce them at compile time.We have developed a domain specific language called Program Description Logic (PDL). PDL allows succinct declarative definitions of programmatic structures which correspond to design rule violations. PDL is based on a fully static and expressive pointcut language. PDL pointcuts allow characterizing a wide range of design rules without sacrificing static verification.We evaluate PDL by comparing it to FxCop, an industrial strength tool for checking design rules.},
  doi        = {10.1145/1218563.1218571},
  file       = {:10.1145_1218563.1218571 - A Static Aspect Language for Checking Design Rules.pdf:PDF},
  isbn       = {1595936157},
  keywords   = {design rule, aspect-oriented programming, skimmed},
  location   = {Vancouver, British Columbia, Canada},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1218563.1218571},
}

@Article{10.1145/1144366.1144370,
  author     = {Kalleberg, Karl Trygve},
  journal    = {XRDS},
  title      = {Stratego: A Programming Language for Program Manipulation},
  year       = {2006},
  issn       = {1528-4972},
  month      = may,
  number     = {3},
  pages      = {4},
  volume     = {12},
  abstract   = {Programming languages have a dual role in the construction of software. The language is both our substrate (the stuff we make software from), and our tool (what we use to construct software). Program transformation (PT) deals with the analysis, manipulation and generation of software. Therefore a close relationship exists between program transformation and programming languages, to the point where the PT field has produced many domain-specific languages for manipulating programs. In this article, I will show you some interesting aspects from one of these languages : Stratego.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1144366.1144370},
  file       = {:10.1145_1144366.1144370 - Stratego_ a Programming Language for Program Manipulation.pdf:PDF},
  issue_date = {March 2006},
  keywords   = {skimmed},
  numpages   = {1},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1144366.1144370},
}

@Article{Lejter1992,
  author     = {Lejter, M. and Meyers, S. and Reiss, S.P.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {Support for maintaining object-oriented programs},
  year       = {1992},
  issn       = {1939-3520},
  month      = dec,
  number     = {12},
  pages      = {1045--1052},
  volume     = {18},
  abstract   = {It is explained how inheritance and dynamic binding make object-oriented programs difficult to maintain, and a concrete example of the problems that arise is given. It is shown that the difficulty lies in the fact that conventional tools are poorly suited for work with object-oriented languages, and it is argued that semantics-based tools are essential for effective maintenance of object-oriented programs. A system developed for working with C++ programs is described. It comprises a relational database system for information about programs and an interactive database interface integrated with a text editor. The authors describe the system architecture, detail the database relations, provide informal evidence on the system's effectiveness, and compare it to other research with similar goals.{\textless}{\textgreater}},
  doi        = {10.1109/32.184759},
  file       = {:C\:/Users/Wernsen/Downloads/Lejter-1992-SMO.pdf:PDF},
  keywords   = {Relational databases, Object oriented databases, Concrete, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1806799.1806827,
  author     = {W\"{u}rsch, Michael and Ghezzi, Giacomo and Reif, Gerald and Gall, Harald C.},
  booktitle  = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
  title      = {Supporting Developers with Natural Language Queries},
  year       = {2010},
  address    = {New York, NY, USA},
  pages      = {165–174},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '10},
  abstract   = {The feature list of modern IDEs is steadily growing and mastering these tools becomes more and more demanding, especially for novice programmers. Despite their remarkable capabilities, IDEs often still cannot directly answer the questions that arise during program comprehension tasks. Instead developers have to map their questions to multiple concrete queries that can be answered only by combining several tools and examining the output of each of them manually to distill an appropriate answer. Existing approaches have in common that they are either limited to a set of predefined, hardcoded questions, or that they require to learn a specific query language only suitable for that limited purpose. We present a framework to query for information about a software system using guided-input natural language resembling plain English. For that, we model data extracted by classical software analysis tools with an OWL ontology and use knowledge processing technologies from the Semantic Web to query it. We use a case study to demonstrate how our framework can be used to answer queries about static source code information for program comprehension purposes.},
  doi        = {10.1145/1806799.1806827},
  file       = {:C\:/Users/Wernsen/Downloads/1806799.1806827.pdf:PDF},
  isbn       = {9781605587196},
  keywords   = {tool support, semantic web, software evolution, software maintenance, conceptual queries, source code analysis, natural language, skimmed},
  location   = {Cape Town, South Africa},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1806799.1806827},
}

@InProceedings{Jarzabek1994,
  author     = {Jarzabek, S.},
  booktitle  = {Proceedings {Eighteenth} {Annual} {International} {Computer} {Software} and {Applications} {Conference} ({COMPSAC} 94)},
  title      = {Systematic design of static program analyzers},
  year       = {1994},
  month      = nov,
  pages      = {281--286},
  abstract   = {Static program analyzers (SPA) are interactive tools that enhance program understanding by answering queries about programs. An SPA parses source programs and builds a so-called program knowledge base (PKB) that enables automatic processing of program queries. An SPA design method described in this paper consists of steps during which we (1) identify, a class of program queries we wish to answer, (2) model program information that is required to resolve queries, (3) define physical representation for programs, based on the concept of a hybrid PKB, and (4) implement other SPA components such as a front-end and user interface. Generally, queries related to global properties of programs are best handled if we store program information in a relational database. On the other hand, detailed queries are best supported if we represent programs as attributed syntax trees. A hybrid PKB described in this paper integrates these two program representations. Our notation for specifying a hybrid PKB forms a basis for a generation system that automates some of the routine, but time consuming, tasks involved in implementation of programming tools.{\textless}{\textgreater}},
  doi        = {10.1109/CMPSAC.1994.342791},
  file       = {:C\:/Users/Wernsen/Downloads/systematic-design-of-static-program-analyzers.pdf:PDF},
  keywords   = {Programming profession, Relational databases, Flow graphs, Computer science, Design methodology, Hybrid power systems, Automatic programming, Data mining, Tree graphs, Reverse engineering, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1809175.1809184,
  author     = {Panchenko, Oleksandr and Treffer, Arian and Zeier, Alexander},
  booktitle  = {Proceedings of 2010 ICSE Workshop on Search-Driven Development: Users, Infrastructure, Tools and Evaluation},
  title      = {Towards Query Formulation and Visualization of Structural Search Results},
  year       = {2010},
  address    = {New York, NY, USA},
  pages      = {33–36},
  publisher  = {Association for Computing Machinery},
  series     = {SUITE '10},
  abstract   = {Source code search goes far beyond simple textual search. One possibility of improving code search is the utilization of structural information in form of abstract syntax trees (ASTs). However, developers usually work with the textual representation of source code and, thus, have difficulties in expressing their queries as fragments of abstract syntax trees and in interpreting the results. This paper addresses assistance of query composition and search result visualization. Query formulation is considered to be an iterative process. After one query is run, the AST vertices neighbored to the result vertices are analyzed to propose refinement options for the next query. Search results are visualized in a tree view which aggregates all matches in a compact way instead of showing a small number of ranked matches.},
  doi        = {10.1145/1809175.1809184},
  file       = {:10.1145_1809175.1809184 - Towards Query Formulation and Visualization of Structural Search Results.pdf:PDF},
  isbn       = {9781605589626},
  keywords   = {XPath, abstract syntax trees, search results visualization, program analysis, source code search, visual programming and program visualization, skimmed},
  location   = {Cape Town, South Africa},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1809175.1809184},
}

@InProceedings{10.1145/1595808.1595825,
  author     = {G\'{o}mez, Ver\'{o}nica Uquillas and Kellens, Andy and Brichau, Johan and D'Hondt, Theo},
  booktitle  = {Proceedings of the Joint International and Annual ERCIM Workshops on Principles of Software Evolution (IWPSE) and Software Evolution (Evol) Workshops},
  title      = {Time Warp, an Approach for Reasoning over System Histories},
  year       = {2009},
  address    = {New York, NY, USA},
  pages      = {79–88},
  publisher  = {Association for Computing Machinery},
  series     = {IWPSE-Evol '09},
  abstract   = {The version history of a software system contains a wealth of information that can assist developers in their daily implementation and maintenance tasks. By reasoning over the role of certain code entities in previous versions of the system, developers can better understand their current state, assess the required maintenance and avoid making the same mistakes over and over again. Unfortunately, current approaches do not offer a means to easily extract specific information about the source code from such a version history.In this paper we present Time Warp, a library of logic predicates that builds on the SOUL language and the FAMIX and Hismo meta-models and that allows writing queries about the history of a system. By means of a number of concrete examples, we demonstrate how our approach can be used to express interesting queries over the version history of a system.},
  doi        = {10.1145/1595808.1595825},
  file       = {:10.1145_1595808.1595825 - Time Warp, an Approach for Reasoning Over System Histories.pdf:PDF},
  isbn       = {9781605586786},
  keywords   = {program querying, source-code history, skimmed},
  location   = {Amsterdam, The Netherlands},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1595808.1595825},
}

@InProceedings{10.1145/1519065.1519087,
  author     = {Lefebvre, Geoffrey and Cully, Brendan and Feeley, Michael J. and Hutchinson, Norman C. and Warfield, Andrew},
  booktitle  = {Proceedings of the 4th ACM European Conference on Computer Systems},
  title      = {Tralfamadore: Unifying Source Code and Execution Experience},
  year       = {2009},
  address    = {New York, NY, USA},
  pages      = {199–204},
  publisher  = {Association for Computing Machinery},
  series     = {EuroSys '09},
  abstract   = {Program source is an intermediate representation of software; it lies between a developer's intention and the hardware's execution. Despite advances in languages and development tools, source itself and the applications we use to view it remain an essentially static representation of software, from which developers can spend considerable energy postulating actual behavior.Emerging techniques in execution logging promise to provide large shared repositories containing high-fidelity recordings of deployed, production software. Tralfamadore is a system that combines source and execution trace analysis to capitalize on these recordings, and to expose information from the "experience" of real execution within the software development environment, allowing developers to inform their understanding of source based on how it behaves during real execution.},
  doi        = {10.1145/1519065.1519087},
  file       = {:1519065.1519087.pdf:PDF},
  isbn       = {9781605584829},
  keywords   = {querying execution, program understanding, dynamic analysis, debugging, trace analysis, skimmed},
  location   = {Nuremberg, Germany},
  numpages   = {6},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1519065.1519087},
}

@InProceedings{10.1145/1328408.1328425,
  author     = {Volanschi, Nic and Rinderknecht, Christian},
  booktitle  = {Proceedings of the 2008 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation},
  title      = {Unparsed Patterns: Easy User-Extensibility of Program Manipulation Tools},
  year       = {2008},
  address    = {New York, NY, USA},
  pages      = {111–121},
  publisher  = {Association for Computing Machinery},
  series     = {PEPM '08},
  abstract   = {Pattern matching in concrete syntax is very useful in program manipulation tools. In particular, user-defined extensions to such tools are written much easier using concrete syntax patterns. A few advanced frameworks for language development implement support for concrete syntax patterns, but mainstream frameworks used today still do not support them. This prevents most existing program manipulation tools from using concrete syntax matching, which in particular severely limits the writing of tool extensions to a few language experts.This paper argues that the major implementation obstacle to the pervasive use of concrete syntax patterns is the pattern parser. We propose an alternative approach based on ''unparsed patterns'', which are concrete syntax patterns that can be efficiently matched without being parsed. This lighter approach gives up static checks that parsed patterns usually do. In turn, it can be integrated within any existing parser-based software tool, almost for free. One possible consequence is enabling a widespread adoption of extensible program manipulation tools by the majority of programmers.Unparsed patterns can be used in any programing language, including multi-lingual environments. To demonstrate our approach, we implemented it both as a minimal patch for the gcc compiler, allowing to scan source code for user-defined patterns, and as a stand alone prototype called matchbox.},
  doi        = {10.1145/1328408.1328425},
  file       = {:C\:/Users/Wernsen/Downloads/1328408.1328425.pdf:PDF},
  isbn       = {9781595939777},
  keywords   = {unparsed patterns, pattern matching, source code, skimmed},
  location   = {San Francisco, California, USA},
  numpages   = {11},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1328408.1328425},
}

@InProceedings{Whaley2005,
  author     = {Whaley, John and Avots, Dzintars and Carbin, Michael and Lam, Monica S.},
  booktitle  = {Programming {Languages} and {Systems}},
  title      = {Using {Datalog} with {Binary} {Decision} {Diagrams} for {Program} {Analysis}},
  year       = {2005},
  address    = {Berlin, Heidelberg},
  editor     = {Yi, Kwangkeun},
  pages      = {97--118},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Many problems in program analysis can be expressed naturally and concisely in a declarative language like Datalog. This makes it easy to specify new analyses or extend or compose existing analyses. However, previous implementations of declarative languages perform poorly compared with traditional implementations. This paper describes bddbddb, a BDD-Based Deductive DataBase, which implements the declarative language Datalog with stratified negation, totally-ordered finite domains and comparison operators. bddbddb uses binary decision diagrams (BDDs) to efficiently represent large relations. BDD operations take time proportional to the size of the data structure, not the number of tuples in a relation, which leads to fast execution times. bddbddb is an effective tool for implementing a large class of program analyses. We show that a context-insensitive points-to analysis implemented with bddbddb is about twice as fast as a carefully hand-tuned version. The use of BDDs also allows us to solve heretofore unsolved problems, like context-sensitive pointer analysis for large programs.},
  doi        = {10.1007/11575467_8},
  file       = {:Whaley2005 - Using Datalog with Binary Decision Diagrams for Program Analysis.pdf:PDF;:Using_Datalog_with_Binary_Decision_Diagrams_for_Pr.pdf:PDF},
  isbn       = {9783540322474},
  keywords   = {Boolean Function , Logic Program , Logic Programming , Intermediate Representation , Binary Decision Diagram , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1806799.1806828,
  author     = {Fritz, Thomas and Murphy, Gail C.},
  booktitle  = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
  title      = {Using Information Fragments to Answer the Questions Developers Ask},
  year       = {2010},
  address    = {New York, NY, USA},
  pages      = {175–184},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '10},
  abstract   = {Each day, a software developer needs to answer a variety of questions that require the integration of different kinds of project information. Currently, answering these questions, such as "What have my co-workers been doing?", is tedious, and sometimes impossible, because the only support available requires the developer to manually link and traverse the information step-by-step. Through interviews with eleven professional developers, we identified 78 questions developers want to ask, but for which support is lacking. We introduce an information fragment model (and prototype tool) that automates the composition of different kinds of information and that allows developers to easily choose how to display the composed information. In a study, 18 professional developers used the prototype tool to answer eight of the 78 questions. All developers were able to easily use the prototype to successfully answer 94% of questions in a mean time of 2.3 minutes per question.},
  doi        = {10.1145/1806799.1806828},
  file       = {:10.1145_1806799.1806828 - Using Information Fragments to Answer the Questions Developers Ask.pdf:PDF},
  isbn       = {9781605587196},
  keywords   = {human-centric software engineering, information fragments, programming tools, skimmed},
  location   = {Cape Town, South Africa},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1806799.1806828},
}

@InProceedings{10.1145/1119655.1119676,
  author     = {Pfeiffer, J.-Hendrik and Gurd, John R.},
  booktitle  = {Proceedings of the 5th International Conference on Aspect-Oriented Software Development},
  title      = {Visualisation-Based Tool Support for the Development of Aspect-Oriented Programs},
  year       = {2006},
  address    = {New York, NY, USA},
  pages      = {146–157},
  publisher  = {Association for Computing Machinery},
  series     = {AOSD '06},
  abstract   = {The development of aspect-oriented software requires tool support to make the aspect-oriented structures explicit and to assist programmers in understanding the overall source code, including aspects. Tools exist to meet this need, but they struggle with large aspect-oriented programs; as a result, navigating the source code becomes difficult.This paper describes the application of Treemaps, a well-known format for visualisation of hierarchical data, to the visualisation of aspect-oriented programs. We present and discuss a new scheme for browsing and visualising such programs, which is capable of coping with programs of large size. Additionally, we describe a prototype tool that implements the presented scheme, and report on the results of a user study which demonstrates the benefits of using the tool.},
  doi        = {10.1145/1119655.1119676},
  file       = {:10.1145_1119655.1119676 - Visualisation Based Tool Support for the Development of Aspect Oriented Programs.pdf:PDF},
  isbn       = {159593300X},
  keywords   = {treemaps, aspects, software visualisation, skimmed},
  location   = {Bonn, Germany},
  numpages   = {12},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1119655.1119676},
}

@InProceedings{10.1145/1094855.1094911,
  author     = {Girgis, Hani Z. and Jayaraman, Bharat and Gestwicki, Paul V.},
  booktitle  = {Companion to the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  title      = {Visualizing Errors in Object Oriented Programs},
  year       = {2005},
  address    = {New York, NY, USA},
  pages      = {156–157},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '05},
  abstract   = {We describe the results of visualizing object oriented programs errors, and utilizing these results in the design of a set of visual queries on the runtime execution history of a program. We bring together under one coherent framework different approaches such as error classification, bug patterns in object oriented programs, and visual queries. Our work is founded on a novel interactive visualization system for Java called JIVE, developed at Buffalo.},
  doi        = {10.1145/1094855.1094911},
  file       = {:10.1145_1094855.1094911 - Visualizing Errors in Object Oriented Programs.pdf:PDF},
  isbn       = {1595931937},
  keywords   = {runtime visual queries, Java, visualization, debugging, errors, skimmed},
  location   = {San Diego, CA, USA},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1094855.1094911},
}

@InProceedings{Menshikov2020,
  author     = {Menshikov, Maxim},
  booktitle  = {Computational {Science} and {Its} {Applications} – {ICCSA} 2020},
  title      = {Midair: {An} {Intermediate} {Representation} for {Multi}-purpose {Program} {Analysis}},
  year       = {2020},
  address    = {Cham},
  editor     = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Garau, Chiara and Blecic, Ivan and Taniar, David and Apduhan, Bernady O. and Rocha, Ana Maria A. C. and Tarantino, Eufemia and Torre, Carmelo Maria and Karaca, Yeliz},
  pages      = {544--559},
  publisher  = {Springer International Publishing},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {The static analysis field had grown enough to be used not only for finding casual defects. In practice, it may be used to enforce the coding style and flag undesired syntax constructs, find logical mistakes, prove that the program satisfies its specification, apply domain-specific checks, or even verify cross-program compatibility. Those are all valid use cases that are required to be handled by the static analyzer, and the intermediate representation (IR) affects how can it be done.A typical compiler or analyzer uses a number of IRs, each of them helps with a specific problem. For our static analyzer project, we found that existing IRs partially do not match our requirements, which led to the creation of Midair—an IR for multi-purpose program analysis. It is positioned right between IRs created primarily for the compilation (like LLVM, MLIR, GIMPLE) and verification IRs (such as Boogie) with the hope that it would be both close to a low level and suitable for verification while applying to practical analysis tools. The IR consists of 4 layers, allowing for a transparent transformation between forms saving time and space. A flexible type system supporting non-machinery types and an ability to augment the representation with external metadata provided by solvers had been added. The application of Midair to our analysis framework uncovered advantages and non-critical issues, which are planned to be worked around.},
  doi        = {10.1007/978-3-030-58817-5_40},
  file       = {:Menshikov2020 - Midair_ an Intermediate Representation for Multi Purpose Program Analysis.pdf:PDF;:978-3-030-58817-5_Chapter_40(1).pdf:PDF},
  isbn       = {9783030588175},
  keywords   = {Intermediate representation , Verification , Program analysis , Static analysis , Framework , skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {Midair},
}

@InProceedings{BellamyMcIntyre2018,
  author     = {Bellamy-McIntyre, Jacob},
  booktitle  = {The {Semantic} {Web}: {ESWC} 2018 {Satellite} {Events}},
  title      = {Modeling and {Querying} {Versioned} {Source} {Code} in {RDF}},
  year       = {2018},
  address    = {Cham},
  editor     = {Gangemi, Aldo and Gentile, Anna Lisa and Nuzzolese, Andrea Giovanni and Rudolph, Sebastian and Maleshkova, Maria and Paulheim, Heiko and Pan, Jeff Z and Alam, Mehwish},
  pages      = {251--261},
  publisher  = {Springer International Publishing},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Source code management is an active and fundamental area of research where one of the key challenges is allowing developers to maintain an understanding of software projects when it is being actively developed in a distributed setting. Despite a number of well established practices and tools to help keep track of modifications to a project, there is a lack of a standard representation and query mechanism to integrate different repositories and serve fine-grained retrieval tasks. In this paper we propose modeling source code in resource description framework (RDF) triples as it is the main standard for sharing semantic information over the web. To support temporal queries over different source code versions we present a temporal extension of SPARQL that uses an in memory index of changes generated from a standard transaction log. We have built a prototype system to demonstrate that the approach is feasible, and present some preliminary results on query execution.},
  doi        = {10.1007/978-3-319-98192-5_44},
  file       = {:BellamyMcIntyre2018 - Modeling and Querying Versioned Source Code in RDF (1).pdf:PDF;:bellamy-mcintyre2018.pdf:PDF;:C\:/Users/Wernsen/Downloads/bellamy-mcintyre2018.pdf:PDF},
  isbn       = {9783319981925},
  keywords   = {Temporal databases , RDF , SPARQL , Static analysis , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@InProceedings{10.1145/2451436.2451448,
  author     = {Mitschke, Ralf and Eichberg, Michael and Mezini, Mira and Garcia, Alessandro and Macia, Isela},
  booktitle  = {Proceedings of the 12th Annual International Conference on Aspect-Oriented Software Development},
  title      = {Modular Specification and Checking of Structural Dependencies},
  year       = {2013},
  address    = {New York, NY, USA},
  pages      = {85–96},
  publisher  = {Association for Computing Machinery},
  series     = {AOSD '13},
  abstract   = {Checking a software's structural dependencies is a line of research on methods and tools for analyzing, modeling and checking the conformance of source code w.r.t. specifications of its intended static structure. Existing approaches have focused on the correctness of the specification, the impact of the approaches on software quality and the expressiveness of the modeling languages. However, large specifications become unmaintainable in the event of evolution without the means to modularize such specifications. We present Vespucci, a novel approach and tool that partitions a specification of the expected and allowed dependencies into a set of cohesive slices. This facilitates modular reasoning and helps individual maintenance of each slice. Our approach is suited for modeling high-level as well as detailed low-level decisions related to the static structure and combines both in a single modeling formalism. To evaluate our approach we conducted an extensive study spanning nine years of the evolution of the architecture of the object-relational mapping framework Hibernate.},
  doi        = {10.1145/2451436.2451448},
  file       = {:10.1145_2451436.2451448 - Modular Specification and Checking of Structural Dependencies.pdf:PDF},
  isbn       = {9781450317665},
  keywords   = {scalability, software architectures, static analysis, modularity, structural dependency constraints, skimmed},
  location   = {Fukuoka, Japan},
  numpages   = {12},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2451436.2451448},
}

@Article{Sah2014,
  author     = {Sah, Sudhakar and Vaidya, Vinay G.},
  journal    = {International Journal of Software Innovation (IJSI)},
  title      = {New {Approach} to {Speedup} {Dynamic} {Program} {Parallelization} {Analysis}},
  year       = {2014},
  issn       = {2166-7160},
  month      = oct,
  number     = {4},
  pages      = {28--47},
  volume     = {2},
  abstract   = {Development of parallel programming tools has become a mainstream research topic and it has produced many useful tools that reduce the burden of parallel programming. The most important aspect of any such tool is the data dependency analysis, which is very complex and computationally intensive. Full...},
  copyright  = {Access limited to members},
  doi        = {10.4018/ijsi.2014100103},
  file       = {:Sah2014 - New Approach to Speedup Dynamic Program Parallelization Analysis.pdf:PDF;:C\:/Users/Wernsen/Downloads/sah2014.pdf:PDF;:C\:/Users/Wernsen/Downloads/sah2014.pdf:PDF;:sah2014.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  publisher  = {IGI Global},
  readstatus = {skimmed},
  url        = {www.igi-global.com/article/new-approach-to-speedup-dynamic-program-parallelization-analysis/120517},
  urldate    = {2021-06-11},
}

@InProceedings{Zhang2015,
  author     = {Zhang, Tian and Pan, Minxue and Zhao, Jizhou and Yu, Yijun and Li, Xuandong},
  booktitle  = {2015 {International} {Symposium} on {Theoretical} {Aspects} of {Software} {Engineering}},
  title      = {An {Open} {Framework} for {Semantic} {Code} {Queries} on {Heterogeneous} {Repositories}},
  year       = {2015},
  month      = sep,
  pages      = {39--46},
  abstract   = {To help developers understand and reuse programs, semantic queries on the source code itself is attractive. Although programs in heterogeneous languages are being controlled for collaborative software development, most queries supported by various source code repositories are based either on the metadata of the repositories, or on indexed identifiers and method signatures. Few provide full support to search for semantic structures that are common across different programming languages. To facilitate the understanding and reuses, in this paper, we propose a novel source code query framework that (1) supports the semantic code queries across different programming languages with a new query language, (2) transforms source code to a unified abstract syntax format and handles heterogeneity at the abstract level, (3) stores source code on a cloud-based NoSQL storage in MangoDB. The efficiency of the framework has been evaluated and confirmed by experiments.},
  doi        = {10.1109/TASE.2015.27},
  file       = {:Zhang2015 - An Open Framework for Semantic Code Queries on Heterogeneous Repositories.pdf:PDF},
  keywords   = {Semantics, Java, XML, Grammar, Databases, Syntactics, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/2304636.2304642,
  author     = {Weiss, Roland J. and Repetto, Daniele and Koziolek, Heiko},
  booktitle  = {Proceedings of the 2012 ACM SIGSOFT Symposium on Industry Day},
  title      = {Perseverance in Sustainable Software Architecting},
  year       = {2012},
  address    = {New York, NY, USA},
  pages      = {11–14},
  publisher  = {Association for Computing Machinery},
  series     = {Industry Day '12},
  abstract   = {In the recent past, there has been an increased interest in better managing the evolution of existing software systems and improving the software engineering practices for this now common task. In this paper, we take a look at the efforts at ABB to advance in this area, with special emphasis on architectures of long-living systems. The review consists of detailing the introduced methods and tools, as well as sharing experiences from applying them. In addition, we present two current case studies from the industrial automation domain that will be used as additional test fields for the developed methods.},
  doi        = {10.1145/2304636.2304642},
  file       = {:10.1145_2304636.2304642 - Perseverance in Sustainable Software Architecting.pdf:PDF},
  isbn       = {9781450313490},
  keywords   = {long-lived software systems, sustainability, validation, skimmed},
  location   = {Bertinoro, Italy},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2304636.2304642},
}

@InProceedings{Buchgeher2018,
  author     = {Buchgeher, Georg and Weinreich, Rainer and Huber, Heinz},
  booktitle  = {Software {Architecture}},
  title      = {A {Platform} for the {Automated} {Provisioning} of {Architecture} {Information} for {Large}-{Scale} {Service}-{Oriented} {Software} {Systems}},
  year       = {2018},
  address    = {Cham},
  editor     = {Cuesta, Carlos E. and Garlan, David and Pérez, Jennifer},
  pages      = {203--218},
  publisher  = {Springer International Publishing},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Providing valid architecture information to stakeholders remains a challenge, as the effort required for documenting and maintaining this information over a longer period of time is very high. Automatically and continuously extracting architecture information from the system implementation makes it possible to document and keep architecture information up-to-date. In large software systems, architecture extraction has to deal with the continuous and efficient extraction of architectural information from very large code bases. In cooperation with a company from the financial sector, we have developed over several years a platform for the automatic extraction and provision of architectural information for large-scale service-oriented software systems. The platform was evaluated in a real industrial environment. The results of this evaluation show that it can provide up-to-date architectural information for large code bases on a daily basis. It also provides information on the trustworthiness of the extracted information and how it can be improved.},
  doi        = {10.1007/978-3-030-00761-4_14},
  file       = {:Buchgeher2018 - A Platform for the Automated Provisioning of Architecture Information for Large Scale Service Oriented Software Systems.pdf:PDF;:buchgeher2018.pdf:PDF},
  isbn       = {9783030007614},
  keywords   = {Software architecture knowledge , Architecture documentation , Architecture model , Architecture extraction , Service-based systems , Code analysis , Software analytics , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@InProceedings{10.1145/2414721.2414728,
  author     = {Urma, Raoul-Gabriel and Mycroft, Alan},
  booktitle  = {Proceedings of the ACM 4th Annual Workshop on Evaluation and Usability of Programming Languages and Tools},
  title      = {Programming Language Evolution via Source Code Query Languages},
  year       = {2012},
  address    = {New York, NY, USA},
  pages      = {35–38},
  publisher  = {Association for Computing Machinery},
  series     = {PLATEAU '12},
  abstract   = {Programming languages evolve just like programs. Language features are added and removed, for example when programs using them are shown to be error-prone. When language features are modified, deprecated, removed or even deemed unsuitable for the project at hand, it is necessary to analyse programs to identify occurrences to refactor.Source code query languages in principle provide a good way to perform this analysis by exploring codebases. Such languages are often used to identify code to refactor, bugs to fix or simply to understand a system better.This paper evaluates seven Java source code query languages: Java Tools Language, Browse-By-Query, SOUL, JQuery, .QL, Jackpot and PMD as to their power at expressing queries required by several use cases (such as code idioms to be refactored).},
  doi        = {10.1145/2414721.2414728},
  file       = {:C\:/Users/Wernsen/Downloads/10.1145_2414721.2414728 - Programming Language Evolution Via Source Code Query Languages.pdf:PDF},
  isbn       = {9781450316316},
  keywords   = {program analysis, query languages, source code, skimmed},
  location   = {Tucson, Arizona, USA},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2414721.2414728},
}

@InProceedings{10.1145/2717124.2717125,
  author     = {Urma, Raoul-Gabriel and Orchard, Dominic and Mycroft, Alan},
  booktitle  = {Proceedings of the 1st Workshop on Programming Language Evolution},
  title      = {Programming Language Evolution Workshop Report},
  year       = {2014},
  address    = {New York, NY, USA},
  pages      = {1–3},
  publisher  = {Association for Computing Machinery},
  series     = {PLE '14},
  abstract   = {Programming languages evolve in response to external and internal factors. External factors include new hardware, new theory or foundational research, trends or fashions in languages, and applications. Internal factors include a change in the way a language is used or the discovery of problems or deficiencies with existing features. However, evolving programming languages can be problematic; evaluating the impact of changes is difficult and it is often unclear how to effectively co-evolve software written in the language. The PLE workshop was initiated to bring together researchers to discuss and tackle these problems.The papers and talks presented at this workshop provided an excellent start to this new workshop and raised many interesting issues relating to programming language evolution. This document briefly reports on the activities of the first workshop, which was co-located with ECOOP 2014 in Uppsala, Sweden, July 2014.},
  doi        = {10.1145/2717124.2717125},
  file       = {:10.1145_2717124.2717125 - Programming Language Evolution Workshop Report.pdf:PDF},
  isbn       = {9781450328876},
  keywords   = {skimmed},
  location   = {Uppsala, Sweden},
  numpages   = {3},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2717124.2717125},
}

@Article{Stevens2019,
  author     = {Stevens, Reinout and Molderez, Tim and De Roover, Coen},
  journal    = {Empirical Software Engineering},
  title      = {Querying distilled code changes to extract executable transformations},
  year       = {2019},
  issn       = {1573-7616},
  month      = feb,
  number     = {1},
  pages      = {491--535},
  volume     = {24},
  abstract   = {Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. We show that our approach is able to recall different implementation variants of the same source code evolution in histories of different software projects. We also evaluate the expressiveness and ease-of-use of our approach in a user study.},
  doi        = {10.1007/s10664-018-9644-3},
  file       = {:Stevens2019 - Querying Distilled Code Changes to Extract Executable Transformations.pdf:PDF;:vub-soft-tr-18-10.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1007/s10664-018-9644-3},
  urldate    = {2021-06-11},
}

@InProceedings{10.1109/ASE.2011.6100076,
  author     = {Kimmig, Markus and Monperrus, Martin and Mezini, Mira},
  booktitle  = {Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering},
  title      = {Querying Source Code with Natural Language},
  year       = {2011},
  address    = {USA},
  pages      = {376–379},
  publisher  = {IEEE Computer Society},
  series     = {ASE '11},
  abstract   = {One common task of developing or maintaining software is searching the source code for information like specific method calls or write accesses to certain fields. This kind of information is required to correctly implement new features and to solve bugs. This paper presents an approach for querying source code with natural language.},
  doi        = {10.1109/ASE.2011.6100076},
  file       = {:C\:/Users/Wernsen/Downloads/ASE.2011.6100076.pdf:PDF},
  isbn       = {9781457716386},
  keywords   = {skimmed},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1109/ASE.2011.6100076},
}

@InProceedings{Stevens2014,
  author     = {Stevens, Reinout and De Roover, Coen},
  booktitle  = {2014 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution}},
  title      = {Querying the {History} of {Software} {Projects} {Using} {QWALKEKO}},
  year       = {2014},
  month      = sep,
  note       = {ISSN: 1063-6773},
  pages      = {585--588},
  abstract   = {We present the QwalKeko meta-programming library for Clojure that enables querying the history of versioned software projects in a declarative manner. Unique to this library is its support for regular path expressions within history queries. Regular path expressions are akin to regular expressions, except that they match a sequence of successive snapshots of a software project along which user-specified logic conditions must hold. Such logic conditions can concern the source code within a snapshot, versioning information associated with the snapshot, as well as patterns of source code changes with respect to other snapshots. We have successfully used the resulting multi-faceted queries to detect refactorings in project histories. In this paper, we discuss how applicative logic meta-programming enabled combining the heterogenous components of QwalKeko into a uniform whole. We focus on the applicative logic interface to a new implementation of a well-known change distilling algorithm. We use the problem of detecting and categorizing changes made to Selenium-based test scripts for illustration purposes.},
  doi        = {10.1109/ICSME.2014.101},
  file       = {:Stevens2014 - Querying the History of Software Projects Using QWALKEKO.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {History, Libraries, Database languages, Java, Software maintenance, Software engineering, declarative programming, program querying, history querying, mining software repositories, skimmed},
  readstatus = {skimmed},
}

@Article{GarciaContreras2016,
  author     = {García-Contreras, Isabel and Morales, José F. and Hermenegildo, Manuel V.},
  journal    = {Theory and Practice of Logic Programming},
  title      = {Semantic code browsing*},
  year       = {2016},
  issn       = {1471-0684, 1475-3081},
  month      = sep,
  number     = {5-6},
  pages      = {721--737},
  volume     = {16},
  abstract   = {Programmers currently enjoy access to a very high number of code repositories and libraries of ever increasing size. The ensuing potential for reuse is however hampered by the fact that searching within all this code becomes an increasingly difficult task. Most code search engines are based on syntactic techniques such as signature matching or keyword extraction. However, these techniques are inaccurate (because they basically rely on documentation) and at the same time do not offer very expressive code query languages. We propose a novel approach that focuses on querying for semantic characteristics of code obtained automatically from the code itself. Program units are pre-processed using static analysis techniques, based on abstract interpretation, obtaining safe semantic approximations. A novel, assertion-based code query language is used to express desired semantic characteristics of the code as partial specifications. Relevant code is found by comparing such partial specifications with the inferred semantics for program elements. Our approach is fully automatic and does not rely on user annotations or documentation. It is more powerful and flexible than signature matching because it is parametric on the abstract domain and properties, and does not require type definitions. Also, it reasons with relations between properties, such as implication and abstraction, rather than just equality. It is also more resilient to syntactic code differences. We describe the approach and report on a prototype implementation within the Ciao system.},
  doi        = {10.1017/S1471068416000417},
  file       = {:GarciaContreras2016 - Semantic Code Browsing_.pdf:PDF;:HERME_A_2016-1.pdf:PDF},
  keywords   = {Semantic Code Search, Abstract Interpretation, Assertions, skimmed},
  language   = {en},
  publisher  = {Cambridge University Press},
  readstatus = {skimmed},
  url        = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/abs/semantic-code-browsing/80FAA7923170C06330276E6E19166E3B},
  urldate    = {2021-06-11},
}

@InProceedings{Come2018,
  author     = {Come, David and Brunel, Julien and Doose, David},
  booktitle  = {Formal {Methods}: {Foundations} and {Applications}},
  title      = {Source {Code} {Analysis} with a {Temporal} {Extension} of {First}-{Order} {Logic}},
  year       = {2018},
  address    = {Cham},
  editor     = {Massoni, Tiago and Mousavi, Mohammad Reza},
  pages      = {20--38},
  publisher  = {Springer International Publishing},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Formal methods and static analysis are widely used in software development, in particular in the context of safety-critical systems. They can be used to prove that the software behavior complies with its specification: the software correctness. In this article, we address another usage of these methods: the verification of the quality of the source code, i.e., the compliance with guidelines, coding rules, design patterns.Such rules can refer to the structure of the source code through its Abstract Syntax Tree (AST) or to execution paths in the Control Flow Graph (CFG) of functions. AST and CFGs offer complementary information and current methods are not able to exploit both of them simultaneously. In this article, we propose an approach to automatically verifying the compliance of an application with specifications (coding rules) that reason about both the AST of the source code and the CFG of its functions. To formally express the specification, we introduce FO++FO++FO{\textasciicircum}\{++\}, a logic defined as a temporal extension of many-sorted first-order logic. In our framework, verifying the compliance of the source code comes down to the model-checking problem for FO++FO++FO{\textasciicircum}\{++\}. We present a correct and complete model checking algorithm for FO++FO++FO{\textasciicircum}\{++\} and establish that the model checking problem of FO++FO++FO{\textasciicircum}\{++\} is PSPACE-complete. This approach is implemented into Pangolin, a tool for analyzing C++ programs. We use Pangolin to analyze two middle-sized open-source projects, looking for violations of six coding rules and report on several detected violations.},
  doi        = {10.1007/978-3-030-03044-5_3},
  file       = {:Come2018 - Source Code Analysis with a Temporal Extension of First Order Logic.pdf:PDF;:C\:/Users/Wernsen/Downloads/DTIS18236.1539008639.pdf:PDF},
  isbn       = {9783030030445},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@Article{Pawlak2016,
  author     = {Pawlak, Renaud and Monperrus, Martin and Petitprez, Nicolas and Noguera, Carlos and Seinturier, Lionel},
  journal    = {Software: Practice and Experience},
  title      = {{SPOON}: {A} library for implementing analyses and transformations of {Java} source code},
  year       = {2016},
  issn       = {1097-024X},
  number     = {9},
  pages      = {1155--1179},
  volume     = {46},
  abstract   = {This paper presents SPOON, a library for the analysis and transformation of Java source code. SPOON enables Java developers to write a large range of domain-specific analyses and transformations in an easy and concise manner. SPOON analyses and transformations are written in plain Java. With SPOON, developers do not need to dive into parsing, to hack a compiler infrastructure, or to master a new formalism. Copyright © 2015 John Wiley \& Sons, Ltd.},
  copyright  = {Copyright © 2015 John Wiley \& Sons, Ltd.},
  doi        = {10.1002/spe.2346},
  file       = {:Pawlak2016 - SPOON_ a Library for Implementing Analyses and Transformations of Java Source Code (1).pdf:PDF;:C\:/Users/Wernsen/Downloads/document.pdf:PDF},
  keywords   = {source code analysis, source code transformation, metaprogramming, skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {{SPOON}},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2346},
  urldate    = {2021-06-11},
}

@InProceedings{Collard2013,
  author     = {Collard, Michael L. and Decker, Michael John and Maletic, Jonathan I.},
  booktitle  = {2013 {IEEE} {International} {Conference} on {Software} {Maintenance}},
  title      = {{srcML}: {An} {Infrastructure} for the {Exploration}, {Analysis}, and {Manipulation} of {Source} {Code}: {A} {Tool} {Demonstration}},
  year       = {2013},
  month      = sep,
  note       = {ISSN: 1063-6773},
  pages      = {516--519},
  abstract   = {SrcML is an XML representation for C/C++/Java source code that forms a platform for the efficient exploration, analysis, and manipulation of large software projects. The lightweight format allows for round-trip transformation from source to srcML and back to source with no loss of information or formatting. The srcML toolkit consists of the src2srcml tool for robust translation to the srcML format and the srcml2src tool for querying via XPath, and transformation via XSLT. In this demonstration a guide of these features is provided along with the use of XPath for constructing source-code queries and XSLT for conducting simple transformations.},
  doi        = {10.1109/ICSM.2013.85},
  file       = {:Collard2013 - SrcML_ an Infrastructure for the Exploration, Analysis, and Manipulation of Source Code_ a Tool Demonstration.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {XML, Syntactics, Libraries, Java, Communities, Software maintenance, srcML, static code analysis, source transformation, skimmed},
  readstatus = {skimmed},
  shorttitle = {{srcML}},
}

@InProceedings{Shastry2017,
  author     = {Shastry, Bhargava and Leutner, Markus and Fiebig, Tobias and Thimmaraju, Kashyap and Yamaguchi, Fabian and Rieck, Konrad and Schmid, Stefan and Seifert, Jean-Pierre and Feldmann, Anja},
  booktitle  = {Research in {Attacks}, {Intrusions}, and {Defenses}},
  title      = {Static {Program} {Analysis} as a {Fuzzing} {Aid}},
  year       = {2017},
  address    = {Cham},
  editor     = {Dacier, Marc and Bailey, Michael and Polychronakis, Michalis and Antonakakis, Manos},
  pages      = {26--47},
  publisher  = {Springer International Publishing},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Fuzz testing is an effective and scalable technique to perform software security assessments. Yet, contemporary fuzzers fall short of thoroughly testing applications with a high degree of control-flow diversity, such as firewalls and network packet analyzers. In this paper, we demonstrate how static program analysis can guide fuzzing by augmenting existing program models maintained by the fuzzer. Based on the insight that code patterns reflect the data format of inputs processed by a program, we automatically construct an input dictionary by statically analyzing program control and data flow. Our analysis is performed before fuzzing commences, and the input dictionary is supplied to an off-the-shelf fuzzer to influence input generation. Evaluations show that our technique not only increases test coverage by 10–15\% over baseline fuzzers such as afl but also reduces the time required to expose vulnerabilities by up to an order of magnitude. As a case study, we have evaluated our approach on two classes of network applications: nDPI, a deep packet inspection library, and tcpdump, a network packet analyzer. Using our approach, we have uncovered 15 zero-day vulnerabilities in the evaluated software that were not found by stand-alone fuzzers. Our work not only provides a practical method to conduct security evaluations more effectively but also demonstrates that the synergy between program analysis and testing can be exploited for a better outcome.},
  doi        = {10.1007/978-3-319-66332-6_2},
  file       = {:Shastry2017 - Static Program Analysis As a Fuzzing Aid.pdf:PDF;:7-static.pdf:PDF},
  isbn       = {9783319663326},
  keywords   = {Program analysis , Fuzzing , Protocol parsers , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@Article{10.1145/2507288.2507312,
  author     = {Singh, Pavitdeep and Singh, Satwinder and Kaur, Jatinder},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Tool for Generating Code Metrics for C# Source Code Using Abstract Syntax Tree Technique},
  year       = {2013},
  issn       = {0163-5948},
  month      = aug,
  number     = {5},
  pages      = {1–6},
  volume     = {38},
  abstract   = {Software maintenance is one of the key activities in any software engineering process in which source code analysis plays a crucial role. Due to the high cost of maintenance, it has become quite necessary to produce high quality software. Over time, numerous analyses have been performed on source code to determine complexity and other metrics. Lots of papers have been published for object oriented languages but mostly concentrating on C++ and Java, very few has been published for more modern languages like C} # . #This #paper #proposes #a #Software #Quality #Assurance #Tool #for #measuring #the #different #code #metrics #for #the #object #oriented #language #C #{ at the class and method levels. The technique consists of generating the abstract syntax tree of the source code using Nfactory libraries. The Interface is built using the Win Form application which provides an impressive GUI for the tool.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2507288.2507312},
  file       = {:10.1145_2507288.2507312 - Tool for Generating Code Metrics for C# Source Code Using Abstract Syntax Tree Technique.pdf:PDF},
  issue_date = {September 2013},
  keywords   = {abstract syntax tree, Nfactory, C#, source code analysis, code metrics, skimmed},
  numpages   = {6},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2507288.2507312},
}

@InProceedings{10.1145/2541348.2541356,
  author     = {Orchard, Dominic and Rice, Andrew},
  booktitle  = {Proceedings of the 2013 ACM Workshop on Workshop on Refactoring Tools},
  title      = {Upgrading Fortran Source Code Using Automatic Refactoring},
  year       = {2013},
  address    = {New York, NY, USA},
  pages      = {29–32},
  publisher  = {Association for Computing Machinery},
  series     = {WRT '13},
  abstract   = {Many of the computer models used in scientific research have been developed in Fortran over many years. This evolutionary process means these models often use deprecated language features and idioms that impede software maintenance, understandability, extension, and verification. To mitigate this, we built CamFort, an open-source automatic refactoring tool for upgrading Fortran source code. We describe functionality in CamFort for removing equivalence statements and common blocks, and for introducing structured data types, and give examples of how these transformations can benefit codebase robustness.},
  doi        = {10.1145/2541348.2541356},
  file       = {:10.1145_2541348.2541356 - Upgrading Fortran Source Code Using Automatic Refactoring.pdf:PDF},
  isbn       = {9781450326049},
  keywords   = {language evolution, refactoring, haskell, fortran, computational science, skimmed},
  location   = {Indianapolis, Indiana, USA},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2541348.2541356},
}

@Article{Maeder2013,
  author     = {Mäder, Patrick and Cleland-Huang, Jane},
  journal    = {Software \& Systems Modeling},
  title      = {A visual language for modeling and executing traceability queries},
  year       = {2013},
  issn       = {1619-1374},
  month      = jul,
  number     = {3},
  pages      = {537--553},
  volume     = {12},
  abstract   = {Current software and systems engineering tools provide only basic trace features, and as a result users are often compelled to construct non-trivial traceability queries using generic query languages such as SQL. In this paper, we present an alternative approach which defines traceability strategies for a project using UML class diagrams and then constructs trace queries as constraints upon subsets of the model. The visual trace modeling language (VTML) allows users to model a broad range of trace queries while hiding underlying technical details and data structures. The viability and expressiveness of VTML for use in a real project are demonstrated through modeling a broadly representative set of queries for a web-based health-care system. It is then evaluated through an experiment with human users to assess the readability and writability of VTML queries in comparison to generic SQL queries. We found that users read and constructed traceability queries considerably faster using VTML than using SQL. Furthermore, visually constructed traceability queries were substantially more correct compared to the same queries constructed with SQL.},
  doi        = {10.1007/s10270-012-0237-0},
  file       = {:Maeder2013 - A Visual Language for Modeling and Executing Traceability Queries.pdf:PDF;:C\:/Users/Wernsen/Downloads/Mäder-Cleland-Huang2013_Article_AVisualLanguageForModelingAndE(1).pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1007/s10270-012-0237-0},
  urldate    = {2021-06-11},
}

@InProceedings{10.1145/2663887.2663905,
  author     = {Vanciu, Radu and Khalaj, Ebrahim and Abi-Antoun, Marwan},
  booktitle  = {Proceedings of the 2014 ACM Workshop on Security Information Workers},
  title      = {Comparative Evaluation of Architectural and Code-Level Approaches for Finding Security Vulnerabilities},
  year       = {2014},
  address    = {New York, NY, USA},
  pages      = {27–34},
  publisher  = {Association for Computing Machinery},
  series     = {SIW '14},
  abstract   = {During architectural risk analysis, Security Information Workers (SIWs) reason about security-relevant architectural flaws using a high-level representation of the system's structure instead of directly reading the code as in during a code review. It is still hard to extract from the code a high-level representation that is sound, conveys design intent, and enables expressive constraints that can find security vulnerabilities. As a result, architecture-level approaches are less mature than code-level ones that extract low-level representations that are not directly intended for use by SIWs.In this paper, we compare an architecture-level approach with a code-level approach in terms of effectiveness (precision and recall) across test cases with injected vulnerabilities that range from coding bugs to architectural flaws. The evaluation shows that an architecture-level approach can uncover some security vulnerabilities with better precision and recall than a code-level approach. Moreover, it shows that the effectiveness of the approaches varies greatly based on whether the security vulnerability is a coding bug or an architectural flaw. These results may help SIWs select the right tools for the job of securing their systems.},
  doi        = {10.1145/2663887.2663905},
  file       = {:C\:/Users/Wernsen/Downloads/10.1.1.1080.1835.pdf:PDF},
  isbn       = {9781450331524},
  keywords   = {skimmed},
  location   = {Scottsdale, Arizona, USA},
  numpages   = {8},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2663887.2663905},
}

@Article{https://doi.org/10.1002/spe.2157,
  author     = {Zhao, Dan and Choudhary, Shauvik Roy and Orso, Alessandro},
  journal    = {Software: Practice and Experience},
  title      = {Developing analysis and testing plug-ins for modern IDEs: an experience report},
  year       = {2013},
  number     = {4},
  pages      = {465-478},
  volume     = {43},
  abstract   = {SUMMARYPlug-ins have become an important part of today's Integrated Development Environments (IDEs). They are useful not only for extending the IDEs’ functionality but also for customizing the IDEs for different types of projects. In this paper, we discuss some features that IDEs should provide to support the development of a specific kind of plug-ins—plug-ins that implement program analysis and software testing techniques. To guide the discussion, we first provide a survey of existing testing and analysis plug-ins and, for each of these plug-ins, discuss the details of the IDE support they use. We then present a case study based on our personal experience with building a regression-testing plug-in for two different IDEs. Finally, we use our findings to make a generalized discussion on the kind of capabilities a platform should provide to better support the development of program analysis and software testing plug-ins.Copyright © 2012 John Wiley \& Sons, Ltd.},
  doi        = {https://doi.org/10.1002/spe.2157},
  eprint     = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2157},
  file       = {:C\:/Users/Wernsen/Downloads/zhao2012.pdf:PDF},
  keywords   = {Integrated Development Environments, plug-ins, software testing, program analysis, skimmed},
  readstatus = {skimmed},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2157},
}

@InBook{deMoor2008,
  author     = {de Moor, Oege and Sereni, Damien and Verbaere, Mathieu and Hajiyev, Elnar and Avgustinov, Pavel and Ekman, Torbj{\"o}rn and Ongkingco, Neil and Tibble, Julian},
  editor     = {L{\"a}mmel, Ralf and Visser, Joost and Saraiva, Jo{\~a}o},
  pages      = {78--133},
  publisher  = {Springer Berlin Heidelberg},
  title      = {.QL: Object-Oriented Queries Made Easy},
  year       = {2008},
  address    = {Berlin, Heidelberg},
  isbn       = {978-3-540-88643-3},
  abstract   = {These notes are an introduction to .QL, an object-oriented query language for any type of structured data. We illustrate the use of .QL in assessing software quality, namely to find bugs, to compute metrics and to enforce coding conventions. The class mechanism of .QL is discussed in depth, and we demonstrate how it can be used to build libraries of reusable queries.},
  booktitle  = {Generative and Transformational Techniques in Software Engineering II: International Summer School, GTTSE 2007, Braga, Portugal, July 2-7, 2007. Revised Papers},
  doi        = {10.1007/978-3-540-88643-3_3},
  file       = {:deMoor2008 - .QL_ Object Oriented Queries Made Easy.pdf:PDF},
  keywords   = {read},
  readstatus = {read},
  url        = {https://doi.org/10.1007/978-3-540-88643-3_3},
}

@InProceedings{10.1145/1062455.1062492,
  author     = {Ko, Amy J. and Aung, Htet and Myers, Brad A.},
  booktitle  = {Proceedings of the 27th International Conference on Software Engineering},
  title      = {Eliciting Design Requirements for Maintenance-Oriented IDEs: A Detailed Study of Corrective and Perfective Maintenance Tasks},
  year       = {2005},
  address    = {New York, NY, USA},
  pages      = {126–135},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '05},
  abstract   = {Recently, several innovative tools have found their way into mainstream use in modern development environments. However, most of these tools have focused on creating and modifying code, despite evidence that most of programmers' time is spent understanding code as part of maintenance tasks. If new tools were designed to directly support these maintenance tasks, what types would be most helpful? To find out, a study of expert Java programmers using Eclipse was performed. The study suggests that maintenance work consists of three activities: (1) forming a working set of task-relevant code fragments; (2) navigating the dependencies within this working set; and (3) repairing or creating the necessary code. The study identified several trends in these activities, as well as many opportunities for new tools that could save programmers up to 35% of the time they currently spend on maintenance tasks.},
  doi        = {10.1145/1062455.1062492},
  file       = {:10.1145_1062455.1062492 - Eliciting Design Requirements for Maintenance Oriented IDEs_ a Detailed Study of Corrective and Perfective Maintenance Tasks.pdf:PDF},
  isbn       = {1581139632},
  keywords   = {software structure, examples, recommender, development environment framework, skimmed},
  location   = {St. Louis, MO, USA},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1062455.1062492},
}

@InProceedings{10.1109/PROMISE.2007.6,
  author     = {Dong, Jing and Zhao, Yajing},
  booktitle  = {Proceedings of the Third International Workshop on Predictor Models in Software Engineering},
  title      = {Experiments on Design Pattern Discovery},
  year       = {2007},
  address    = {USA},
  pages      = {12},
  publisher  = {IEEE Computer Society},
  series     = {PROMISE '07},
  abstract   = {Design patterns have been applied in many large software systems to help developers coping with recurring design problems. However, pattern-related information is generally lost in system source code. Discovering design pattern instances from source code can help to understand and analyze the software systems. In this paper, we present several experiments on design pattern discovery using our tool. We also compare the results of our experiments with other approaches and identify the need of benchmarks.},
  doi        = {10.1109/PROMISE.2007.6},
  file       = {:10.1109_PROMISE.2007.6 - Experiments on Design Pattern Discovery.pdf:PDF},
  isbn       = {0769529542},
  keywords   = {skimmed},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1109/PROMISE.2007.6},
}

@InProceedings{10.1145/143062.143148,
  author     = {Devanbu, Premkumar T.},
  booktitle  = {Proceedings of the 14th International Conference on Software Engineering},
  title      = {GENOA: A Customizable Language- and Front-End Independent Code Analyzer},
  year       = {1992},
  address    = {New York, NY, USA},
  pages      = {307–317},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '92},
  doi        = {10.1145/143062.143148},
  file       = {:10.1145_143062.143148 - GENOA_ a Customizable Language and Front End Independent Code Analyzer.pdf:PDF},
  isbn       = {0897915046},
  keywords   = {skimmed},
  location   = {Melbourne, Australia},
  numpages   = {11},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/143062.143148},
}

@Article{10.1145/1082983.1083253,
  author     = {Pettersson, Niklas},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Measuring Precision for Static and Dynamic Design Pattern Recognition as a Function of Coverage},
  year       = {2005},
  issn       = {0163-5948},
  month      = may,
  number     = {4},
  pages      = {1–7},
  volume     = {30},
  abstract   = {We strive to detect design pattern like patterns in software. This cannot be done efficiently with sufficient precision using only static analysis; we need to combine static and dynamic analysis. In this process, the pattern candidates produced from static analysis are monitored during executions of the software: Candidates detected by static analysis violating the expected dynamic protocol of the pattern are excluded. In this article, we investigate where to put effort when trying to perform high precision pattern detection in code. We do so by investigating which parameters are most important to improve the precision of the detection process: precision of initial static analysis or coverage of the dynamic analysis. Varying the precision of the dynamic analysis is a third important parameter, but this parameter is left as a constant during our experiments. The results show that simple behavioral protocols double the precision when 30% coverage is obtained. We also have indications that simple behavioral protocols give very high precision when high coverage is obtained. In such case, the quality of the static analysis is only interesting for precision if high coverage cannot be reached.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1082983.1083253},
  file       = {:10.1145_1082983.1083253 - Measuring Precision for Static and Dynamic Design Pattern Recognition As a Function of Coverage.pdf:PDF},
  issue_date = {July 2005},
  keywords   = {skimmed},
  numpages   = {7},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1082983.1083253},
}

@Article{10.1145/1041685.1029926,
  author     = {Zhang, Xiaofang and Young, Michal and Lasseter, John H. E. F.},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Refining Code-Design Mapping with Flow Analysis},
  year       = {2004},
  issn       = {0163-5948},
  month      = oct,
  number     = {6},
  pages      = {231–240},
  volume     = {29},
  abstract   = {We address the problem of refining and completing a partially specified high-level design model and a partially-defined mapping from source code to design model. This is related but not identical to tasks that have been automated with a variety of reverse engineering tools to support software modification tasks. We posited that set-based flow analysis algorithms would provide a convenient and powerful basis for refining an initial rough model and partial mapping, and in particular that the ability to compute fixed points of set equations would be useful in propagating constraints on the relations among the model, the mapping, and facts extracted from the implementation. Here we report our experience applying this approach to a modest but realistic example problem. We were successful in expressing a variety of useful transformations very succinctly as flow equations, and the propagation of recursively-defined constraints was indeed useful in refining the mapping from implementation to model. On the other hand, our experience highlights remaining challenges to make this an attractive approach for general use. Special measures are required to identify and remove inconsistent constraints before they propagate through a system. Also, while the required flow equations are succinct, they are also rather opaque; it is not obvious how their expressive power might be preserved in a more accessible notation.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1041685.1029926},
  file       = {:10.1145_1041685.1029926 - Refining Code Design Mapping with Flow Analysis.pdf:PDF},
  issue_date = {November 2004},
  keywords   = {architecture recovery, constraint propagation, flow analysis, skimmed},
  numpages   = {10},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1041685.1029926},
}

@InProceedings{10.1145/336512.336526,
  author     = {M\"{u}ller, Hausi A. and Jahnke, Jens H. and Smith, Dennis B. and Storey, Margaret-Anne and Tilley, Scott R. and Wong, Kenny},
  booktitle  = {Proceedings of the Conference on The Future of Software Engineering},
  title      = {Reverse Engineering: A Roadmap},
  year       = {2000},
  address    = {New York, NY, USA},
  pages      = {47–60},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '00},
  doi        = {10.1145/336512.336526},
  file       = {:10.1145_336512.336526 - Reverse Engineering_ a Roadmap.pdf:PDF},
  isbn       = {1581132530},
  keywords   = {software reengineering, software evolution, software tools, software maintenance, tool adoption, tool evaluation, program understanding, data reverse engineering, program comprehension, reverse engineering, software migration, software analysis, software engineering, skimmed},
  location   = {Limerick, Ireland},
  numpages   = {14},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/336512.336526},
}

@InProceedings{Beyer2003,
  author     = {Beyer, D. and Noack, A. and Lewerentz, C.},
  booktitle  = {10th {Working} {Conference} on {Reverse} {Engineering}, 2003. {WCRE} 2003. {Proceedings}.},
  title      = {Simple and efficient relational querying of software structures},
  year       = {2003},
  month      = nov,
  note       = {ISSN: 1095-1350},
  pages      = {216--225},
  doi        = {10.1109/WCRE.2003.1287252},
  file       = {:Beyer2003 - Simple and Efficient Relational Querying of Software Structures.pdf:PDF},
  issn       = {1095-1350},
  keywords   = {Software systems, Computer architecture, Data structures, Reverse engineering, Cloning, Calculus, Boolean functions, Design engineering, Systems engineering and theory, Pattern analysis, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/143062.143106,
  author     = {Consens, Mariano and Mendelzon, Alberto and Ryman, Arthur},
  booktitle  = {Proceedings of the 14th International Conference on Software Engineering},
  title      = {Visualizing and Querying Software Structures},
  year       = {1992},
  address    = {New York, NY, USA},
  pages      = {138–156},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '92},
  doi        = {10.1145/143062.143106},
  file       = {:10.1145_143062.143106 - Visualizing and Querying Software Structures.pdf:PDF},
  isbn       = {0897915046},
  keywords   = {skimmed},
  location   = {Melbourne, Australia},
  numpages   = {19},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/143062.143106},
}

@InProceedings{10.1145/1999747.1999761,
  author     = {Striewe, Michael and Goedicke, Michael},
  booktitle  = {Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education},
  title      = {Automated Checks on UML Diagrams},
  year       = {2011},
  address    = {New York, NY, USA},
  pages      = {38–42},
  publisher  = {Association for Computing Machinery},
  series     = {ITiCSE '11},
  abstract   = {Automated checks for software artefacts like UML diagrams used in automated assessment or tutoring systems do often rely on direct comparisons between a solution and a sample solution. This approach has drawbacks regarding flexibility in face of different possible solutions which are quite common in modeling tasks. This paper presents an alternative technique for checking UML class diagrams based on graph queries which promises to be more flexible.},
  doi        = {10.1145/1999747.1999761},
  file       = {:10.1145_1999747.1999761 - Automated Checks on UML Diagrams.pdf:PDF},
  isbn       = {9781450306973},
  keywords   = {intelligent tutoring systems, automated tutoring, diagram analysis, skimmed},
  location   = {Darmstadt, Germany},
  numpages   = {5},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1999747.1999761},
}

@InProceedings{10.1145/1869542.1869626,
  author     = {Silva Parreiras, Fernando and Walter, Tobias and Wende, Christian and Thomas, Edward},
  booktitle  = {Proceedings of the ACM International Conference Companion on Object Oriented Programming Systems Languages and Applications Companion},
  title      = {Bridging Software Languages and Ontology Technologies: Tutorial Summary},
  year       = {2010},
  address    = {New York, NY, USA},
  pages      = {311–315},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '10},
  abstract   = {Current model-driven development approaches allow for a more productive way of developing software systems. However, building tools and languages for software development still suffer a neglect of semantics in modeling and metamodeling.An interest to strengthen semantics in modeling and metamodeling that gained scientific and commercial attention is the integration of ontology technology and software development. Ontology formalisms for consistency validation and dynamic classification as well as semantic web technologies for enabling shared terminologies and automated reasoning provide means for leveraging metamodeling and language engineering.This tutorial summary (1) enlightens the potential of ontology and semantic web technology for modeling and metamodeling in software development, positioning it among modeling standards like UML, and MOF; and (2) illustrates ontology-enabled software development with real application scenarios in areas like software design patterns, domainspecific languages and variability management.},
  doi        = {10.1145/1869542.1869626},
  file       = {:1869542.1869626.pdf:PDF},
  isbn       = {9781450302401},
  keywords   = {UML, semantic web, ontology technology, DSL, software languages, model-driven development, skimmed},
  location   = {Reno/Tahoe, Nevada, USA},
  numpages   = {5},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1869542.1869626},
}

@InProceedings{10.1145/3196321.3197546,
  author     = {Porkol\'{a}b, Zolt\'{a}n and Brunner, Tibor and Krupp, D\'{a}niel and Csord\'{a}s, M\'{a}rton},
  booktitle  = {Proceedings of the 26th Conference on Program Comprehension},
  title      = {Codecompass: An Open Software Comprehension Framework for Industrial Usage},
  year       = {2018},
  address    = {New York, NY, USA},
  pages      = {361–369},
  publisher  = {Association for Computing Machinery},
  series     = {ICPC '18},
  abstract   = {CodeCompass is an open source LLVM/Clang-based tool developed by Ericsson Ltd. and E\"{o}tv\"{o}s Lor\'{a}nd University, Budapest to help the understanding of large legacy software systems. Based on the LLVM/Clang compiler infrastructure, CodeCompass gives exact information on complex C/C++ language elements like overloading, inheritance, the usage of variables and types, possible uses of function pointers and virtual functions - features that various existing tools support only partially. Steensgaard's and Andersen's pointer analysis algorithms are used to compute and visualize the use of pointers/references. The wide range of interactive visualizations extends further than the usual class and function call diagrams; architectural, component and interface diagrams are a few of the implemented graphs. To make comprehension more extensive, CodeCompass also utilizes build information to explore the system architecture as well as version control information.CodeCompass is regularly used by hundreds of designers and developers. Having a web-based, pluginable, extensible architecture, the CodeCompass framework can be an open platform to further code comprehension, static analysis and software metrics efforts. The source code and a tutorial is publicly available on GitHub, and a live demo is also available online.},
  doi        = {10.1145/3196321.3197546},
  file       = {:10.1145_3196321.3197546 - Codecompass_ an Open Software Comprehension Framework for Industrial Usage.pdf:PDF},
  isbn       = {9781450357142},
  keywords   = {software visualization, C/C++ programming language, code comprehension, skimmed},
  location   = {Gothenburg, Sweden},
  numpages   = {9},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/3196321.3197546},
}

@InProceedings{10.1145/581339.581390,
  author     = {Robillard, Martin P. and Murphy, Gail C.},
  booktitle  = {Proceedings of the 24th International Conference on Software Engineering},
  title      = {Concern Graphs: Finding and Describing Concerns Using Structural Program Dependencies},
  year       = {2002},
  address    = {New York, NY, USA},
  pages      = {406–416},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '02},
  abstract   = {Many maintenance tasks address concerns, or features, that are not well modularized in the source code comprising a system. Existing approaches available to help software developers locate and manage scattered concerns use a representation based on lines of source code, complicating the analysis of the concerns. In this paper, we introduce the Concern Graph representation that abstracts the implementation details of a concern and makes explicit the relationships between different parts of the concern. The abstraction used in a Concern Graph has been designed to allow an obvious and inexpensive mapping back to the corresponding source code. To investigate the practical tradeoffs related to this approach, we have built the Feature Exploration and Analysis tool (FEAT) that allows a developer to manipulate a concern representation extracted from a Java system, and to analyze the relationships of that concern to the code base. We have used this tool to find and describe concerns related to software change tasks. We have performed case studies to evaluate the feasibility, usability, and scalability of the approach. Our results indicate that Concern Graphs can be used to document a concern for change, that developers unfamiliar with Concern Graphs can use them effectively, and that the underlying technology scales to industrial-sized programs.},
  doi        = {10.1145/581339.581390},
  file       = {:C\:/Users/Wernsen/Downloads/581339.581390.pdf:PDF},
  isbn       = {158113472X},
  keywords   = {skimmed},
  location   = {Orlando, Florida},
  numpages   = {11},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/581339.581390},
}

@InProceedings{10.1145/1062455.1062599,
  author     = {Chung, William and Harrison, William and Kruskal, Vincent and Ossher, Harold and Sutton, Stanley M. and Tarr, Peri and Chapman, Matthew and Clement, Andrew and Hawkins, Helen and January, Sian},
  booktitle  = {Proceedings of the 27th International Conference on Software Engineering},
  title      = {The Concern Manipulation Environment},
  year       = {2005},
  address    = {New York, NY, USA},
  pages      = {666–667},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '05},
  abstract   = {No abstract available},
  doi        = {10.1145/1062455.1062599},
  file       = {:10.1145_1062455.1062599 - The Concern Manipulation Environment.pdf:PDF},
  isbn       = {1581139632},
  keywords   = {aspect-oriented software development, eclipse, skimmed},
  location   = {St. Louis, MO, USA},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1062455.1062599},
}

@Article{10.1145/2237796.2237808,
  author     = {Li, Dan and Li, Xiaoshan and Stolz, Volker},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Model Querying with Graphical Notation of QVT Relations},
  year       = {2012},
  issn       = {0163-5948},
  month      = jul,
  number     = {4},
  pages      = {1–8},
  volume     = {37},
  abstract   = {As a standard high-level model transformation language, QVT Relations defines a graphical notation, which provides a concise, intuitive way to specify transformations. However, QVT Relations relies only on the textual language OCL for model querying, leading to verbose and complicated OCL expressions. Here, we present a graphical model query facility based on the checking semantics and pattern matching of QVT Relations. The query facility also borrows from QVT Relations the graphical notation. In addition we propose an approach to map the queries into XSLT to facilitate their execution. We have developed a tool for designing the queries and automatically generating the XSLT programs.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2237796.2237808},
  file       = {:10.1145_2237796.2237808 - Model Querying with Graphical Notation of QVT Relations.pdf:PDF},
  issue_date = {July 2012},
  keywords   = {graphical model querying, OCL, UML, XPath, QVT, skimmed},
  numpages   = {8},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2237796.2237808},
}

@Article{10.1145/381788.316185,
  author     = {Balmas, Fran\c{c}oise},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Query by Outlines: A New Paradigm to Help Manage Programs},
  year       = {1999},
  issn       = {0163-5948},
  month      = sep,
  number     = {5},
  pages      = {86–94},
  volume     = {24},
  abstract   = {We propose a new paradigm to query information about programs, namely query by outlines. This paradigm relies on an outlining model that conceptually describe units of code according to the computations they perform. Outlines are automatically constructed by our system PRISME for C and Lisp programs. Currently, both our model and our system are restricted to loops.QBO is a prototype tool that implements the query by outline paradigm. It proposes to browse the loops of a program directly through their outline, and allows to restrict these loops to browse with queries expressed as constraints on the outlines. Thus it enables to answer questions such as "where is this variable modified?", "where is this kind of computation performed?", or "are there many places where this computation is performed?".In this paper, we sketch our outlining model, introduce QBO and argue that query by outline is a helpful paradigm to manage programs.},
  address    = {New York, NY, USA},
  doi        = {10.1145/381788.316185},
  file       = {:10.1145_381788.316185 - Query by Outlines_ a New Paradigm to Help Manage Programs.pdf:PDF},
  issue_date = {Sept. 1999},
  keywords   = {skimmed},
  numpages   = {9},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/381788.316185},
}

@InProceedings{Stein2004,
  author     = {Stein, Dominik and Hanenberg, Stefan and Unland, Rainer},
  booktitle  = {«{UML}» 2004 — {The} {Unified} {Modeling} {Language}. {Modeling} {Languages} and {Applications}},
  title      = {Query {Models}},
  year       = {2004},
  address    = {Berlin, Heidelberg},
  editor     = {Baar, Thomas and Strohmeier, Alfred and Moreira, Ana and Mellor, Stephen J.},
  pages      = {98--112},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {The need for querying software artifacts is a new emerging design issue in modern software development. Novel techniques such as Model-Driven Architecture or Aspect-Oriented Software Development heavily depend on powerful designation means to allocate elements in software artifacts, which are then either modified by transformation or enhanced by weaving processes. In this paper we present a new modeling notation for representing queries using the UML. We introduce special symbols for common selection purposes and specify their OCL selection semantics, which may be executed on existing UML models in order to allocate all selected model elements therein. By doing so, we aim to give forth the advantages of modeling to query design: Our query models facilitate the specification of queries independent from particular programming languages, ease their comprehension, and support their validation in a modeling context.},
  doi        = {10.1007/978-3-540-30187-5_8},
  file       = {:Stein2004 - Query Models.pdf:PDF;:C\:/Users/Wernsen/Downloads/10.1007_b101232(1).pdf:PDF},
  isbn       = {9783540301875},
  keywords   = {Unify Modeling Language , Object Constraint Language , Query Model , Graphical Notation , Unify Modeling Language Model , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1134285.1134420,
  author     = {Beyer, Dirk},
  booktitle  = {Proceedings of the 28th International Conference on Software Engineering},
  title      = {Relational Programming with CrocoPat},
  year       = {2006},
  address    = {New York, NY, USA},
  pages      = {807–810},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '06},
  abstract   = {Many structural analyses of software systems are naturally formalized as relational queries, for example, the detection of design patterns, patterns of problematic design, code clones, dead code, and differences between the as-built and the as-designed architecture. This paper describes CrocoPat, an application-independent tool for relational programming. Through its efficiency and its expressive language, CrocoPat enables practically important analyses of real-world software systems that are not possible with other graph analysis tools, in particular analyses that involve transitive closures and the detection of patterns in graphs. The language is easy to use, because it is based on the well-known first-order predicate logic. The tool is easy to integrate into other software systems, because it is a small command-line tool that uses a simple text format for input and output of relations.},
  doi        = {10.1145/1134285.1134420},
  file       = {:10.1145_1134285.1134420 - Relational Programming with CrocoPat.pdf:PDF},
  isbn       = {1595933751},
  keywords   = {relational algebra, transitive closure, graph models, software analysis, BDD, predicate logic, pattern matching, skimmed},
  location   = {Shanghai, China},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1134285.1134420},
}

@Article{10.1145/251759.251849,
  author     = {van den Brand, M. G. J. and Klint, P. and Verhoef, C.},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Reverse Engineering and System Renovation—an Annotated Bibliography},
  year       = {1997},
  issn       = {0163-5948},
  month      = jan,
  number     = {1},
  pages      = {57–68},
  volume     = {22},
  abstract   = {To facilitate research in the field of reverse engineering and system renovation we have compiled an annotated bibliography. We put the contributions not only in alphabetical order but also grouped by topic so that readers focusing on a certain topic can read their annotations in the alphabetical listing. We also compiled an annotated list of pointers to information about reverse engineering and system renovation that can be reached via Internet. For the sake of ease we also incorporated a brief introduction to the field of reverse engineering.},
  address    = {New York, NY, USA},
  doi        = {10.1145/251759.251849},
  file       = {:251759.251849.pdf:PDF},
  issue_date = {Jan. 1997},
  keywords   = {system renovation, reverse engineering, annotated bibliography, skimmed},
  numpages   = {12},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/251759.251849},
}

@InProceedings{10.1145/587051.587067,
  author     = {Chu-Carroll, Mark C. and Wright, James and Shields, David},
  booktitle  = {Proceedings of the 10th ACM SIGSOFT Symposium on Foundations of Software Engineering},
  title      = {Supporting Aggregation in Fine Grained Software Configuration Management},
  year       = {2002},
  address    = {New York, NY, USA},
  pages      = {99–108},
  publisher  = {Association for Computing Machinery},
  series     = {SIGSOFT '02/FSE-10},
  abstract   = {Fine-grained software configuration management offers substantial benefits for large-scale collaborative software development, enabling a variety of interesting and useful features including complexity management, support for aspect-oriented software development, and support for communication and coordination within software engineering teams, as described in [4]. However, fine granularity by itself is not sufficient to achieve these benefits. Most of the benefits of fine granularity result from the ability to combine fine-grained artifacts in various ways: supporting multiple overlapping organizations of program source by combining fine-grained artifacts into virtual source files (VSFs); supporting coordination by allowing developers to precisely mark the set of artifacts affected by a change; associating products from different phases of the development process; etc.In this paper, we describe how a general aggregation mechanism can be used to support the various functionality enabled by fine grained SCM. We present a set of requirements that an aggregation facility must provide in order to yield these benefits, and we provide a description of the implementation of such an aggregation system in our experimental SCM system.},
  doi        = {10.1145/587051.587067},
  file       = {:10.1145_587051.587067 - Supporting Aggregation in Fine Grained Software Configuration Management.pdf:PDF},
  isbn       = {1581135149},
  keywords   = {aggregation, dynamic program organization, fine grained storage, skimmed},
  location   = {Charleston, South Carolina, USA},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/587051.587067},
}

@InProceedings{Herold2008,
  author     = {Herold, Sebastian},
  booktitle  = {On the {Move} to {Meaningful} {Internet} {Systems}: {OTM} 2008 {Workshops}},
  title      = {Towards {Checking} {Architectural} {Rules} in {Component}-{Based} {Design}},
  year       = {2008},
  address    = {Berlin, Heidelberg},
  editor     = {Meersman, Robert and Tari, Zahir and Herrero, Pilar},
  pages      = {473--478},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Current component-based software development (CBSD) approaches rather focus on the design of software systems than on the system’s high-level, coarse-grained architecture. They provide modeling techniques to describe the concrete structure of components and their interfaces, how they are connected and how they interact. As an effect of their focus on the design, they are not appropriate to explicitly model the fundamental rules of a software architecture like architectural patterns or reference architectures that restrict the component-based design.In this paper, we are going to identify some architectural rules in a small example. Furthermore, we will outline how these rules can be used to constrain the component design based upon a modeling approach called DisCComp.},
  doi        = {10.1007/978-3-540-88875-8_69},
  file       = {:Herold2008 - Towards Checking Architectural Rules in Component Based Design.pdf:PDF;:C\:/Users/Wernsen/Downloads/10.1007_978-3-540-88875-8.pdf:PDF},
  isbn       = {9783540888758},
  keywords   = {Component-Based Software Development , Software Architecture , Architectural Rules , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@InProceedings{10.1145/3275219.3275221,
  author     = {Zou, Yanzhen and Ling, Chunyang and Lin, Zeqi and Xie, Bing},
  booktitle  = {Proceedings of the Tenth Asia-Pacific Symposium on Internetware},
  title      = {Graph Embedding Based Code Search in Software Project},
  year       = {2018},
  address    = {New York, NY, USA},
  publisher  = {Association for Computing Machinery},
  series     = {Internetware '18},
  abstract   = {Source code search is one of the most important methods to study and reuse software project. Currently, natural language based code search mainly faces the following two challenges: 1) More accurate search results are required when software projects evolve to be more heterogeneous and complex. 2) The semantic relationships between code elements (classes, methods, etc.) need to be illustrated so that developers could better understand their usage scenarios. To deal with these issues, we propose a novel approach to searching a software project's source code based on graph embedding. First, we build a software project's code graph automatically from its source code and represent each code element in the code graph with graph embedding. Second, we search code graph with natural language questions, return corresponding subgraph that composed of relevant code elements and their associated relationships, as the best answer of the search question. In experiments, we select two famous open source projects, Apache Lucene and POI, as examples to perform source code search tasks. The experimental results show that our approach improves F1-score by 10% than existing shortest path based code graph search approach, while reduces average response time about 60 times.},
  articleno  = {1},
  doi        = {10.1145/3275219.3275221},
  file       = {:C\:/Users/Wernsen/Downloads/10.1145_3275219.3275221 - Graph Embedding Based Code Search in Software Project.pdf:PDF},
  isbn       = {9781450365901},
  keywords   = {Graph embedding, Code graph, Software reuse, Code search, skimmed},
  location   = {Beijing, China},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/3275219.3275221},
}

@InProceedings{10.1145/1321631.1321669,
  author     = {Dagenais, Barth\'{e}l\'{e}my and Breu, Silvia and Warr, Fr\'{e}d\'{e}ric Weigand and Robillard, Martin P.},
  booktitle  = {Proceedings of the Twenty-Second IEEE/ACM International Conference on Automated Software Engineering},
  title      = {Inferring Structural Patterns for Concern Traceability in Evolving Software},
  year       = {2007},
  address    = {New York, NY, USA},
  pages      = {254–263},
  publisher  = {Association for Computing Machinery},
  series     = {ASE '07},
  abstract   = {As part of the evolution of software systems, effort is often invested to discover in what parts of the source code a feature (or other concern) is implemented. Unfortunately, knowledge about a concern's implementation can become invalid as the system evolves. We propose to mitigate this problem by automatically inferring structural patterns among the elements identified as relevant to a concern's implementation. We then document the inferred patterns as rules that can be checked as the source code evolves. Checking whether structural patterns hold across different versions of a system enables the automatic identification of new elements related to a documented concern. We implemented our technique for JAVA in an Eclipse plug-in called ISIS and applied it to a number of concerns. With a case study spanning 34 versions of the development history of an open-source system, we show how our approach supports the tracking of a concern's implementation through modifications such as extensions and refactorings},
  doi        = {10.1145/1321631.1321669},
  file       = {:10.1145_1321631.1321669 - Inferring Structural Patterns for Concern Traceability in Evolving Software.pdf:PDF},
  isbn       = {9781595938824},
  keywords   = {software evolution, concern tracking, intension template, feature location, skimmed},
  location   = {Atlanta, Georgia, USA},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1321631.1321669},
}

@InProceedings{10.1145/1028664.1028670,
  author     = {McCormick, Edward and De Volder, Kris},
  booktitle  = {Companion to the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications},
  title      = {JQuery: Finding Your Way through Tangled Code},
  year       = {2004},
  address    = {New York, NY, USA},
  pages      = {9–10},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '04},
  abstract   = {A typical IDE based exploration of an OOP system will often involve multiple searches through class hierarchies, field accesses, method calls, regular expression matches and more. Developers who must follow connections between these disconnected views may find great difficulty in combining the capabilities of each view and may as well suffer significant disorientation due to loss of context while switching. toolname is a flexible, query-based source code browser that alleviates this disorientation by allowing the user to explore the various types of structural relationships between elements of the code without the distraction of switching tools. Using toolname, a developer can define his or her own top-level browsers on-the-fly by formulating logic queries and running them against the source code. Elements in the tree can then be queried individually in the same manner, allowing further exploration of the complex web of relationships that exist between scattered elements of code.},
  doi        = {10.1145/1028664.1028670},
  file       = {:C\:/Users/Wernsen/Downloads/10.1145_1028664.1028670 - JQuery_ Finding Your Way through Tangled Code.pdf:PDF},
  isbn       = {1581138334},
  keywords   = {exploration, query engine, browser, navigation, visualization, skimmed},
  location   = {Vancouver, BC, CANADA},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1028664.1028670},
}

@InProceedings{10.1145/1117696.1117701,
  author     = {Sinha, Vineet and Karger, David and Miller, Rob},
  booktitle  = {Proceedings of the 2005 OOPSLA Workshop on Eclipse Technology EXchange},
  title      = {Relo: Helping Users Manage Context during Interactive Exploratory Visualization of Large Codebases},
  year       = {2005},
  address    = {New York, NY, USA},
  pages      = {21–25},
  publisher  = {Association for Computing Machinery},
  series     = {eclipse '05},
  abstract   = {As software systems grow in size and use more third-party libraries and frameworks, the need for developers to understand unfamiliar large codebases is rapidly increasing. In this paper, we present a tool, Relo, that supports developers' understanding by allowing interactive exploration of code. As the developer explores relationships found in the code, Relo builds and automatically manages the context in a visualization, thereby helping build the developer's mental representation of the code. Developers can group viewed artifacts or use the viewed items to ask Relo for further exploration suggestions. Relo is built as an Eclipse plug-in integrated into the Java Tooling (JDT), and uses a standard, RDF, based backend allowing for maintaining code relationships and performing inferences about the relationships.},
  doi        = {10.1145/1117696.1117701},
  file       = {:10.1145_1117696.1117701 - Relo_ Helping Users Manage Context during Interactive Exploratory Visualization of Large Codebases.pdf:PDF},
  isbn       = {1595933425},
  keywords   = {large software systems, program understanding, program comprehension, software visualization, skimmed},
  location   = {San Diego, California},
  numpages   = {5},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1117696.1117701},
}

@InProceedings{10.1145/1101908.1101937,
  author     = {Shepherd, David and Palm, Jeffrey and Pollock, Lori and Chu-Carroll, Mark},
  booktitle  = {Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering},
  title      = {Timna: A Framework for Automatically Combining Aspect Mining Analyses},
  year       = {2005},
  address    = {New York, NY, USA},
  pages      = {184–193},
  publisher  = {Association for Computing Machinery},
  series     = {ASE '05},
  abstract   = {To realize the benefits of Aspect Oriented Programming (AOP), developers must refactor active and legacy code bases into an AOP language. When refactoring, developers first need to identify refactoring candidates, a process called aspect mining. Humans perform mining by using a variety of clues to determine which code to refactor. However, existing approaches to automating the aspect mining process focus on developing analyses of a single program characteristic. Each analysis often finds only a subset of possible refactoring candidates and is unlikely to find candidates which humans find by combining analyses. In this paper, we present Timna, a framework for enabling the automatic combination of aspect mining analyses. The key insight is the use of machine learning to learn when to refactor, from vetted examples. Experimental evaluation of the cost-effectiveness of Timna in comparison to Fan-in, a leading aspect mining analysis, indicates that such a framework for automatically combining analyses is very promising.},
  doi        = {10.1145/1101908.1101937},
  file       = {:10.1145_1101908.1101937 - Timna_ a Framework for Automatically Combining Aspect Mining Analyses.pdf:PDF},
  isbn       = {1581139934},
  keywords   = {aspect mining, machine learning, program analysis, reverse engineering, skimmed},
  location   = {Long Beach, CA, USA},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1101908.1101937},
}

@InProceedings{10.1145/2451592.2451594,
  author     = {Aly, Mohamed and Charfi, Anis and Wu, Di and Mezini, Mira},
  booktitle  = {Proceedings of the 1st Workshop on Comprehension of Complex Systems},
  title      = {Understanding Multilayered Applications for Building Extensions},
  year       = {2013},
  address    = {New York, NY, USA},
  pages      = {1–6},
  publisher  = {Association for Computing Machinery},
  series     = {CoCoS '13},
  abstract   = {Modern software applications typically consist of several logical layers (for example user interface, databases, business process, code, etc.). Software is usually delivered by a software provider to support a certain application domain through a set of predefined functionalities. A user acquiring the software can obtain extra functionalities through extensions. Extensions can be developed by the software provider, by the customer, or a third-party and then integrated with the core software. In order to enable core software for accommodating extensions, software providers must give the right means for enabling developers to build extensions. Based on the new functionalities required, developers building extensions usually consider different layers of the core software when developing extensions. For example, a simple user interface extension in a business application would need a developer to consider extensible artifacts from underlying user interfaces, business processes, databases, and code. Usually a developer relies on code based artifacts, documentation, examples, and/or training in order to be able to understand the extensibility model of a core application. Various program comprehension tools have helped developers in carrying out general development tasks. However, most of the tools focus on the code level, lack the support for multilayered applications, and do not support extensibility. In this paper we discuss the challenges and problems of the developers when building extensions for multilayered applications. We also assess the feasibility of several program comprehension tools in addressing these challenges.},
  doi        = {10.1145/2451592.2451594},
  file       = {:2451592.2451594.pdf:PDF},
  isbn       = {9781450318631},
  keywords   = {IDE, comprehension, multilayer, extensibility, skimmed},
  location   = {Fukuoka, Japan},
  numpages   = {6},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2451592.2451594},
}

@Article{10.1145/390011.808258,
  author     = {Linton, Mark A.},
  journal    = {SIGPLAN Not.},
  title      = {Implementing Relational Views of Programs},
  year       = {1984},
  issn       = {0362-1340},
  month      = apr,
  number     = {5},
  pages      = {132–140},
  volume     = {19},
  abstract   = {Configurations, versions, call graphs, and slices are all examples of views, or cross-sections, of programs. To provide a powerful mechanism for defining, retrieving, and updating these views, the OMEGA programming system uses a relational database system to manage all program information.We have built a prototype implementation of the OMEGA-database system interface. This implementation includes the design of a relational schema for a Pascal-like language, a program for taking software stored as text and translating it into the database representation, and a simple, pointing-oriented user interface. Initial performance measurements indicate that response is too slow in our current environment, but that advances in database software technology and hardware should make response fast enough in the near future.},
  address    = {New York, NY, USA},
  doi        = {10.1145/390011.808258},
  file       = {:10.1145_390011.808258 - Implementing Relational Views of Programs.pdf:PDF},
  issue_date = {May 1984},
  keywords   = {skimmed},
  numpages   = {9},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/390011.808258},
}

@InProceedings{10.5555/962198.962229,
  author    = {Javey, Shahram and Mitsui, Kin'ichi and Nakamura, Hiroaki and Ohira, Tsuyoshi and Yasuda, Kazu and Kuse, Kazushi and Kamimura, Tsutomu and Helm, Richard},
  booktitle = {Proceedings of the 1992 Conference of the Centre for Advanced Studies on Collaborative Research - Volume 1},
  title     = {Architecture of the XL C++ Browser},
  year      = {1992},
  pages     = {369–379},
  publisher = {IBM Press},
  series    = {CASCON '92},
  abstract  = {In this paper we describe the high-level architecture for a distributed static analyzer for the C++ programming language. Key features of this technology are its support for semantic queries -- queries that make use of the C++ semantics to interpret information about programs; its use of rules for describing the relations between the program symbols; and its capability to browse remote databases across a network.},
  location  = {Toronto, Ontario, Canada},
  numpages  = {11},
}

@Article{10.1145/191843.191927,
  author     = {Sagonas, Konstantinos and Swift, Terrance and Warren, David S.},
  journal    = {SIGMOD Rec.},
  title      = {XSB as an Efficient Deductive Database Engine},
  year       = {1994},
  issn       = {0163-5808},
  month      = may,
  number     = {2},
  pages      = {442–453},
  volume     = {23},
  abstract   = {This paper describes the XSB system, and its use as an in-memory deductive database engine. XSB began from a Prolog foundation, and traditional Prolog systems are known to have serious deficiencies when used as database systems. Accordingly, XSB has a fundamental bottom-up extension, introduced through tabling (or memoing)[4], which makes it appropriate as an underlying query engine for deductive database systems. Because it eliminates redundant computation, the tabling extension makes XSB able to compute all modularly stratified datalog programs finitely and with polynomial data complexity. For non-stratified programs, a meta-interpreter with the same properties is provided. In addition XSB significantly extends and improves the indexing capabilities over those of standard Prolog.  Finally, its syntactic basis in HiLog [2], lends it flexibility for data modelling.The implementation of XSB derives from the WAM [25], the most common Prolog engine. XSB inherits the WAM's efficiency and can take advantage of extensive compiler technology developed for Prolog. As a result, performance comparisons indicate that XSB is significantly faster than other deductive database systems for a wide range of queries and stratified rule sets. XSB is under continuous development, and version 1.3 is available through anonymous ftp.},
  address    = {New York, NY, USA},
  doi        = {10.1145/191843.191927},
  file       = {:10.1145_191843.191927 - XSB As an Efficient Deductive Database Engine.pdf:PDF},
  issue_date = {June 1994},
  keywords   = {skimmed},
  numpages   = {12},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/191843.191927},
}

@InProceedings{Paul1994a,
  author     = {{Paul} and {Prakash}},
  booktitle  = {Proceedings 1994 {International} {Conference} on {Software} {Maintenance}},
  title      = {Querying source code using an algebraic query language},
  year       = {1994},
  month      = sep,
  pages      = {127--136},
  abstract   = {Querying and analyzing source code interactively is a critical task in reverse engineering and program understanding. Current source code query systems lack sufficient formalism and offer limited query capabilities. We introduce the formal framework of Source Code Algebra (SCA), and outline a source code query system based on it. SCA provides a formal data model for source code, an algebraic expression-based query language, and opportunities for query optimization. An algebraic model of source code addresses the issues of conceptual integrity, expressive power, and performance of a source code query system within a unified framework.{\textless}{\textgreater}},
  doi        = {10.1109/ICSM.1994.336782},
  file       = {:Paul1994a - Querying Source Code Using an Algebraic Query Language.pdf:PDF},
  keywords   = {Software maintenance, Query languages, Software fault diagnosis, Computer-aided software engineering, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.5555/1267950.1267968,
  author     = {Crew, Roger F.},
  booktitle  = {Proceedings of the Conference on Domain-Specific Languages on Conference on Domain-Specific Languages (DSL), 1997},
  title      = {ASTLOG: A Language for Examining Abstract Syntax Trees},
  year       = {1997},
  address    = {USA},
  pages      = {18},
  publisher  = {USENIX Association},
  series     = {DSL'97},
  abstract   = {We desired a facility for locating/analyzing syntactic artifacts in abstract syntax trees of C/C++ programs, similar to the facility grep or awk provides for locating artifacts at the lexical level. Prolog, with its implicit pattern-matching and backtracking capabilities, is a natural choice for such an application. We have developed a Prolog variant that avoids the overhead of translating the source syntactic structures into the form of a Prolog database; this is crucial to obtaining acceptable performance on large programs. An interpreter for this language has been implemented and used to find various kinds of syntactic bugs and other questionable constructs in real programs like Microsoft SQL server (450Klines) and Microsoft Word (2Mlines) in time comparable to the runtime of the actual compiler.The model in which terms are matched against an implicit current object, rather than simply proven against a database of facts, leads to a distinct "inside-out functional" programming style that is quite unlike typical Prolog, but one that is, in fact, well-suited to the examination of trees. Also, various second-order Prolog set-predicates may be implemented via manipulation of the current object, thus retaining an important feature without entailing that the database be dynamically extensible as the usual implementation does.},
  file       = {:10.5555_1267950.1267968 - ASTLOG_ a Language for Examining Abstract Syntax Trees.pdf:PDF},
  keywords   = {skimmed},
  location   = {Santa Barbara, California},
  numpages   = {1},
  readstatus = {skimmed},
}

@Article{10.1145/249069.231399,
  author     = {Dawson, Steven and Ramakrishnan, C. R. and Warren, David S.},
  journal    = {SIGPLAN Not.},
  title      = {Practical Program Analysis Using General Purpose Logic Programming Systems—a Case Study},
  year       = {1996},
  issn       = {0362-1340},
  month      = may,
  number     = {5},
  pages      = {117–126},
  volume     = {31},
  abstract   = {Many analysis problems can be cast in the form of evaluating minimal models of a logic program. Although such formulations are appealing due to their simplicity and declarativeness, they have not been widely used in practice because, either existing logic programming systems do not guarantee completeness, or those that do have been viewed as too inefficient for integration into a compiler. The objective of this paper is to re-examine this issue in the context of recent advances in implementation technologies of logic programming systems.We find that such declarative formulations can indeed be used in practical systems, when combined with the appropriate tool for evaluation. We use existing formulations of analysis problems --- groundness analysis of logic programs, and strictness analysis of functional programs --- in this case study, and the XSB system, a table-based logic programming system, as the evaluation tool of choice. We give experimental evidence that the resultant groundness and strictness analysis systems are practical in terms of both time and space. In terms of implementation effort, the analyzers took less than 2 man-weeks (in total), to develop, optimize and evaluate. The analyzer itself consists of about 100 lines of tabled Prolog code and the entire system, including the components to read and preprocess input programs and to collect the analysis results, consists of about 500 lines of code.},
  address    = {New York, NY, USA},
  doi        = {10.1145/249069.231399},
  file       = {:10.1145_249069.231399 - Practical Program Analysis Using General Purpose Logic Programming Systems—a Case Study.pdf:PDF},
  issue_date = {May 1996},
  keywords   = {skimmed},
  numpages   = {10},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/249069.231399},
}

@InProceedings{Eichberg2007OpenID,
  author     = {Michael Eichberg},
  title      = {Open integrated development and analysis environments},
  year       = {2007},
  file       = {:Eichberg2007OpenID - Open Integrated Development and Analysis Environments.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Eichberg2007,
  author     = {Eichberg, Michael and Kahl, Matthias and Saha, Diptikalyan and Mezini, Mira and Ostermann, Klaus},
  booktitle  = {Practical {Aspects} of {Declarative} {Languages}},
  title      = {Automatic {Incrementalization} of {Prolog} {Based} {Static} {Analyses}},
  year       = {2007},
  address    = {Berlin, Heidelberg},
  editor     = {Hanus, Michael},
  pages      = {109--123},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Modern development environments integrate various static analyses into the build process. Analyses that analyze the whole project whenever the project changes are impractical in this context. We present an approach to automatic incrementalization of analyses that are specified as tabled logic programs and evaluated using incremental tabled evaluation, a technique for efficiently updating memo tables in response to changes in facts and rules. The approach has been implemented and integrated into the Eclipse IDE. Our measurements show that this technique is effective for automatically incrementalizing a broad range of static analyses.},
  doi        = {10.1007/978-3-540-69611-7_7},
  file       = {:Eichberg2007 - Automatic Incrementalization of Prolog Based Static Analyses.pdf:PDF;:C\:/Users/Wernsen/Downloads/10.1.1.433.2951.pdf:PDF},
  isbn       = {9783540696117},
  keywords   = {Incremental Evaluation , Incremental Algorithm , Strongly Connect Component , General Logic Program , Alias Analysis , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@Article{10.1145/996893.996861,
  author     = {Lhot\'{a}k, Ond\v{r}ej and Hendren, Laurie},
  journal    = {SIGPLAN Not.},
  title      = {Jedd: A BDD-Based Relational Extension of Java},
  year       = {2004},
  issn       = {0362-1340},
  month      = jun,
  number     = {6},
  pages      = {158–169},
  volume     = {39},
  abstract   = {In this paper we present Jedd, a language extension to Java that supports a convenient way of programming with Binary Decision Diagrams (BDDs). The Jedd language abstracts BDDs as database-style relations and operations on relations, and provides static type rules to ensure that relational operations are used correctly.The paper provides a description of the Jedd language and reports on the design and implementation of the Jedd translator and associated runtime system. Of particular interest is the approach to assigning attributes from the high-level relations to physical domains in the underlying BDDs, which is done by expressing the constraints as a SAT problem and using a modern SAT solver to compute the solution. Further, a runtime system is defined that handles memory management issues and supports a browsable profiling tool for tuning the key BDD operations.The motivation for designing Jedd was to support the development of whole program analyses based on BDDs, and we have used Jedd to express five key interrelated whole program analyses in our Soot compiler framework. We provide some examples of this application and discuss our experiences using Jedd.},
  address    = {New York, NY, USA},
  doi        = {10.1145/996893.996861},
  file       = {:10.1145_996893.996861 - Jedd_ a BDD Based Relational Extension of Java.pdf:PDF},
  issue_date = {May 2004},
  keywords   = {binary decision diagrams, relations, Java, boolean formula satisfiability, program analysis, language design, skimmed},
  numpages   = {12},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/996893.996861},
}

@InProceedings{DeVolder1999,
  author     = {De Volder, Kris and D’Hondt, Theo},
  booktitle  = {Meta-{Level} {Architectures} and {Reflection}},
  title      = {Aspect-{Oriented} {Logic} {Meta} {Programming}},
  year       = {1999},
  address    = {Berlin, Heidelberg},
  editor     = {Cointe, Pierre},
  pages      = {250--272},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {We propose to use a logic meta-system as a general frame-work for aspect-oriented programming. We illustrate our approach with the implementation of a simplified version of the cool aspect language for expressing synchronization of Java programs. Using this case as an example we illustrate the principle of aspect-oriented logic meta programming and how it is useful for implementing weavers on the one hand and on the other hand also allows users of aop to fine-tune, extend and adapt an aspect language to their specific needs.},
  doi        = {10.1007/3-540-48443-4_22},
  file       = {:DeVolder1999 - Aspect Oriented Logic Meta Programming (1).pdf:PDF;:10.1007_3-540-48443-4.pdf:PDF},
  isbn       = {9783540484431},
  keywords   = {Logic Program , Mutual Exclusion , Logic Rule , Logic Fact , Logic Language , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@InProceedings{eichberg2004pointcuts,
  author       = {Eichberg, Michael and Mezini, Mira and Ostermann, Klaus},
  booktitle    = {Asian Symposium on Programming Languages and Systems},
  title        = {Pointcuts as functional queries},
  year         = {2004},
  organization = {Springer},
  pages        = {366--381},
  file         = {:eichberg2004pointcuts - Pointcuts As Functional Queries.pdf:PDF},
  keywords     = {skimmed},
  readstatus   = {skimmed},
}

@Article{Feijs1998,
  author     = {Feijs, L. and Krikhaar, R. and Ommering, R. Van},
  journal    = {Software: Practice and Experience},
  title      = {A relational approach to support software architecture analysis},
  year       = {1998},
  issn       = {1097-024X},
  number     = {4},
  pages      = {371--400},
  volume     = {28},
  abstract   = {This paper reports on our experience with a relational approach to support the analysis of existing software architectures. The analysis options provide for visualization and view calculation. The approach has been applied for reverse engineering. It is also possible to check concrete designs against architecture-related rules. The paper surveys the theory, the tools and some of the applications developed so far. © 1998 John Wiley \& Sons, Ltd.},
  copyright  = {Copyright © 1998 John Wiley \& Sons, Ltd.},
  doi        = {10.1002/(SICI)1097-024X(19980410)28:4<371::AID-SPE154>3.0.CO;2-1},
  file       = {:Feijs1998 - A Relational Approach to Support Software Architecture Analysis.pdf:PDF;:feijs1998(1).pdf:PDF},
  keywords   = {software structuring, reverse engineering, relational algebra, software architecture, skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-024X%2819980410%2928%3A4%3C371%3A%3AAID-SPE154%3E3.0.CO%3B2-1},
  urldate    = {2021-06-17},
}

@InProceedings{Holt1998,
  author     = {Holt, R.C.},
  booktitle  = {Proceedings {Fifth} {Working} {Conference} on {Reverse} {Engineering} ({Cat}. {No}.{98TB100261})},
  title      = {Structural manipulations of software architecture using {Tarski} relational algebra},
  year       = {1998},
  month      = oct,
  pages      = {210--219},
  abstract   = {A software architecture is typically drawn as a nested set of box and arrow diagrams. The boxes represent components of the software system and the edges represent interactions These diagrams correspond to typed graphs, in which there are a number of types or colors of edges, and in which there is a distinguished contain relation that represents the system hierarchy (the nesting of boxes). During reverse engineering, one often transforms such diagrams in various ways to make them easier to understand. These transformations include edge aggregation, box abstraction (closing a box to hide its contents), and box separation (separating a box from its surrounding system). Such transformations are essential in helping make software architecture diagrams useful in practice. Paper shows how structural manipulations such as these can be specified and automatically carried out in a notation based on Tarski's relational algebra. The operators in this algebra include relational composition, union, subtraction, etc. These operators are supported in a language called Grok. Grok scripts have been used in manipulating the graphs for large scale software systems, such as Linux, to help in program visualization and understanding.},
  doi        = {10.1109/WCRE.1998.723191},
  file       = {:Holt1998 - Structural Manipulations of Software Architecture Using Tarski Relational Algebra.pdf:PDF},
  keywords   = {Software architecture, Algebra, Software systems, Reverse engineering, Computer architecture, Program processors, Large-scale systems, Linux, Visualization, Concrete, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Klint2003,
  author     = {Klint, P.},
  booktitle  = {11th {IEEE} {International} {Workshop} on {Program} {Comprehension}, 2003.},
  title      = {How understanding and restructuring differ from compiling -a rewriting perspective},
  year       = {2003},
  month      = may,
  note       = {ISSN: 1092-8138},
  pages      = {2--11},
  abstract   = {Syntactic and semantic analysis are established topics in the area of compiler construction. Their application to the understanding and restructuring of large software systems reveals, however that they have various shortcomings that need to be addressed. In this paper we study these shortcomings and propose several solutions. First, grammar recovery and grammar composition are discussed as well as the symbiosis of lexical syntax and context-free syntax. Next, it is shown how a relational calculus can be defined by way of term rewriting and how a fusion of term rewriting and this relational calculus can be obtained to provide semantics-directed querying and restructuring. Finally, we discuss how the distance between concrete syntax and abstract syntax can be minimized for the benefit of restructuring. In particular we pay attention to origin tracking, a systematic technique to maintain a mapping between the output and the input of the rewriting process. Along the way, opportunities for further research are indicated.},
  doi        = {10.1109/WPC.2003.1199184},
  file       = {:Klint2003 - How Understanding and Restructuring Differ from Compiling a Rewriting Perspective.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Software systems, Calculus, Application software, Symbiosis, Concrete, Books, Optimizing compilers, Parallel architectures, Software standards, Power generation, skimmed},
  readstatus = {skimmed},
}

@InProceedings{rademakerMT,
  author     = {Master’s thesis},
  title      = {Binary relational querying for structural source code analysis},
  file       = {:C\:/Users/Wernsen/Downloads/peterrademaker-msc.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Ebert3359,
  author     = {Jürgen Ebert and Daniel Bildhauer and Hannes Schwarz and Volker Riediger},
  title      = {Using Difference Information to Reuse Software Cases},
  year       = {3359},
  file       = {:C\:/Users/Wernsen/Downloads/10.1.1.511.8355.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Beyer2005,
  author     = {Beyer, D. and Noack, A. and Lewerentz, C.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {Efficient relational calculation for software analysis},
  year       = {2005},
  issn       = {1939-3520},
  month      = feb,
  number     = {2},
  pages      = {137--149},
  volume     = {31},
  abstract   = {Calculating with graphs and relations has many applications in the analysis of software systems, for example, the detection of design patterns or patterns of problematic design and the computation of design metrics. These applications require an expressive query language, in particular, for the detection of graph patterns, and an efficient evaluation of the queries even for large graphs. In this paper, we introduce RML, a simple language for querying and manipulating relations based on predicate calculus, and CrocoPat, an interpreter for RML programs. RML is general because it enables the manipulation not only of graphs (i.e., binary relations), but of relations of arbitrary arity. CrocoPat executes RML programs efficiently because it internally represents relations as binary decision diagrams, a data structure that is well-known as a compact representation of large relations in computer-aided verification. We evaluate RML by giving example programs for several software analyses and CrocoPat by comparing its performance with calculators for binary relations, a Prolog system, and a relational database management system.},
  doi        = {10.1109/TSE.2005.23},
  file       = {:Beyer2005 - Efficient Relational Calculation for Software Analysis.pdf:PDF},
  keywords   = {Application software, Data structures, Pattern analysis, Software systems, Database languages, Calculus, Boolean functions, Software performance, Performance analysis, Calculators, Index Terms- Logic programming, graph algorithms, data structures, reverse engineering, reengineering., skimmed},
  readstatus = {skimmed},
}

@InProceedings{Kniesel2006,
  author     = {Kniesel, Gunter and Bardey, Uwe},
  booktitle  = {2006 13th {Working} {Conference} on {Reverse} {Engineering}},
  title      = {An {Analysis} of the {Correctness} and {Completeness} of {Aspect} {Weaving}},
  year       = {2006},
  month      = oct,
  note       = {ISSN: 2375-5369},
  pages      = {324--333},
  abstract   = {Jointly deployed aspects may interact with each other. While some interactions might be intended, unintended interactions (interferences) can break a program. Detecting and resolving interferences is particularly hard if aspects are developed independently, without knowledge of each other. Work on interference detection has focused so far on the correctness of weaved programs. In this paper we focus on the correctness and completeness of aspect weaving. We show that a large class of interferences result from incorrect or incomplete weaving and present a language independent correctness, and completeness. Our technique can check aspect interferences independent of any base program and is applicable to aspects that contain implicit mutual dependencies in their implementation, without needing special purpose program annotations or formal specifications of aspect semantics},
  doi        = {10.1109/WCRE.2006.10},
  file       = {:Kniesel2006 - An Analysis of the Correctness and Completeness of Aspect Weaving.pdf:PDF},
  issn       = {2375-5369},
  keywords   = {Weaving, Filters, Runtime, Terminology, Formal specifications, Algorithm design and analysis, Programming, Interference constraints, Reverse engineering, skimmed},
  readstatus = {skimmed},
}

@Article{article,
  author     = {Alves, Tiago and Rademaker, Peter},
  title      = {Evaluation of code query technologies for industrial use},
  year       = {2008},
  month      = {01},
  file       = {:article - Evaluation of Code Query Technologies for Industrial Use.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Ceri1989,
  author     = {Ceri, S. and Gottlob, G. and Tanca, L.},
  journal    = {IEEE Transactions on Knowledge and Data Engineering},
  title      = {What you always wanted to know about {Datalog} (and never dared to ask)},
  year       = {1989},
  issn       = {1558-2191},
  month      = mar,
  number     = {1},
  pages      = {146--166},
  volume     = {1},
  abstract   = {Datalog, a database query language based on the logic programming paradigm, is described. The syntax and semantics of Datalog and its use for querying a relational database are presented. Optimization methods for achieving efficient evaluations of Datalog queries are classified, and the most relevant methods are presented. Various improvements of Datalog currently under study are discussed, and what is still needed in order to extend Datalog's applicability to the solution of real-life problems is indicated.{\textless}{\textgreater}},
  doi        = {10.1109/69.43410},
  file       = {:Ceri1989 - What You Always Wanted to Know about Datalog (and Never Dared to Ask).pdf:PDF},
  keywords   = {Amplitude shift keying, Relational databases, Logic programming, Artificial intelligence, Books, Database languages, Query processing, Merging, Optimization methods, Programming environments, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Bildhauer2008QueryingSA,
  author     = {Daniel Bildhauer and J. Ebert},
  title      = {Querying Software Abstraction Graphs},
  year       = {2008},
  file       = {:Bildhauer2008QueryingSA - Querying Software Abstraction Graphs.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{article,
  author     = {Wu, Jingwei and Holt, Richard and Winter, Andreas},
  title      = {Towards a Common Query Language for Reverse Engineering},
  year       = {2002},
  month      = {08},
  file       = {:article - Towards a Common Query Language for Reverse Engineering.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{article,
  author     = {Schürr, Andy and Winter, Andreas and Zündorf, Albert and Softwaretechnik, Ag},
  title      = {The Progres Approach: Language And Environment},
  year       = {1998},
  month      = {12},
  file       = {:article - The Progres Approach_ Language and Environment.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Wu2006OpenSS,
  author     = {J. Wu},
  title      = {Open source software evolution and its dynamics},
  year       = {2006},
  file       = {:Wu2006OpenSS - Open Source Software Evolution and Its Dynamics.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/1244381.1244396,
  author     = {de Moor, Oege and Hajiyev, Elnar and Verbaere, Mathieu},
  booktitle  = {Proceedings of the 2007 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation},
  title      = {Object-Oriented Queries over Software Systems: (Abstract of Invited Talk)},
  year       = {2007},
  address    = {New York, NY, USA},
  pages      = {91},
  publisher  = {Association for Computing Machinery},
  series     = {PEPM '07},
  abstract   = {Code queries are useful for enforcing coding conventions, navigating a large code base, and for identifying locations to refactor. The program understanding community has long advocated the use of a relational database to facilitate such code queries [3, 9]. While the idea has found some uptake in industry [2, 11], relational queries over code have not yet found widespread use.},
  doi        = {10.1145/1244381.1244396},
  file       = {:1244381.1244396.pdf:PDF},
  isbn       = {9781595936202},
  keywords   = {skimmed},
  location   = {Nice, France},
  numpages   = {1},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1244381.1244396},
}

@InProceedings{Moor2007,
  author     = {Moor, Oege de and Verbaere, Mathieu and Hajiyev, Elnar and Avgustinov, Pavel and Ekman, Torbjorn and Ongkingco, Neil and Sereni, Damien and Tibble, Julian},
  booktitle  = {Seventh {IEEE} {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation} ({SCAM} 2007)},
  title      = {Keynote {Address}: .{QL} for {Source} {Code} {Analysis}},
  year       = {2007},
  month      = sep,
  pages      = {3--16},
  abstract   = {Many tasks in source code analysis can be viewed as evaluating queries over a relational representation of the code. Here we present an object-oriented query language, named .QL, and demonstrate its use for general navigation, bug finding and enforcing coding conventions. We then focus on the particular problem of specifying metrics as queries.},
  doi        = {10.1109/SCAM.2007.31},
  file       = {:Moor2007 - Keynote Address_ .QL for Source Code Analysis.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
  shorttitle = {Keynote {Address}},
}

@Article{10.1145/214762.214769,
  author     = {Ebert, J\"{u}rgen},
  journal    = {Commun. ACM},
  title      = {A Versatile Data Structure for Edge-Oriented Graph Algorithms},
  year       = {1987},
  issn       = {0001-0782},
  month      = jun,
  number     = {6},
  pages      = {513–519},
  volume     = {30},
  abstract   = {An abstract graph module that allows for easy and secure programming of a great number of graph algorithms is implemented by symmetrically stored forward and backward adjacency lists, thus supporting edge-oriented traversals of general directed and undirected graphs.},
  address    = {New York, NY, USA},
  doi        = {10.1145/214762.214769},
  file       = {:10.1145_214762.214769 - A Versatile Data Structure for Edge Oriented Graph Algorithms.pdf:PDF},
  issue_date = {June 1987},
  keywords   = {skimmed},
  numpages   = {7},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/214762.214769},
}

@Article{article,
  author     = {Beyer, Dirk and Lewerentz, Claus},
  title      = {CrocoPat: A Tool for Efficient Pattern Recognition in Large Object-Oriented Programs},
  year       = {2003},
  month      = {05},
  file       = {:article - CrocoPat_ a Tool for Efficient Pattern Recognition in Large Object Oriented Programs.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Lange2001,
  author     = {Lange, C. and Sneed, H.M. and Winter, A.},
  booktitle  = {Proceedings 9th {International} {Workshop} on {Program} {Comprehension}. {IWPC} 2001},
  title      = {Comparing graph-based program comprehension tools to relational database-based tools},
  year       = {2001},
  month      = may,
  note       = {ISSN: 1092-8138},
  pages      = {209--218},
  abstract   = {In this paper we compare the experiences of applying the graph-based GUPRO approach to experiences in applying ANAUSoftSpec-an approach based on relational databases. We present the results of a case study in which GUPRO has been applied to a multi-language software system for stock trading (GEOS). Comparing the results of the case study, with experiences of applying ANAL/SoftSpec to GEOS we show that the graph-oriented approach enables an efficient way of source code analysis and program understanding.},
  doi        = {10.1109/WPC.2001.921732},
  file       = {:Lange2001 - Comparing Graph Based Program Comprehension Tools to Relational Database Based Tools.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Relational databases, Software tools, Software systems, Reverse engineering, Industrial relations, Application software, Usability, Programming profession, Embedded system, Layout, skimmed},
  readstatus = {skimmed},
}

@Article{Mens2002,
  author     = {Mens, Kim and Michiels, Isabel and Wuyts, Roel},
  journal    = {Expert Systems with Applications},
  title      = {Supporting software development through declaratively codified programming patterns},
  year       = {2002},
  issn       = {0957-4174},
  month      = nov,
  number     = {4},
  pages      = {405--413},
  volume     = {23},
  abstract   = {In current-day software development, programmers often use programming patterns to clarify their intents and to increase the understandability of their programs. Unfortunately, most software development environments do not adequately support the declaration and use of such patterns. To explicitly codify these patterns, we adopt a declarative meta programming approach. In this approach, we reify the structure of a (object-oriented) program in terms of logic clauses. We declare programming patterns as logic rules on top of these clauses. By querying the logic system, these rules allow us to check, enforce and search for occurrences of certain patterns in the software. As such, the programming patterns become an active part of the software development and maintenance environment.},
  doi        = {10.1016/S0957-4174(02)00076-3},
  file       = {:Mens2002 - Supporting Software Development through Declaratively Codified Programming Patterns.pdf:PDF},
  keywords   = {Programming patterns, Logic programming, Meta programming, Tool support, Object-oriented programming, skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://www.sciencedirect.com/science/article/pii/S0957417402000763},
  urldate    = {2021-06-17},
}

@Article{10.1145/1103845.1094841,
  author     = {Goldsmith, Simon F. and O'Callahan, Robert and Aiken, Alex},
  journal    = {SIGPLAN Not.},
  title      = {Relational Queries over Program Traces},
  year       = {2005},
  issn       = {0362-1340},
  month      = oct,
  number     = {10},
  pages      = {385–402},
  volume     = {40},
  abstract   = {Instrumenting programs with code to monitor runtime behavior is a common technique for profiling and debugging. In practice, instrumentation is either inserted manually by programmers, or automatically by specialized tools that monitor particular properties. We propose Program Trace Query Language (PTQL), a language based on relational queries over program traces, in which programmers can write expressive, declarative queries about program behavior. We also describe our compiler, Partiqle. Given a PTQL query and a Java program, Partiqle instruments the program to execute the query on-line. We apply several PTQL queries to a set of benchmark programs, including the Apache Tomcat Web server. Our queries reveal significant performance bugs in the jack SpecJVM98 benchmark, in Tomcat, and in the IBM Java class library, as well as some correct though uncomfortably subtle code in the Xerces XML parser. We present performance measurements demonstrating that our prototype system has usable performance.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1103845.1094841},
  file       = {:10.1145_1103845.1094841 - Relational Queries Over Program Traces.pdf:PDF},
  issue_date = {October 2005},
  keywords   = {PTQL, relational, program trace query language, partiqle, skimmed},
  numpages   = {18},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1103845.1094841},
}

@InProceedings{Verbaere2008,
  author     = {Verbaere, Mathieu and Godfrey, Michael W. and Girba, Tudor},
  booktitle  = {2008 16th {IEEE} {International} {Conference} on {Program} {Comprehension}},
  title      = {Query {Technologies} and {Applications} for {Program} {Comprehension} ({QTAPC} 2008)},
  year       = {2008},
  month      = jun,
  note       = {ISSN: 1092-8138},
  pages      = {285--288},
  abstract   = {Industrial software systems are large and complex, both in terms of the software entities and their relationships. Consequently, understanding how a software system works requires the ability to pose queries over the design-level entities of the system. Traditionally, this task has been supported by simple tools (e.g., grep) combined with the programmer's intuition and experience. Recently, however, specialized code query technologies have matured to the point where they can be used in industrial situations, providing more intelligent, timely, and efficient responses to developer queries. This working session aims to explore the state of the art in code query technologies, and discover new ways in which these technologies may be useful in program comprehension. The session brings together researchers and practitioners. We survey existing techniques and applications, trying to understand the strengths and weaknesses of the various approaches, and sketch out new frontiers that hold promise.},
  doi        = {10.1109/ICPC.2008.27},
  file       = {:Verbaere2008 - Query Technologies and Applications for Program Comprehension (QTAPC 2008).pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Application software, Software engineering, Computer bugs, Software systems, Visualization, Quality assurance, Monitoring, Computer industry, Industrial relations, Software quality, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.5555/55823.55832,
  author     = {M\"{u}ller, H. A. and Klashinsky, K.},
  booktitle  = {Proceedings of the 10th International Conference on Software Engineering},
  title      = {Rigi-A System for Programming-in-the-Large},
  year       = {1988},
  address    = {Washington, DC, USA},
  pages      = {80–86},
  publisher  = {IEEE Computer Society Press},
  series     = {ICSE '88},
  abstract   = {This paper describes Rigi, a model and a tool for programming-in-the-large. Rigi uses a graph model and abstraction mechanisms to structure and represent the information accumulated during the development process. The objects and relationships of the graph model represent system components and their dependencies. The objects can be arranged in aggregation and generalization hierarchies. The Rigi editor assists the designers, programmers, integrators, and maintainers in defining, manipulating, exploring, and understanding, the structure of large, integrated, evolving software systems. Rigi was designed to address three of the most difficult problems in the area of programming-in-the-large: the mastery of the structural complexity of large software systems, the effective presentation of development information, and the definition of procedures for checking and maintaining the completeness, consistency, and traceability of system descriptions. Thus, the major objective of Rigi is to effectively represent and manipulate the building blocks of a software system and their myriad dependencies, thereby aiding the development phases of the project.},
  file       = {:10.5555_55823.55832 - Rigi a System for Programming in the Large.pdf:PDF},
  isbn       = {0897912586},
  keywords   = {skimmed},
  location   = {Singapore},
  numpages   = {7},
  readstatus = {skimmed},
}

@Article{Bois2007SupportingRS,
  author     = {B. Bois and B. Rompaey and Karel Meijfroidt and Eric Suijs},
  journal    = {Electron. Commun. Eur. Assoc. Softw. Sci. Technol.},
  title      = {Supporting Reengineering Scenarios with FETCH: an Experience Report},
  year       = {2007},
  volume     = {8},
  file       = {:Bois2007SupportingRS - Supporting Reengineering Scenarios with FETCH_ an Experience Report.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Holt2008GrokkingSA,
  author     = {R. Holt},
  title      = {Grokking Software Architecture},
  year       = {2008},
  file       = {:Holt2008GrokkingSA - Grokking Software Architecture.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Cordy2006,
  author     = {Cordy, James R.},
  journal    = {Science of Computer Programming},
  title      = {The {TXL} source transformation language},
  year       = {2006},
  issn       = {0167-6423},
  month      = aug,
  number     = {3},
  pages      = {190--210},
  volume     = {61},
  abstract   = {TXL is a special-purpose programming language designed for creating, manipulating and rapidly prototyping language descriptions, tools and applications. TXL is designed to allow explicit programmer control over the interpretation, application, order and backtracking of both parsing and rewriting rules. Using first order functional programming at the higher level and term rewriting at the lower level, TXL provides for flexible programming of traversals, guards, scope of application and parameterized context. This flexibility has allowed TXL users to express and experiment with both new ideas in parsing, such as robust, island and agile parsing, and new paradigms in rewriting, such as XML mark-up, rewriting strategies and contextualized rules, without any change to TXL itself. This paper outlines the history, evolution and concepts of TXL with emphasis on its distinctive style and philosophy, and gives examples of its use in expressing and applying recent new paradigms in language processing.},
  doi        = {10.1016/j.scico.2006.04.002},
  file       = {:Cordy2006 - The TXL Source Transformation Language.pdf:PDF;:1-s2.0-S0167642306000669-main.pdf:PDF},
  keywords   = {Source transformation, Functional programming, Term rewriting, Grammars, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Special {Issue} on {The} {Fourth} {Workshop} on {Language} {Descriptions}, {Tools}, and {Applications} ({LDTA} ’04)},
  url        = {https://www.sciencedirect.com/science/article/pii/S0167642306000669},
  urldate    = {2021-06-17},
}

@Article{10.1145/1082983.1083161,
  author     = {Hindle, Abram and German, Daniel M.},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {SCQL: A Formal Model and a Query Language for Source Control Repositories},
  year       = {2005},
  issn       = {0163-5948},
  month      = may,
  number     = {4},
  pages      = {1–5},
  volume     = {30},
  abstract   = {Source Control Repositories are used in most software projects to store revisions to source code files. These repositories operate at the file level and support multiple users. A generalized formal model of source control repositories is described herein. The model is a graph in which the different entities stored in the repository become vertices and their relationships become edges. We then define SCQL, a first order, and temporal logic based query language for source control repositories. We demonstrate how SCQL can be used to specify some questions and then evaluate them using the source control repositories of five different large software projects.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1082983.1083161},
  file       = {:10.1145_1082983.1083161 - SCQL_ a Formal Model and a Query Language for Source Control Repositories.pdf:PDF},
  issue_date = {July 2005},
  keywords   = {skimmed},
  numpages   = {5},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1082983.1083161},
}

@InProceedings{10.5555/2486788.2486844,
  author     = {Dyer, Robert and Nguyen, Hoan Anh and Rajan, Hridesh and Nguyen, Tien N.},
  booktitle  = {Proceedings of the 2013 International Conference on Software Engineering},
  title      = {Boa: A Language and Infrastructure for Analyzing Ultra-Large-Scale Software Repositories},
  year       = {2013},
  pages      = {422–431},
  publisher  = {IEEE Press},
  series     = {ICSE '13},
  abstract   = {In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers.},
  file       = {:10.5555_2486788.2486844 - Boa_ a Language and Infrastructure for Analyzing Ultra Large Scale Software Repositories.pdf:PDF},
  isbn       = {9781467330763},
  keywords   = {skimmed},
  location   = {San Francisco, CA, USA},
  numpages   = {10},
  readstatus = {skimmed},
}

@Article{Stevens2014a,
  author     = {Stevens, Reinout and De Roover, Coen and Noguera, Carlos and Kellens, Andy and Jonckers, Viviane},
  journal    = {Science of Computer Programming},
  title      = {A logic foundation for a general-purpose history querying tool},
  year       = {2014},
  issn       = {0167-6423},
  month      = dec,
  pages      = {107--120},
  volume     = {96},
  abstract   = {Version control systems (VCS) have become indispensable software development tools. The version snapshots they store to provide support for change coordination and release management, effectively track the evolution of the versioned software and its development process. Despite this wealth of historical information, it has only been leveraged by tools that are dedicated to a specific task such as empirical validation of software engineering practices or fault prediction. General-purpose tool support for reasoning about the historical information stored in a version control system is limited. This paper provides a comprehensive description of a logic-based, general-purpose history query tool called Absinthe. Absinthe supports querying versioned Smalltalk system using logic queries in which quantified regular path expressions are embedded. These expressions lend themselves to specifying the properties that each individual version in a sequence of successive software versions ought to exhibit. To demonstrate the general-purpose nature of our history query tool, we use it to verify development process constraints, to identify temporal bad smells and to answer questions that developers commonly ask. Finally, we compare a query written in Absinthe to an equivalent one written in Smalltalk.},
  doi        = {10.1016/j.scico.2014.02.014},
  file       = {:Stevens2014a - A Logic Foundation for a General Purpose History Querying Tool.pdf:PDF},
  keywords   = {History queries, Meta-programming, Smalltalk, Logic programming, Program analysis, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Special issue on {Advances} in {Smalltalk} based {Systems}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0167642314000653},
  urldate    = {2021-06-17},
}

@Article{10.1145/982962.964004,
  author     = {Sittampalam, Ganesh and de Moor, Oege and Larsen, Ken Friis},
  journal    = {SIGPLAN Not.},
  title      = {Incremental Execution of Transformation Specifications},
  year       = {2004},
  issn       = {0362-1340},
  month      = jan,
  number     = {1},
  pages      = {26–38},
  volume     = {39},
  abstract   = {We aim to specify program transformations in a declarative style, and then to generate executable program transformers from such specifications. Many transformations require non-trivial program analysis to check their applicability, and it is prohibitively expensive to re-run such analyses after each transformation. It is desirable, therefore, that the analysis information is incrementally updated.We achieve this by drawing on two pieces of previous work: first, Bernhard Steffen's proposal to use model checking for certain analysis problems, and second, John Conway's theory of language factors. The first allows the neat specification of transformations, while the second opens the way for an incremental implementation. The two ideas are linked by using regular patterns instead of Steffen's modal logic: these patterns can be viewed as queries on the set of program paths.},
  address    = {New York, NY, USA},
  doi        = {10.1145/982962.964004},
  file       = {:10.1145_982962.964004 - Incremental Execution of Transformation Specifications.pdf:PDF},
  issue_date = {January 2004},
  keywords   = {constraints, incremental algorithm, language factors, logic programming, residuation operators, transformation specification, program analysis, program transformation, skimmed},
  numpages   = {13},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/982962.964004},
}

@Article{Bravenboer2008,
  author     = {Bravenboer, Martin and Kalleberg, Karl Trygve and Vermaas, Rob and Visser, Eelco},
  journal    = {Science of Computer Programming},
  title      = {Stratego/{XT} 0.17. {A} language and toolset for program transformation},
  year       = {2008},
  issn       = {0167-6423},
  month      = jun,
  number     = {1},
  pages      = {52--70},
  volume     = {72},
  abstract   = {Stratego/XT is a language and toolset for program transformation. The Stratego language provides rewrite rules for expressing basic transformations, programmable rewriting strategies for controlling the application of rules, concrete syntax for expressing the patterns of rules in the syntax of the object language, and dynamic rewrite rules for expressing context-sensitive transformations, thus supporting the development of transformation components at a high level of abstraction. The XT toolset offers a collection of flexible, reusable transformation components, and tools for generating such components from declarative specifications. Complete program transformation systems are composed from these components. This paper gives an overview of Stratego/XT 0.17, including a description of the Stratego language and XT transformation tools; a discussion of the implementation techniques and software engineering process; and a description of applications built with Stratego/XT.},
  doi        = {10.1016/j.scico.2007.11.003},
  file       = {:Bravenboer2008 - Stratego_XT 0.17. a Language and Toolset for Program Transformation.pdf:PDF},
  keywords   = {Stratego, Stratego/XT, Program transformation, Rewriting strategies, Rewrite rules, Concrete syntax, Dynamic rewrite rules, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Special {Issue} on {Second} issue of experimental software and toolkits ({EST})},
  url        = {https://www.sciencedirect.com/science/article/pii/S0167642308000452},
  urldate    = {2021-06-17},
}

@InCollection{Reps1995,
  author     = {Reps, Thomas W.},
  publisher  = {Springer US},
  title      = {Demand {Interprocedural} {Program} {Analysis} {Using} {Logic} {Databases}},
  year       = {1995},
  address    = {Boston, MA},
  editor     = {Ramakrishnan, Raghu},
  isbn       = {9781461522072},
  pages      = {163--196},
  series     = {The {Springer} {International} {Series} in {Engineering} and {Computer} {Science}},
  abstract   = {This paper describes how algorithms for demand versions of interprocedural program-analysis problems can be obtained from their exhaustive counterparts essentially for free, by applying the so-called magic-sets transformation that was developed in the logic-programming and deductive-database communities. Applications to interprocedural dataflow analysis and interprocedural program slicing are described.1},
  doi        = {10.1007/978-1-4615-2207-2_8},
  file       = {:Reps1995 - Demand Interprocedural Program Analysis Using Logic Databases.pdf:PDF},
  keywords   = {Logic Program , Dependence Graph , Program Point , Call Site , Valid Path , skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1007/978-1-4615-2207-2_8},
  urldate    = {2021-06-17},
}

@Article{10.1145/75309.75318,
  author     = {Kotik, G. and Markosian, L.},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Automating Software Analysis and Testing Using a Program Transformation System},
  year       = {1989},
  issn       = {0163-5948},
  month      = nov,
  number     = {8},
  pages      = {75–84},
  volume     = {14},
  abstract   = {We describe an approach to software analysis and test generation that combines several technologies: object-oriented databases and parsers for capturing and representing software; pattern languages for writing program templates and querying and analyzing a database of software; and transformation rules for automatically generating test cases based on the analysis results, and for automatically creating program “mutants” to determine adequacy of coverage of the test cases. We present a program transformation system, REFINE™1, that incorporates these technologies in an open environment for software analysis and test generation. Next we present concrete examples of how our approach is being applied to analysis and test generation for C software.},
  address    = {New York, NY, USA},
  doi        = {10.1145/75309.75318},
  file       = {:10.1145_75309.75318 - Automating Software Analysis and Testing Using a Program Transformation System.pdf:PDF},
  issue_date = {Dec. 1989},
  keywords   = {skimmed},
  numpages   = {10},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/75309.75318},
}

@Article{Kozaczynski1992,
  author     = {Kozaczynski, W. and Ning, J. and Engberts, A.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {Program concept recognition and transformation},
  year       = {1992},
  issn       = {1939-3520},
  month      = dec,
  number     = {12},
  pages      = {1065--1075},
  volume     = {18},
  abstract   = {The automated recognition of abstract high-level conceptual information or concepts, which can greatly aid the understanding of programs and therefore support many software maintenance and reengineering activities, is considered. An approach to automated concept recognition and its application to maintenance-related program transformations are described. A unique characteristic of this approach is that transformations of code can be expressed as transformations of abstract concepts. This significantly elevates the level of transformation specifications.{\textless}{\textgreater}},
  doi        = {10.1109/32.184761},
  file       = {:Kozaczynski1992 - Program Concept Recognition and Transformation.pdf:PDF},
  keywords   = {Software maintenance, Application software, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.5555/998675.999466,
  author     = {Baxter, Ira D. and Pidgeon, Christopher and Mehlich, Michael},
  booktitle  = {Proceedings of the 26th International Conference on Software Engineering},
  title      = {DMS®: Program Transformations for Practical Scalable Software Evolution},
  year       = {2004},
  address    = {USA},
  pages      = {625–634},
  publisher  = {IEEE Computer Society},
  series     = {ICSE '04},
  abstract   = {While a number of research systems have demonstratedthe potential value of program transformations, very few ofthese systems have made it into practice. The core technologyfor such systems is well understood; what remains isintegration and more importantly, the problem of handlingthe scale of the applications to be processed.This paper describes DMS, a practical, commercialprogram analysis and transformation system, and sketchesa variety of tasks to which it has been applied, from redocumentingto large-scale system migration. Its successderives partly from a vision of design maintenance and theconstruction of infrastructure that appears necessary tosupport that vision. DMS handles program scale by carefulspace management, computational scale via parallelismand knowledge acquisition scale via domains.},
  file       = {:10.5555_998675.999466 - DMS®_ Program Transformations for Practical Scalable Software Evolution.pdf:PDF},
  isbn       = {0769521630},
  keywords   = {skimmed},
  numpages   = {10},
  readstatus = {skimmed},
}

@InProceedings{Reichenbach2009,
  author     = {Reichenbach, Christoph and Coughlin, Devin and Diwan, Amer},
  booktitle  = {{ECOOP} 2009 – {Object}-{Oriented} {Programming}},
  title      = {Program {Metamorphosis}},
  year       = {2009},
  address    = {Berlin, Heidelberg},
  editor     = {Drossopoulou, Sophia},
  pages      = {394--418},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Modern development environments support refactoring by providing atomically behaviour-preserving transformations. While useful, these transformations are limited in three ways: (i) atomicity forces transformations to be complex and opaque, (ii) the behaviour preservation requirement disallows deliberate behaviour evolution, and (iii) atomicity limits code reuse opportunities for refactoring implementers.We present ‘program metamorphosis’, a novel approach for program evolution and refactoring that addresses the above limitations by breaking refactorings into smaller steps that need not preserve behaviour individually. Instead, we ensure that sequences of transformations preserve behaviour together, and simultaneously permit selective behavioural change.To evaluate program metamorphosis, we have implemented a prototype plugin for Eclipse. Our analysis and experiments show that (1) our plugin provides correctness guarantees on par with those of Eclipse’s own refactorings, (2) both our plugin and our approach address the aforementioned limitations, and (3) our approach fully subsumes traditional refactoring.},
  doi        = {10.1007/978-3-642-03013-0_18},
  file       = {:Reichenbach2009 - Program Metamorphosis.pdf:PDF;:10.1007_978-3-642-03013-0.pdf:PDF;:C\:/Users/Wernsen/Documents/School/Open Universiteit/Voorbereiden afstuderen/New/sec/pm.pdf:PDF},
  isbn       = {9783642030130},
  keywords   = {Refactoring , Program Evolution , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@Article{article,
  author     = {Kim, T.-W and Kim, T.-G and Seu, J.-H},
  journal    = {International Journal of Software Engineering and its Applications},
  title      = {Specification and automated detection of code smells using OCL},
  year       = {2013},
  month      = {01},
  pages      = {35-44},
  volume     = {7},
  file       = {:article - Specification and Automated Detection of Code Smells Using OCL.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{,
  title = {Theory and design of source code query systems},
}

@InProceedings{200135,
  author     = {S. Paul and A. Prakash},
  booktitle  = {Proceedings of the Fifth International Workshop on Computer-Aided Software Engineering},
  title      = {Source code retrieval using program patterns},
  year       = {1992},
  address    = {Los Alamitos, CA, USA},
  month      = {jul},
  pages      = {92,93,94,95,96,97,98,99,100,101,102,103,104,105},
  publisher  = {IEEE Computer Society},
  doi        = {10.1109/CASE.1992.200135},
  file       = {:200135 - Source Code Retrieval Using Program Patterns.pdf:PDF},
  keywords   = {software maintenance;optimizing compilers;software tools;programming profession;software systems;laboratories;maintenance engineering;inspection;reverse engineering;power engineering and energy, skimmed},
  readstatus = {skimmed},
  url        = {https://doi.ieeecomputersociety.org/10.1109/CASE.1992.200135},
}

@InProceedings{DeVolder2006,
  author     = {De Volder, Kris},
  booktitle  = {Practical {Aspects} of {Declarative} {Languages}},
  title      = {{JQuery}: {A} {Generic} {Code} {Browser} with a {Declarative} {Configuration} {Language}},
  year       = {2006},
  address    = {Berlin, Heidelberg},
  editor     = {Van Hentenryck, Pascal},
  pages      = {88--102},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Modern IDEs have an open-ended plugin architecture to allow customizability. However, developing a plugin is costly in terms of effort and expertise required by the customizer. We present a two-pronged approach that allows for open-ended customizations while keeping the customization cost low. First, we explicitly limit the portion of the design space targeted by the configuration mechanism. This reduces customization cost by simplifying the configuration interface. Second, we use a declarative programming language as our configuration language. This facilitates open-ended specification of behavior without burdening the user with operational details.},
  doi        = {10.1007/11603023_7},
  file       = {:DeVolder2006 - JQuery_ a Generic Code Browser with a Declarative Configuration Language.pdf:PDF;:jquery-tyruba.pdf:PDF},
  isbn       = {9783540316855},
  keywords   = {integrated development environment , program database , domain-specific language , logic programming , skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {{JQuery}},
}

@Article{Hou2006,
  author     = {Hou, D. and Hoover, H.J.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {Using {SCL} to specify and check design intent in source code},
  year       = {2006},
  issn       = {1939-3520},
  month      = jun,
  number     = {6},
  pages      = {404--423},
  volume     = {32},
  abstract   = {Software developers often fail to respect the intentions of designers due to missing or ignored documentation of design intent. SCL (Structural Constraint Language) addresses this problem by enabling designers to formalize and confirm compliance with design intent. The designer expresses his intent as constraints on the program model using the SCL language. The SCL conformance checking tool examines developer code to confirm that the code honors these constraints. This paper presents the design of the SCL language and its checker, a set of practical examples of applying SCL, and our experience with using it both in an industrial setting and on open-source software},
  doi        = {10.1109/TSE.2006.60},
  file       = {:C\:/Users/Wernsen/Downloads/hou2006.pdf:PDF},
  keywords   = {Open source software, Programming, Context modeling, Application software, Documentation, Object oriented modeling, Computer industry, Roads, Government, Design intent, structural constraints, program analysis, object-oriented software, SCL, FCL., skimmed},
  readstatus = {skimmed},
}

@Article{Dietrich1992,
  author     = {Dietrich, Suzanne W. and Calliss, Frank W.},
  journal    = {Journal of Software Maintenance: Research and Practice},
  title      = {A conceptual design for a code analysis knowledge base},
  year       = {1992},
  issn       = {1096-908X},
  number     = {1},
  pages      = {19--36},
  volume     = {4},
  abstract   = {A knowledge base system for inter-module code analysis is presented. Inter-module code analysis is a technique that a programmer can use to analyse programs written in languages that contain a clustering construct called a module (or package). A module allows a programmer to control the visibility of a component within a program. The knowledge base for inter-module code analysis is designed using the enhanced entity-relationship conceptual data model, which is a graphical representation of the data and its relationships. The conceptual design for a code analysis enterprise is mapped into a knowledge base system, which uses a declarative logic-based language to represent data as both facts and rules. The use of this knowledge base in an inter-module code analysis application is demonstrated.},
  copyright  = {Copyright © 1992 John Wiley \& Sons, Ltd},
  doi        = {10.1002/smr.4360040103},
  file       = {:Dietrich1992 - A Conceptual Design for a Code Analysis Knowledge Base.pdf:PDF;:dietrich1992.pdf:PDF},
  keywords   = {Code analysis, Deductive databases, Knowledge bases, skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.4360040103},
  urldate    = {2021-06-17},
}

@Article{Harandi1990,
  author     = {Harandi, M.T. and Ning, J.Q.},
  journal    = {IEEE Software},
  title      = {Knowledge-based program analysis},
  year       = {1990},
  issn       = {1937-4194},
  month      = jan,
  number     = {1},
  pages      = {74--81},
  volume     = {7},
  abstract   = {Automatic program analysis is regarded here as both the mechanized process of understanding high-level concepts from program text and the use of those concepts to guide program maintenance. A knowledge-based program analysis tool called PAT, which realizes this concept, is described. PAT uses an object-oriented framework to represent programming concepts and a heuristic-based concept-recognition mechanism to derive high-level functional concepts from the source code. As an example, a segment of a much larger program written in a Pascal-like language is analyzed using PAT.{\textless}{\textgreater}},
  doi        = {10.1109/52.43052},
  file       = {:C\:/Users/Wernsen/Downloads/harandi-pat.pdf:PDF},
  keywords   = {Cognitive science, Programming profession, Object oriented modeling, Documentation, Functional programming, Object oriented programming, Tree graphs, Petri nets, Abstracts, Humans, skimmed},
  readstatus = {skimmed},
}

@Article{Cleveland1989,
  author     = {Cleveland, L.},
  journal    = {IBM Systems Journal},
  title      = {A program understanding support environment},
  year       = {1989},
  issn       = {0018-8670},
  number     = {2},
  pages      = {324--344},
  volume     = {28},
  abstract   = {Software maintenance represents the largest cost element in the life of a software system, and the process of understanding the software system utilizes 50 percent of the time spent on software maintenance. Thus there is a need for tools to aid the program understanding task. The tool described in this paper—program UNderstanding Support environment (PUNS)—provides the needed environment. Here the program understanding task is supported with multiple views of the program and a simple strategy for moving between views and exploring a particular view in depth. PUNS consists of a repository component that loads and manages a repository of information about the program to be understood and a user interface component that presents the information in the repository, utilizing graphics to emphasize the relationships and allowing the user to move among the pieces of information quickly and easily.},
  doi        = {10.1147/sj.282.0324},
  file       = {:C\:/Users/Wernsen/Downloads/cleveland1989.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Horwitz1990,
  author     = {Horwitz, Susan},
  journal    = {Theoretical Computer Science},
  title      = {Adding relational query facilities to software development environments},
  year       = {1990},
  issn       = {0304-3975},
  month      = jun,
  number     = {2},
  pages      = {213--230},
  volume     = {73},
  abstract   = {Software development environments should include query handlers. Query handlers based on the relational database model are attractive because the model provides a uniform, non-procedural approach to query writing. There are two drawbacks to using the relational model to support query handlers in software development systems: (1) Standard relational database systems require that all information be stored in relations; however, the data structures used by existing software development environments are generally non-relational, and it is impractical to replace them with relations. (2) The standard relational operators are not powerful enough to express certain important classes of queries. We have previously proposed a model of editing environments (Horwitz, Teitelbaum, 1986), based on the use of relationally-attributed grammars, that supports a relational query facility. We introduced a new kind of relation, implicit relations, and a new approach to query evaluation to handle queries that use implicit relations. In this paper we illustrate the utility of implicit relations in contexts other than relationally-attributed grammars. We extend the definition of implicit relations and show how they can be used to support relational query facilities in software development environments without giving up the use of non-relational data structures. Implicit relations can also be used to provide non-standard relational operations such as transitive closure.},
  doi        = {10.1016/0304-3975(90)90146-9},
  file       = {:Horwitz1990 - Adding Relational Query Facilities to Software Development Environments.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://www.sciencedirect.com/science/article/pii/0304397590901469},
  urldate    = {2021-06-17},
}

@Article{Mueller1993,
  author     = {Müller, Hausi A. and Orgun, Mehmet A. and Tilley, Scott R. and Uhl, James S.},
  journal    = {Journal of Software Maintenance: Research and Practice},
  title      = {A reverse-engineering approach to subsystem structure identification},
  year       = {1993},
  issn       = {1096-908X},
  number     = {4},
  pages      = {181--204},
  volume     = {5},
  abstract   = {Reverse-engineering is the process of extracting system abstractions and design information out of existing software systems. This process involves the identification of software artefacts in a particular subject system, the exploration of how these artefacts interact with one another, and their aggregation to form more abstract system representations that facilitate program understanding. This paper describes our approach to creating higher-level abstract representations of a subject system, which involves the identification of related components and dependencies, the construction of layered subsystem structures, and the computation of exact interfaces among subsystems. We show how top-down decompositions of a subject system can be (re)constructed via bottom-up subsystem composition. This process involves identifying groups of building blocks (e.g., variables, procedures, modules, and subsystems) using composition operations based on software engineering principles such as low coupling and high cohesion. The result is an architecture of layered subsystem structures. The structures are manipulated and recorded using the Rigi system, which consists of a distributed graph editor and a parsing system with a central repository. The editor provides graph filters and clustering operations to build and explore subsystem hierarchies interactively. The paper concludes with a detailed, step-by-step analysis of a 30-module software system using Rigi.},
  copyright  = {Copyright © 1993 John Wiley \& Sons, Ltd},
  doi        = {10.1002/smr.4360050402},
  file       = {:Mueller1993 - A Reverse Engineering Approach to Subsystem Structure Identification.pdf:PDF;:A_reverse_engineering_approach_to_subsys.pdf:PDF},
  keywords   = {Software maintenance, Reverse-engineering, Program understanding, Software engineering principles, Resource-flow graphs, Subsystem hierarchies, Subsystem composition, Exact interfaces, Re-engineering, Change analysis, skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.4360050402},
  urldate    = {2021-06-17},
}

@PhdThesis{paul1995design,
  author  = {Paul, Santanu},
  school  = {University of Michigan},
  title   = {Design and implementation of query languages for Program databases},
  year    = {1995},
  address = {https://www.proquest.com/docview/304229891},
  file    = {:paul1995design - Design and Implementation of Query Languages for Program Databases.pdf:PDF},
}

@Article{paul1994supporting,
  author     = {Paul, Santanu and Prakash, Atul},
  journal    = {International Journal of Software Engineering and Knowledge Engineering},
  title      = {Supporting queries on source code: A formal framework},
  year       = {1994},
  number     = {3},
  pages      = {325--348},
  volume     = {4},
  file       = {:paul1994supporting - Supporting Queries on Source Code_ a Formal Framework.pdf:PDF},
  keywords   = {skimmed},
  publisher  = {Citeseer},
  readstatus = {skimmed},
}

@InProceedings{Holt1996BinaryRA,
  author     = {R. Holt},
  title      = {Binary Relational Algebra Applied to Software Architecture},
  year       = {1996},
  file       = {:C\:/Users/Wernsen/Downloads/10.1.1.31.1284.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/253228.253496,
  author     = {Storey, Margaret-Anne D. and Wong, Kenny and M\"{u}ller, Hausi A.},
  booktitle  = {Proceedings of the 19th International Conference on Software Engineering},
  title      = {Rigi: A Visualization Environment for Reverse Engineering},
  year       = {1997},
  address    = {New York, NY, USA},
  pages      = {606–607},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '97},
  doi        = {10.1145/253228.253496},
  file       = {:10.1145_253228.253496 - Rigi_ a Visualization Environment for Reverse Engineering.pdf:PDF},
  isbn       = {0897919149},
  keywords   = {software visualization, graph editor, nested graphs, reverse engineering, fisheye views, skimmed},
  location   = {Boston, Massachusetts, USA},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/253228.253496},
}

@InProceedings{Berghammer1996,
  author     = {Berghammer, Rudolf and von Karger, Burghard and Ulke, Christiane},
  booktitle  = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
  title      = {Relation-algebraic analysis of {Petri} nets with {RELVIEW}},
  year       = {1996},
  address    = {Berlin, Heidelberg},
  editor     = {Margaria, Tiziana and Steffen, Bernhard},
  pages      = {49--69},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {We present a method for specifying and implementing algorithms for the analysis of Petri nets. It is formally grounded in relational algebra. Specifications are written in ordinary predicate logic and then transformed systematically into relational programs which can be executed directly in RELVIEW, a graphical computer system for calculating with relations. Our method yields programs that are correct by construction. Its simplicity and efficiency is illustrated in many examples.},
  doi        = {10.1007/3-540-61042-1_38},
  file       = {:Berghammer1996 - Relation Algebraic Analysis of Petri Nets with RELVIEW.pdf:PDF},
  isbn       = {9783540498742},
  keywords   = {Predicate Logic , Relational Algebra , Relational Program , Boolean Matrix , Relational Operation , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@InProceedings{Chen1995,
  author     = {Chen, Y.-F.R. and Fowler, G.S. and Koutsofios, E. and Wallach, R.S.},
  booktitle  = {Proceedings of {International} {Conference} on {Software} {Maintenance}},
  title      = {Ciao: a graphical navigator for software and document repositories},
  year       = {1995},
  month      = oct,
  note       = {ISSN: 1063-6773},
  pages      = {66--75},
  abstract   = {Programmers frequently have to retrieve and link information from various software documents to accomplish a maintenance task. Ciao is a graph-based navigator that helps programmers query and browse structural connections embedded in different software and document repositories. A repository consists of a collection of source documents with an associated database that describes their structure. Ciao supports repositories organized in an architecture style called Aero, which exploits the duality between a class of entity-relationship (ER) databases and directed attributed graphs (DAG). Database queries and graph analysis operators in Aero are plug-compatible because they all take an ER database and produce yet another ER database by default. Various presentation filters generate graph views, source views, and relational views from any compatible ER database. The architecture promotes the construction of successively more complex operators using a notion of virtual database pipelines. Ciao has been instantiated for C and C++ program databases, and program difference databases. The latter allows programmers to explore program structure changes by browsing and expanding graphs that highlight changed, deleted, and added entities and relationships. The unifying ER model under ciao also allows users to navigate different software repositories and make necessary connections. We have linked program difference databases and modification request (MR) databases so that users can investigate the connections between MRs and affected entities. Ciao has been applied to several large communications software projects and we report experiences and lessons learned from these applications.},
  doi        = {10.1109/ICSM.1995.526528},
  file       = {:Chen1995 - Ciao_ a Graphical Navigator for Software and Document Repositories.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {Navigation, Relational databases, Erbium, Programming profession, Information retrieval, Software maintenance, Embedded software, Computer architecture, Filters, Pipelines, skimmed},
  readstatus = {skimmed},
  shorttitle = {Ciao},
}

@InProceedings{Masinter1979GlobalPA,
  author     = {L. Masinter},
  title      = {Global program analysis in an interactive environment},
  year       = {1979},
  file       = {:Masinter1979GlobalPA - Global Program Analysis in an Interactive Environment.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{taentzer2003agg,
  author       = {Taentzer, Gabriele},
  booktitle    = {International Workshop on Applications of Graph Transformations with Industrial Relevance},
  title        = {AGG: A graph transformation environment for modeling and validation of software},
  year         = {2003},
  organization = {Springer},
  pages        = {446--453},
  file         = {:C\:/Users/Wernsen/Downloads/Tae04.pdf:PDF},
  keywords     = {skimmed},
  readstatus   = {skimmed},
}

@InProceedings{Collard2003,
  author     = {Collard, M.L. and Kagdi, H.H. and Maletic, J.I.},
  booktitle  = {11th {IEEE} {International} {Workshop} on {Program} {Comprehension}, 2003.},
  title      = {An {XML}-based lightweight {C}++ fact extractor},
  year       = {2003},
  month      = may,
  note       = {ISSN: 1092-8138},
  pages      = {134--143},
  abstract   = {A lightweight fact extractor is presented that utilizes XML tools, such as XPath and XSLT to extract static information from C++ source code programs. The source code is first converted into an XML representation, srcML, to facilitate the use of a wide variety of XML tools. The method is deemed lightweight because only a partial parsing of the source is done. Additionally, the technique is quite robust and can be applied to incomplete and noncompilable source code. The trade off to this approach is that queries on some low level details cannot be directly addressed. This approach is applied to a fact extractor benchmark as comparison with other, heavier weight, fact extractors. Fact extractors are widely used to support understanding tasks associated with maintenance, reverse engineering and various other software engineering tasks.},
  doi        = {10.1109/WPC.2003.1199197},
  file       = {:Collard2003 - An XML Based Lightweight C++ Fact Extractor.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Data mining, XML, Robustness, Software engineering, Computer science, Reverse engineering, Software testing, System testing, Software systems, White spaces, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Boerboom2006FactEQ,
  author     = {F. Boerboom and A. Janssen},
  title      = {Fact extraction, querying and visualization of large C++ code bases},
  year       = {2006},
  file       = {:Boerboom2006FactEQ - Fact Extraction, Querying and Visualization of Large C++ Code Bases.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{10.1145/251621.251633,
  author     = {van den Brand, Mark and Klint, Paul and Verhoef, Chris},
  journal    = {SIGPLAN Not.},
  title      = {Re-Engineering Needs Generic Programming Language Technology},
  year       = {1997},
  issn       = {0362-1340},
  month      = feb,
  number     = {2},
  pages      = {54–61},
  volume     = {32},
  abstract   = {Generic language technology and compiler construction techniques are a prerequisite to build analysis and conversion tools that are needed for the re-engineering of large software systems. We argue that generic language technology is a crucial means to do fundamental re-engineering. Furthermore, we address the issue that the application of compiler construction techniques in re-engineering generates new research questions in the field of compiler construction.},
  address    = {New York, NY, USA},
  doi        = {10.1145/251621.251633},
  file       = {:10.1145_251621.251633 - Re Engineering Needs Generic Programming Language Technology.pdf:PDF},
  issue_date = {Feb. 1997},
  keywords   = {system renovation, intermediate data representation, programming environment generator, reverse engineering, re-engineering, compiler construction techniques, generic language technology, skimmed},
  numpages   = {8},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/251621.251633},
}

@InProceedings{Lin2003,
  author     = {Lin, Yuan and Holt, R.C. and Malton, A.J.},
  booktitle  = {10th {Working} {Conference} on {Reverse} {Engineering}, 2003. {WCRE} 2003. {Proceedings}.},
  title      = {Completeness of a fact extractor},
  year       = {2003},
  month      = nov,
  note       = {ISSN: 1095-1350},
  pages      = {196--205},
  doi        = {10.1109/WCRE.2003.1287250},
  file       = {:Lin2003 - Completeness of a Fact Extractor.pdf:PDF},
  issn       = {1095-1350},
  keywords   = {Data mining, Reverse engineering, Tree graphs, Concrete, Computer science, Assembly, Information analysis, Tree data structures, Visualization, XML, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Ferenc2004,
  author     = {Ferenc, R. and Siket, I. and Gyimothy, T.},
  booktitle  = {20th {IEEE} {International} {Conference} on {Software} {Maintenance}, 2004. {Proceedings}.},
  title      = {Extracting facts from open source software},
  year       = {2004},
  month      = sep,
  note       = {ISSN: 1063-6773},
  pages      = {60--69},
  abstract   = {Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But because open source software is often developed without proper management, the quality and reliability of the code may be uncertain. The quality of the code needs to be measured and this can be done only with the help of proper tools. We describe a framework called Columbus with which we calculate the object oriented metrics validated by Basili et al. for illustrating how fault-proneness detection from the open source Web and e-mail suite called Mozilla can be done. We also compare the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development. The Columbus framework has been further developed recently with a compiler wrapping technology that now gives us the possibility of automatically analyzing and extracting information from software systems without modifying any of the source code or makefiles. We also introduce our fact extraction process here to show what logic drives the various tools of the Columbus framework and what steps need to be taken to obtain the desired facts.},
  doi        = {10.1109/ICSM.2004.1357790},
  file       = {:Ferenc2004 - Extracting Facts from Open Source Software.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {Open source software, Software systems, Data mining, Quality management, Software development management, Fault detection, Electronic mail, Wrapping, Information analysis, Logic, skimmed},
  readstatus = {skimmed},
}

@InProceedings{begel2007codifier,
  author     = {Begel, Andrew},
  booktitle  = {First Workshop on Human-Computer Interaction and Information Retrieval},
  title      = {Codifier: A Programmer-Centric Search User Interface},
  year       = {2007},
  month      = {October},
  abstract   = {Search tools have transformed knowledge discovery by exposing information from previously hidden repositories to the workers who need it. Search engines like Google and Live.com provide search capabilities via a simple one-line text query box, and present results in a paged HTML list. When the repository being searched contains structured information with extractable metadata (e.g. program source code), it can be advantageous to index the metadata and use it to enable queries that are more task-centric and suitable for an domain-specific audience.},
  edition    = {First Workshop on Human-Computer Interaction and Information Retrieval},
  file       = {:begel2007codifier - Codifier_ a Programmer Centric Search User Interface.bib:bib;:C\:/Users/Wernsen/Downloads/hcir07.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
  url        = {https://www.microsoft.com/en-us/research/publication/codifier-a-programmer-centric-search-user-interface/},
}

@InProceedings{Yamaguchi2014,
  author     = {Yamaguchi, Fabian and Golde, Nico and Arp, Daniel and Rieck, Konrad},
  booktitle  = {2014 {IEEE} {Symposium} on {Security} and {Privacy}},
  title      = {Modeling and {Discovering} {Vulnerabilities} with {Code} {Property} {Graphs}},
  year       = {2014},
  month      = may,
  note       = {ISSN: 2375-1207},
  pages      = {590--604},
  abstract   = {The vast majority of security breaches encountered today are a direct result of insecure code. Consequently, the protection of computer systems critically depends on the rigorous identification of vulnerabilities in software, a tedious and error-prone process requiring significant expertise. Unfortunately, a single flaw suffices to undermine the security of a system and thus the sheer amount of code to audit plays into the attacker's cards. In this paper, we present a method to effectively mine large amounts of source code for vulnerabilities. To this end, we introduce a novel representation of source code called a code property graph that merges concepts of classic program analysis, namely abstract syntax trees, control flow graphs and program dependence graphs, into a joint data structure. This comprehensive representation enables us to elegantly model templates for common vulnerabilities with graph traversals that, for instance, can identify buffer overflows, integer overflows, format string vulnerabilities, or memory disclosures. We implement our approach using a popular graph database and demonstrate its efficacy by identifying 18 previously unknown vulnerabilities in the source code of the Linux kernel.},
  doi        = {10.1109/SP.2014.44},
  file       = {:Yamaguchi2014 - Modeling and Discovering Vulnerabilities with Code Property Graphs.pdf:PDF;:yamaguchi2014.pdf:PDF},
  issn       = {2375-1207},
  keywords   = {Syntactics, Security, Abstracts, Joints, Databases, Kernel, Vulnerabilities, Static Analysis, Graph Databases, skimmed},
  readstatus = {skimmed},
}

@Article{10.1145/1095430.1081736,
  author     = {Bevan, Jennifer and Whitehead, E. James and Kim, Sunghun and Godfrey, Michael},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Facilitating Software Evolution Research with Kenyon},
  year       = {2005},
  issn       = {0163-5948},
  month      = sep,
  number     = {5},
  pages      = {177–186},
  volume     = {30},
  abstract   = {Software evolution research inherently has several resource-intensive logistical constraints. Archived project artifacts, such as those found in source code repositories and bug tracking systems, are the principal source of input data. Analysis-specific facts, such as commit metadata or the location of design patterns within the code, must be extracted for each change or configuration of interest. The results of this resource-intensive "fact extraction" phase must be stored efficiently, for later use by more experimental types of research tasks, such as algorithm or model refinement. In order to perform any type of software evolution research, each of these logistical issues must be addressed and an implementation to manage it created. In this paper, we introduce Kenyon, a system designed to facilitate software evolution research by providing a common set of solutions to these common logistical problems. We have used Kenyon for processing source code data from 12 systems of varying sizes and domains, archived in 3 different types of software configuration management systems. We present our experiences using Kenyon with these systems, and also describe Kenyon's usage by students in a graduate seminar class.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1095430.1081736},
  file       = {:10.1145_1095430.1081736 - Facilitating Software Evolution Research with Kenyon.pdf:PDF},
  issue_date = {September 2005},
  keywords   = {software configuration management, software stratigraphy, software evolution, skimmed},
  numpages   = {10},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1095430.1081736},
}

@InProceedings{10.5555/519621.853375,
  author     = {Richner, Tamar and Ducasse, St\'{e}phane},
  booktitle  = {Proceedings of the IEEE International Conference on Software Maintenance},
  title      = {Recovering High-Level Views of Object-Oriented Applications from Static and Dynamic Information},
  year       = {1999},
  address    = {USA},
  pages      = {13},
  publisher  = {IEEE Computer Society},
  series     = {ICSM '99},
  abstract   = {Recovering architectural documentation from code is crucial to maintaining and reengineering software systems. Reverse engineering and program understanding approaches are often limited by the fact that (1) they propose a fixed set of predefined views and (2) they consider either purely static or purely dynamic views of the application. In this paper we present an environment supporting the generation of tailorable views of object-oriented systems from both static and dynamic information. Our approach is based on the combination of user-defined queries which allow an engineer to create high-level abstractions and to produce views using these abstractions.},
  file       = {:10.5555_519621.853375 - Recovering High Level Views of Object Oriented Applications from Static and Dynamic Information.pdf:PDF},
  isbn       = {0769500161},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Schafer2006,
  author     = {Schafer, T. and Eichberg, M. and Haupt, M. and Mezini, M.},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {The {SEXTANT} {Software} {Exploration} {Tool}},
  year       = {2006},
  issn       = {1939-3520},
  month      = sep,
  number     = {9},
  pages      = {753--768},
  volume     = {32},
  abstract   = {In this paper, we discuss a set of functional requirements for software exploration tools and provide initial evidence that various combinations of these features are needed to effectively assist developers in understanding software. We observe that current tools for software exploration only partly support these features. This has motivated the development of SEXTANT, a software exploration tool tightly integrated into the Eclipse IDE that has been developed to fill this gap. By means of case studies, we demonstrate how the requirements fulfilled by SEXTANT are conducive to an understanding needed to perform a maintenance task},
  doi        = {10.1109/TSE.2006.94},
  file       = {:C\:/Users/Wernsen/Downloads/schafer2006.pdf:PDF},
  keywords   = {Software tools, Visualization, Navigation, Software maintenance, Cognition, Computer architecture, Reverse engineering, Software exploration, program comprehension, reverse engineering, software maintenance, software visualization., skimmed},
  readstatus = {skimmed},
}

@InProceedings{Favre2001,
  author     = {Favre, J.-M.},
  booktitle  = {Proceedings 9th {International} {Workshop} on {Program} {Comprehension}. {IWPC} 2001},
  title      = {G/sup {SEE}/: a {Generic} {Software} {Exploration} {Environment}},
  year       = {2001},
  month      = may,
  note       = {ISSN: 1092-8138},
  pages      = {233--244},
  abstract   = {Large software products are very difficult to understand. One way to cope with this problem is to provide tools generating different software views. Unfortunately, there are so many different entity types and relationships in a large software product that building a specific tool for each view is not cost-effective. This paper presents G/sup SEE/ (Generic Software Exploration Environment). G/sup SEE/ consists of an object-oriented framework and a set of customizable tools. Thanks to this environment, only a few lines are needed to produce graphical views from virtually any data source. G/sup SEE/ has been successfully applied to improving the understanding of different software artifacts, including a multi-million lines-of-code (LOC) program.},
  doi        = {10.1109/WPC.2001.921734},
  file       = {:C\:/Users/Wernsen/Downloads/10.1.1.22.2383.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Software tools, Costs, Software maintenance, Concrete, Visualization, Lab-on-a-chip, Software testing, Software libraries, Software design, Navigation, skimmed},
  readstatus = {skimmed},
  shorttitle = {G/sup {SEE}/},
}

@InProceedings{Ferenc2002,
  author     = {Ferenc, R. and Beszedes, A. and Tarkiainen, M. and Gyimothy, T.},
  booktitle  = {International {Conference} on {Software} {Maintenance}, 2002. {Proceedings}.},
  title      = {Columbus - reverse engineering tool and schema for {C}++},
  year       = {2002},
  month      = oct,
  note       = {ISSN: 1063-6773},
  pages      = {172--181},
  abstract   = {One of the most critical issues in large-scale software development and maintenance is the rapidly growing size and complexity of software systems. As a result of this rapid growth there is a need to better understand the relationships between the different parts of a large software system. In this paper we present a reverse engineering framework called Columbus that is able to analyze large C++ projects, and a schema for C++ that prescribes the form of the extracted data. The flexible architecture of the Columbus system with a powerful C++ analyzer and schema makes it a versatile and readily extendible toolset for reverse engineering. This tool is free for scientific and educational purposes and we fervently hope that it will assist academic persons in any research work related to C++ re- and reverse engineering.},
  doi        = {10.1109/ICSM.2002.1167764},
  file       = {:Ferenc2002 - Columbus Reverse Engineering Tool and Schema for C++.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {Reverse engineering, Data mining, Software systems, Software maintenance, Large-scale systems, Programming, Data visualization, Filtering, Artificial intelligence, Computer architecture, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Bergmann2011,
  author     = {Bergmann, Gábor and Ujhelyi, Zoltán and Ráth, István and Varró, Dániel},
  booktitle  = {Theory and {Practice} of {Model} {Transformations}},
  title      = {A {Graph} {Query} {Language} for {EMF} {Models}},
  year       = {2011},
  address    = {Berlin, Heidelberg},
  editor     = {Cabot, Jordi and Visser, Eelco},
  pages      = {167--182},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {While model queries are important components in model-driven tool chains, they are still frequently implemented using traditional programming languages, despite the availability of model query languages due to performance and expressiveness issues. In the current paper, we propose EMF-IncQuery as a novel, graph-based query language for EMF models by adapting the query language of the Viatra2 model transformation framework to inherit its concise, declarative nature, but to properly tailor the new query language to the modeling specificities of EMF. The EMF-IncQuery language includes (i) structural restrictions for queries imposed by EMF models, (ii) syntactic sugar and notational shorthand in queries, (iii) true semantic extensions which introduce new query features, and (iv) a constraint-based static type checking method to detect violations of EMF-specific type inference rules.},
  doi        = {10.1007/978-3-642-21732-6_12},
  file       = {:Bergmann2011 - A Graph Query Language for EMF Models.pdf:PDF;:10.1007_978-3-642-21732-6.pdf:PDF},
  isbn       = {9783642217326},
  keywords   = {Model Transformation , Query Language , Transitive Closure , Graph Transformation , Graph Pattern , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@InProceedings{10.5555/1926458.1926467,
  author     = {Bergmann, G\'{a}bor and Horv\'{a}th, \'{A}kos and R\'{a}th, Istv\'{a}n and Varr\'{o}, D\'{a}niel and Balogh, Andr\'{a}s and Balogh, Zolt\'{a}n and \"{O}kr\"{o}s, Andr\'{a}s},
  booktitle  = {Proceedings of the 13th International Conference on Model Driven Engineering Languages and Systems: Part I},
  title      = {Incremental Evaluation of Model Queries over EMF Models},
  year       = {2010},
  address    = {Berlin, Heidelberg},
  pages      = {76–90},
  publisher  = {Springer-Verlag},
  series     = {MODELS'10},
  abstract   = {Model-driven development tools built on industry standard platforms, such as the Eclipse Modeling Framework (EMF), heavily utilize model queries in model transformation, well-formedness constraint validation and domain-specific model execution. As these queries are executed rather frequently in interactive modeling applications, they have a significant impact on runtime performance and end user experience. However, due to their complexity, these queries can be time consuming to implement and optimize on a case-by-case basis. Consequently, there is a need for a model query framework that combines an easy-touse and concise declarative query formalism with high runtime performance.In this paper, we propose a declarative EMF model query framework using the graph pattern formalism as the query specification language. These graph patterns describe the arrangement and properties of model elements that correspond to, e.g. a well-formedness constraint, or an application context of a model transformation rule.For improved runtime performance, we employ incremental pattern matching techniques: matches of patterns are stored and incrementally maintained upon model manipulation. As a result, query operations can be executed instantly, independently of the complexity of the constraint and the size of the model. We demonstrate our approach in an industrial (AUTOSAR) model validation context and compare it against other solutions.},
  file       = {:10.5555_1926458.1926467 - Incremental Evaluation of Model Queries Over EMF Models.pdf:PDF},
  isbn       = {3642161448},
  keywords   = {model query, model validation, EMF, incremental pattern matching, skimmed},
  location   = {Oslo, Norway},
  numpages   = {15},
  readstatus = {skimmed},
}

@Article{Perez2010,
  author     = {Pérez, Javier and Crespo, Yania and Hoffmann, Berthold and Mens, Tom},
  journal    = {International Journal on Software Tools for Technology Transfer},
  title      = {A case study to evaluate the suitability of graph transformation tools for program refactoring},
  year       = {2010},
  issn       = {1433-2787},
  month      = jul,
  number     = {3},
  pages      = {183--199},
  volume     = {12},
  abstract   = {This article proposes a case study to evaluate the suitability of graph transformation tools for program refactoring. To qualify for this purpose, a graph transformation system must be able to (1) import a graph-based representation of models of Java programs, (2) allow these models to be transformed interactively with well-known program refactorings and (3) export the resulting models in the same graph-based format used as input. The case study aims to enable comparison of various features of graph transformation tools, such as their expressiveness and their ability to interact with the user. The model of Java programs is presented and some examples for translating Java source code into the model are provided. The refactorings selected for the case study are specified in detail.},
  doi        = {10.1007/s10009-010-0153-y},
  file       = {:Perez2010 - A Case Study to Evaluate the Suitability of Graph Transformation Tools for Program Refactoring.pdf:PDF;:C\:/Users/Wernsen/Downloads/Pérez2010_Article_ACaseStudyToEvaluateTheSuitabi.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1007/s10009-010-0153-y},
  urldate    = {2021-06-18},
}

@Article{Mens2005,
  author     = {Mens, Tom and Eetvelde, Niels Van and Demeyer, Serge and Janssens, Dirk},
  journal    = {Journal of Software Maintenance and Evolution: Research and Practice},
  title      = {Formalizing refactorings with graph transformations},
  year       = {2005},
  issn       = {1532-0618},
  number     = {4},
  pages      = {247--276},
  volume     = {17},
  abstract   = {The widespread interest in refactoring—transforming the source-code of an object-oriented program without changing its external behaviour—has increased the need for a precise definition of refactoring transformations and their properties. In this paper we explore the use of graph rewriting for specifying refactorings and their effect on programs. We introduce a graph representation for programs and show how two representative refactorings can be expressed by graph productions. Then we demonstrate that it is possible to prove that refactorings preserve certain program properties, and that graph rewriting is a suitable formalism for such proofs. Copyright © 2005 John Wiley \& Sons, Ltd.},
  copyright  = {Copyright © 2005 John Wiley \& Sons, Ltd.},
  doi        = {10.1002/smr.316},
  file       = {:Mens2005 - Formalizing Refactorings with Graph Transformations.pdf:PDF;:Formalising_refactorings_with_graph_tran.pdf:PDF},
  keywords   = {refactoring, formal specification, graph transformation, behaviour preservation, skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.316},
  urldate    = {2021-06-18},
}

@InProceedings{10.1145/581339.581382,
  author     = {Niere, J\"{o}rg and Sch\"{a}fer, Wilhelm and Wadsack, J\"{o}rg P. and Wendehals, Lothar and Welsh, Jim},
  booktitle  = {Proceedings of the 24th International Conference on Software Engineering},
  title      = {Towards Pattern-Based Design Recovery},
  year       = {2002},
  address    = {New York, NY, USA},
  pages      = {338–348},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '02},
  abstract   = {A method and a corresponding tool is described which assist design recovery and program understanding by recognising instances of design patterns semi-automatically. The approach taken is specifically designed to overcome the existing scalability problems caused by many design and implementation variants of design pattern instances. Our approach is based on a new recognition algorithm which works incrementally rather than trying to analyse a possibly large software system in one pass without any human intervention. The new algorithm exploits domain and context knowledge given by a reverse engineer and by a special underlying data structure, namely a special form of an annotated abstract syntax graph. A comparative and quantitative evaluation of applying the approach to the Java AWT and JGL libraries is also given.},
  doi        = {10.1145/581339.581382},
  file       = {:10.1145_581339.581382 - Towards Pattern Based Design Recovery.pdf:PDF},
  isbn       = {158113472X},
  keywords   = {skimmed},
  location   = {Orlando, Florida},
  numpages   = {11},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/581339.581382},
}

@InProceedings{10.1145/2328876.2328878,
  author     = {Arendt, Thorsten and Taentzer, Gabriele},
  booktitle  = {Proceedings of the Fifth Workshop on Refactoring Tools},
  title      = {Integration of Smells and Refactorings within the Eclipse Modeling Framework},
  year       = {2012},
  address    = {New York, NY, USA},
  pages      = {8–15},
  publisher  = {Association for Computing Machinery},
  series     = {WRT '12},
  abstract   = {Models are primary artifacts in model-based, and especially, in model-driven software development processes. Therefore, software quality and quality assurance frequently leads back to the quality and quality assurance of the involved models. In our approach, we propose a model quality assurance process that is based on static model analysis and uses techniques like model metrics and model smells. Based on the outcome of the model analysis, appropriate model refactoring steps are performed. Appropriate tools support the included techniques, i.e. metrics, smells, and refactorings, for models that are based on the Eclipse Modeling Framework (EMF). In this paper, we present the integration of the two model quality tools EMF Smell and EMF Refactor. This integration provides modelers with a quick and easy way to erase model smells by automatically suggesting appropriate model refactorings, and to get warnings in cases where new model smells come in by applying a certain refactoring.},
  doi        = {10.1145/2328876.2328878},
  file       = {:10.1145_2328876.2328878 - Integration of Smells and Refactorings within the Eclipse Modeling Framework.pdf:PDF},
  isbn       = {9781450315005},
  keywords   = {model smell, model refactoring, Eclipse modeling framework, model quality, skimmed},
  location   = {Rapperswil, Switzerland},
  numpages   = {8},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/2328876.2328878},
}

@Article{Moha2010,
  author     = {Moha, Naouel and Gueheneuc, Yann-Gael and Duchien, Laurence and Le Meur, Anne-Francoise},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {{DECOR}: {A} {Method} for the {Specification} and {Detection} of {Code} and {Design} {Smells}},
  year       = {2010},
  issn       = {1939-3520},
  month      = jan,
  number     = {1},
  pages      = {20--36},
  volume     = {36},
  abstract   = {Code and design smells are poor solutions to recurring implementation and design problems. They may hinder the evolution of a system by making it hard for software engineers to carry out changes. We propose three contributions to the research field related to code and design smells: (1) DECOR, a method that embodies and defines all the steps necessary for the specification and detection of code and design smells, (2) DETEX, a detection technique that instantiates this method, and (3) an empirical validation in terms of precision and recall of DETEX. The originality of DETEX stems from the ability for software engineers to specify smells at a high level of abstraction using a consistent vocabulary and domain-specific language for automatically generating detection algorithms. Using DETEX, we specify four well-known design smells: the antipatterns Blob, Functional Decomposition, Spaghetti Code, and Swiss Army Knife, and their 15 underlying code smells, and we automatically generate their detection algorithms. We apply and validate the detection algorithms in terms of precision and recall on XERCES v2.7.0, and discuss the precision of these algorithms on 11 open-source systems.},
  doi        = {10.1109/TSE.2009.50},
  file       = {:Moha2010 - DECOR_ a Method for the Specification and Detection of Code and Design Smells.pdf:PDF},
  keywords   = {Detection algorithms, Vocabulary, Domain specific languages, Algorithm design and analysis, Metamodeling, Java, Design engineering, Object oriented programming, Phase detection, Costs, Antipatterns, design smells, code smells, specification, metamodeling, detection, Java., skimmed},
  readstatus = {skimmed},
  shorttitle = {{DECOR}},
}

@InProceedings{Stevens2013,
  author     = {Stevens, Reinout and De Roover, Coen and Noguera, Carlos and Jonckers, Viviane},
  booktitle  = {2013 17th {European} {Conference} on {Software} {Maintenance} and {Reengineering}},
  title      = {A {History} {Querying} {Tool} and {Its} {Application} to {Detect} {Multi}-version {Refactorings}},
  year       = {2013},
  month      = mar,
  note       = {ISSN: 1534-5351},
  pages      = {335--338},
  abstract   = {Version Control Systems (VCS) have become indispensable in developing software. In order to provide support for change management, they track the history of software projects. Tool builders can exploit this latent historical information to provide insights in the evolution of the project. For example, the information needed to identify when and where a particular refactoring was applied is implicitly present in the VCS. However, tool support for eliciting this information is lacking. So far, no general-purpose history querying tool capable of answering a wide variety of questions about the evolution of software exists. Therefore, we generalize the idea of a program querying tool to a history querying tool. A program querying tool reifies the program's code into a knowledge base, from which it retrieves elements that exhibit characteristics specified through a user-provided program query. Our history querying tool, QwalKeko, enables specifying the evolution of source code characteristics across multiple versions of Java projects versioned in Git. We apply QwalKeko to the problem of detecting refactorings, specified as the code changes induced by each refactoring. These specifications stem from the literature, but are limited to changes between two successive versions. We demonstrate the expressiveness of our tool by generalizing the specifications such that refactorings can span multiple versions.},
  doi        = {10.1109/CSMR.2013.44},
  file       = {:C\:/Users/Wernsen/Downloads/vub-soft-tr-13-02.pdf:PDF},
  issn       = {1534-5351},
  keywords   = {History, Java, Database languages, Europe, Software maintenance, Control systems, program comprehension tools, software repositories, refactoring, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Klint2009,
  author     = {Klint, Paul and van der Storm, Tijs and Vinju, Jurgen},
  booktitle  = {2009 {Ninth} {IEEE} {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation}},
  title      = {{RASCAL}: {A} {Domain} {Specific} {Language} for {Source} {Code} {Analysis} and {Manipulation}},
  year       = {2009},
  month      = sep,
  pages      = {168--177},
  abstract   = {Many automated software engineering tools require tight integration of techniques for source code analysis and manipulation. State-of-the-art tools exist for both, but the domains have remained notoriously separate because different computational paradigms fit each domain best. This impedance mismatch hampers the development of new solutions because the desired functionality and scalability can only be achieved by repeated and ad hoc integration of different techniques. RASCAL is a domain-specific language that takes away most of this boilerplate by integrating source code analysis and manipulation at the conceptual, syntactic, semantic and technical level. We give an overview of the language and assess its merits by implementing a complex refactoring.},
  doi        = {10.1109/SCAM.2009.28},
  file       = {:Klint2009 - RASCAL_ a Domain Specific Language for Source Code Analysis and Manipulation.pdf:PDF},
  keywords   = {Domain specific languages, Libraries, Logic programming, Java, Pattern matching, Storms, Informatics, Software engineering, Impedance, Scalability, source code analysis, source code manipulation, meta-programming, transformation, skimmed},
  readstatus = {skimmed},
  shorttitle = {{RASCAL}},
}

@InProceedings{10.1109/SCAM.2014.32,
  author     = {De Roover, Coen and Inoue, Katsuro},
  title      = {The Ekeko/X Program Transformation Tool},
  year       = {2014},
  month      = {12},
  doi        = {10.1109/SCAM.2014.32},
  file       = {:inproceedings - The Ekeko_X Program Transformation Tool.pdf:PDF},
  journal    = {Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Fabry2004,
  author     = {Fabry, Johan and Mens, Tom},
  journal    = {Computer Languages, Systems \& Structures},
  title      = {Language-independent detection of object-oriented design patterns},
  year       = {2004},
  issn       = {1477-8424},
  month      = apr,
  number     = {1},
  pages      = {21--33},
  volume     = {30},
  abstract   = {This paper shows that one can reason at a meta level about the structure of object-oriented source code in a language-independent way. To achieve this, we propose a language-independent meta-level interface to extract complex information about the structure of the source code. This approach is validated by defining a set of logic queries to detect object-oriented best practice patterns and design patterns in two different languages: Smalltalk and Java. The queries were applied to two similar medium-sized applications available for each language, and the results were confirmed by manually investigating the source code and available documentation.},
  doi        = {10.1016/j.cl.2003.09.002},
  file       = {:C\:/Users/Wernsen/Downloads/CLSS2004-FabryMens.pdf:PDF},
  keywords   = {Logic meta-programming, Object-oriented programming, Design patterns, Best practice patterns, Smalltalk, Java, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Smalltalk {Language}},
  url        = {https://www.sciencedirect.com/science/article/pii/S1477842403000423},
  urldate    = {2021-06-23},
}

@Article{Koellmann2007,
  author     = {Köllmann, Carsten and Goedicke, Michael},
  journal    = {Electronic Communications of the EASST},
  title      = {Automation of {Java} {Code} {Analysis} for {Programming} {Exercises}},
  year       = {2007},
  issn       = {1863-2122},
  month      = jul,
  number     = {0},
  volume     = {1},
  abstract   = {In this paper we present a tool environment for semi-automatic verification of basic programming exercises. We describe how graph transformation can be used for analysis of code structures and present an example from a current course.},
  copyright  = {Copyright (c)},
  doi        = {10.14279/tuj.eceasst.1.78},
  file       = {:Koellmann2007 - Automation of Java Code Analysis for Programming Exercises.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://journal.ub.tu-berlin.de/eceasst/article/view/78},
  urldate    = {2021-06-24},
}

@TechReport{Czarnecki07framework-specificmodeling,
  author      = {Krzysztof Czarnecki},
  institution = {ECE, U. of Waterloo, Tech. Rep},
  title       = {Framework-specific modeling languages; examples and algorithms},
  year        = {2007},
  file        = {:C\:/Users/Wernsen/Downloads/2007-antkiewicz-fsmls-examples-algorithms.pdf:PDF},
  keywords    = {skimmed},
  readstatus  = {skimmed},
}

@InProceedings{10.1145/1028664.1028688,
  author     = {Eichberg, Michael and Sch\"{a}fer, Thorsten},
  booktitle  = {Companion to the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications},
  title      = {XIRC: Cross-Artifact Information Retrieval [GPCE]},
  year       = {2004},
  address    = {New York, NY, USA},
  pages      = {43–44},
  publisher  = {Association for Computing Machinery},
  series     = {OOPSLA '04},
  abstract   = {In large scale software development projects, in particular in the field of Component-Based Software Engineering (CBSE), different kinds of a project's artifacts are used and related information is spread over the different artifacts. E.g., the transaction attributes ("Require", "Requires-New",etc.) of methods of an Enterprise Java Bean are defined in the deployment descriptor while the method bodies are defined in a Java class. If we want to put these information into relation, e.g., to find all methods with a specific transaction attribute, we have to use different search engines and have to map the information manually. It is not possible to execute "one query" that returns the desired result.XIRC is a platform that enables to define queries over a uniform representation of all artifacts of a software project. XIRC maps all artifacts of a project to XML representations and stores the documents in a database. The database can be queried using XQuery, a functional query language for XML documents. XIRC can be used as a sophisticated search engine, as a tool to check implementation restrictions, to find errors or as a basis for further tools for code generation and visualization.},
  doi        = {10.1145/1028664.1028688},
  file       = {:10.1145_1028664.1028688 - XIRC_ Cross Artifact Information Retrieval [GPCE].pdf:PDF},
  isbn       = {1581138334},
  keywords   = {search engine, cross-artifact information engineering, code validation, skimmed},
  location   = {Vancouver, BC, CANADA},
  numpages   = {2},
  readstatus = {skimmed},
  url        = {https://doi-org.ezproxy.elib11.ub.unimaas.nl/10.1145/1028664.1028688},
}

@Article{srcml2005,
  author     = {Collard, Michael},
  title      = {Addressing source code using srcml},
  year       = {2005},
  month      = {01},
  file       = {:article - Addressing Source Code Using Srcml.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Collard2010,
  author     = {Collard, Michael L. and Maletic, Jonathan I. and Robinson, Brian P.},
  booktitle  = {2010 {IEEE} {International} {Conference} on {Software} {Maintenance}},
  title      = {A lightweight transformational approach to support large scale adaptive changes},
  year       = {2010},
  month      = sep,
  note       = {ISSN: 1063-6773},
  pages      = {1--10},
  abstract   = {An approach to automate adaptive maintenance changes on large-scale software systems is presented. This approach uses lightweight parsing and lightweight on-the-fly static analysis to support transformations that make corrections to source code in response to adaptive maintenance changes, such as platform changes. SrcML, an XML source code representation, is used and transformations can be performed using either XSLT or LINQ. A number of specific adaptive changes are presented, based on recent adaptive maintenance needs from products at ABB Inc. The transformations are described in detail and then demonstrated on a number of examples from the production systems. The results are compared with manual adaptive changes that were done by professional developers. The approach performed better than the manual changes, as it successfully transformed instances missed by the developers while not missing any instances itself. The work demonstrates that this lightweight approach is both efficient and accurate with an overall cost savings in development time and effort.},
  doi        = {10.1109/ICSM.2010.5609719},
  file       = {:Collard2010 - A Lightweight Transformational Approach to Support Large Scale Adaptive Changes.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {Maintenance engineering, XML, Adaptive systems, Organizations, Context, Software, Manuals, Source Code Transformation, static analysis, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Kramer1996,
  author     = {Kramer, C. and Prechelt, L.},
  booktitle  = {Proceedings of {WCRE} '96: 4rd {Working} {Conference} on {Reverse} {Engineering}},
  title      = {Design recovery by automated search for structural design patterns in object-oriented software},
  year       = {1996},
  month      = nov,
  pages      = {208--215},
  abstract   = {The object-oriented design community has recently begun to collect so-called design patterns: cliches plus hints to their recommended use in software construction. The structural design patterns Adapter, Bridge, Composite, Decorator, and Proxy represent packaged problem/context/solution/properties descriptions to common problems in object-oriented design. Localizing instances of these patterns in existing software produced without explicit use of patterns can improve the maintainability of software. In the authors' approach, called the Pat system, design information is extracted directly from C++ header files and stored in a repository. The patterns are expressed as PROLOG rules and the design information is translated into facts. A single PROLOG query is then used to search for all patterns. They examined four applications, including the popular class libraries zApp and LEDA, with Pat. With some restrictions all pattern instances are found; the precision is about 40 percent. Since manual filtering of the output is relatively easy, they consider Pat a useful tool for discovering or recovering design information.},
  doi        = {10.1109/WCRE.1996.558905},
  file       = {:Kramer1996 - Design Recovery by Automated Search for Structural Design Patterns in Object Oriented Software.pdf:PDF},
  keywords   = {Packaging, Computer aided software engineering, Software maintenance, Software engineering, Microarchitecture, Context, Terminology, Software tools, Bridges, Reverse engineering, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Noguera2006OpenSP,
  author     = {Carlos Noguera and R. Pawlak},
  title      = {Open Static Pointcuts Through Source Code Templates},
  year       = {2006},
  file       = {:C\:/Users/Wernsen/Downloads/nogueraPawlak_ODAL06.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Linstead2009,
  author     = {Linstead, Erik and Bajracharya, Sushil and Ngo, Trung and Rigor, Paul and Lopes, Cristina and Baldi, Pierre},
  journal    = {Data Mining and Knowledge Discovery},
  title      = {Sourcerer: mining and searching internet-scale software repositories},
  year       = {2009},
  issn       = {1573-756X},
  month      = apr,
  number     = {2},
  pages      = {300--336},
  volume     = {18},
  abstract   = {Large repositories of source code available over the Internet, or within large organizations, create new challenges and opportunities for data mining and statistical machine learning. Here we first develop Sourcerer, an infrastructure for the automated crawling, parsing, fingerprinting, and database storage of open source software on an Internet-scale. In one experiment, we gather 4,632 Java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data first reveal robust power-law behavior for package, method call, and lexical containment distributions. We then develop and apply unsupervised, probabilistic, topic and author-topic (AT) models to automatically discover the topics embedded in the code and extract topic-word, document-topic, and AT distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing source file similarity, developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering an software development staffing. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to significantly improve software retrieval performance, increasing the area under the curve (AUC) retrieval metric to 0.92– roughly 10–30\% better than previous approaches based on text alone. A prototype of the system is available at: http://sourcerer.ics.uci.edu.},
  doi        = {10.1007/s10618-008-0118-x},
  file       = {:Linstead2009 - Sourcerer_ Mining and Searching Internet Scale Software Repositories.pdf:PDF;:10.1.1.395.7047.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {Sourcerer},
  url        = {https://doi.org/10.1007/s10618-008-0118-x},
  urldate    = {2021-06-24},
}

@InProceedings{10.5555/800099.803204,
  author     = {Browne, J. C. and Johnson, David B.},
  booktitle  = {Proceedings of the 3rd International Conference on Software Engineering},
  title      = {FAST: A Second Generation Program Analysis System},
  year       = {1978},
  pages      = {142–148},
  publisher  = {IEEE Press},
  series     = {ICSE '78},
  abstract   = {FAST (Fortran Analysis System) implements a powerful set of analysis capabilities on Fortran source language programs. Its implementation was accomplished through the integration of existing software systems and by the use of modern language system development tools. The result is an order of magnitude reduction in effort of implementation coupled with a sizable increase in system capabilities. The use of a general purpose, commercially available data management system as a data handler and data correlator was a dominant factor in both reduction in effort of implementation and generation of additional power and flexibility in the analysis capabilities offered by FAST. FAST implements a capability for systematically qualified program analyses which is unique among existing program analyzers. This capability should be particularly useful in the program maintenance environment.},
  file       = {:10.5555_800099.803204 - FAST_ a Second Generation Program Analysis System.pdf:PDF},
  keywords   = {skimmed},
  location   = {Atlanta, Georgia, USA},
  numpages   = {7},
  readstatus = {skimmed},
}

@InProceedings{Eichberg2006,
  author     = {Eichberg, M. and Germanus, D. and Mezini, M. and Mrokon, L. and Schafer, T.},
  booktitle  = {Conference on {Software} {Maintenance} and {Reengineering} ({CSMR}'06)},
  title      = {{QScope}: an open, extensible framework for measuring software projects},
  year       = {2006},
  month      = mar,
  note       = {ISSN: 1534-5351},
  pages      = {10 pp.--122},
  abstract   = {To measure the particularities of modern software development projects that use different types of documents for the implementation of a program, new metrics need to be defined. Further, well established metrics, such as e.g., lack of cohesion or coupling between objects need to be reconsidered in the presence of new language features. Not being able to thoroughly measure a project can lead to false conclusions with respect to the measured source files. Currently, a large number of metrics tools exists, but unfortunately most tools are not extensible, or they are limited with respect to the types of documents that can be taken into account. Further, support for testing newly developed metrics is also missing. In this paper, we present QScope \$an open, extensible metrics framework. QScope is open with respect to the supported artifacts and explicitly enables the user to implement new metrics by reasoning over all artifacts using a declarative query language. As we showed in this paper, using a declarative query language enables a concise definition of new metrics},
  doi        = {10.1109/CSMR.2006.42},
  file       = {:Eichberg2006 - QScope_ an Open, Extensible Framework for Measuring Software Projects.pdf:PDF},
  issn       = {1534-5351},
  keywords   = {Software measurement, Software systems, XML, Particle measurements, Programming, Database languages, Computer science, Software quality, Software testing, Software maintenance, skimmed},
  readstatus = {skimmed},
  shorttitle = {{QScope}},
}

@Misc{Wuyts01alogic,
  author     = {Roel Wuyts and Advisor Prof and Dr. Theo D’hondt and Brian Herbert and Kevin J. Anderson and Prelude Dune and House Harkonnen},
  title      = {A Logic Meta-Programming Approach to Support the Co-Evolution of Object-Oriented Design and Implementation},
  year       = {2001},
  file       = {:Wuyts01alogic - A Logic Meta Programming Approach to Support the Co Evolution of Object Oriented Design and Implementation.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Mens2006,
  author     = {Mens, K. and Kellens, A.},
  booktitle  = {Conference on {Software} {Maintenance} and {Reengineering} ({CSMR}'06)},
  title      = {{IntensiVE}, a toolsuite for documenting and checking structural source-code regularities},
  year       = {2006},
  month      = mar,
  note       = {ISSN: 1534-5351},
  pages      = {10 pp.--248},
  abstract   = {As size and complexity of software systems increase, preserving the design and specification of their implementation structure gains importance in order to maintain the evolvability of the system. However, due to constant changes, the implementation structure and its documentation tend to dilute over time. To address this problem, we developed IntensiVE: a toolsuite for documenting and checking structural source-code regularities. Building on the underlying models of intensional views and relations, the toolsuite helps a developer in documenting structural source-code regularities, verifying them and offering fine-grained feedback when the source-code does not satisfy those regularities. By illustrating our tools on a Smalltalk application, we show that violations of the source code against the structural regularities can be detected easily and that our toolsuite provides useful feedback for a developer to refine the regularities or to fix the code so that it does satisfy the regularities},
  doi        = {10.1109/CSMR.2006.29},
  file       = {:Mens2006 - IntensiVE, a Toolsuite for Documenting and Checking Structural Source Code Regularities.pdf:PDF},
  issn       = {1534-5351},
  keywords   = {Documentation, Feedback, Software systems, Object oriented modeling, Software maintenance, Application software, Buildings, Scholarships, Technological innovation, Packaging, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Eichberg2004,
  author     = {Eichberg, M. and Mezini, M. and Ostermann, K. and Schafer, T.},
  booktitle  = {11th {Working} {Conference} on {Reverse} {Engineering}},
  title      = {{XIRC}: a kernel for cross-artifact information engineering in software development environments},
  year       = {2004},
  month      = nov,
  note       = {ISSN: 1095-1350},
  pages      = {182--191},
  abstract   = {We describe XIRC, a tool and architecture that enables to define queries over a uniform representation of all artifacts of a software project. These queries can be used for general cross-artifact information retrieval or for more special applications like checking implementation restrictions or conformance to style guides. XIRC is also a good basis to implement a broad range of tools for refactoring, generators, aspect-oriented programming and many other domains on top of it.},
  doi        = {10.1109/WCRE.2004.45},
  file       = {:Eichberg2004 - XIRC_ a Kernel for Cross Artifact Information Engineering in Software Development Environments.pdf:PDF},
  issn       = {1095-1350},
  keywords   = {Kernel, Programming, Information retrieval, Documentation, Java, Software tools, Software systems, Reverse engineering, Computer bugs, Visualization, skimmed},
  readstatus = {skimmed},
  shorttitle = {{XIRC}},
}

@InProceedings{10.1145/2591062.2591065,
  author     = {Deering, Tom and Kothari, Suresh and Sauceda, Jeremias and Mathews, Jon},
  booktitle  = {Companion Proceedings of the 36th International Conference on Software Engineering},
  title      = {Atlas: A New Way to Explore Software, Build Analysis Tools},
  year       = {2014},
  address    = {New York, NY, USA},
  pages      = {588–591},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE Companion 2014},
  abstract   = {Atlas is a new software analysis platform from EnSoft Corp. Atlas decouples the domain-specific analysis goal from its underlying mechanism by splitting analysis into two distinct phases. In the first phase, polynomial-time static analyzers index the software AST, building a rich graph database. In the second phase, users can explore the graph directly or run custom analysis scripts written using a convenient API. These features make Atlas ideal for both interaction and automation. In this paper, we describe the motivation, design, and use of Atlas. We present validation case studies, including the verification of safe synchronization of the Linux kernel, and the detection of malware in Android applications. Our ICSE 2014 demo explores the comprehension and malware detection use cases. Video: http://youtu.be/cZOWlJ-IO0k},
  doi        = {10.1145/2591062.2591065},
  file       = {:10.1145_2591062.2591065 - Atlas_ a New Way to Explore Software, Build Analysis Tools.pdf:PDF},
  isbn       = {9781450327688},
  keywords   = {Analysis platform, Static analysis, Human-in-the-loop, skimmed},
  location   = {Hyderabad, India},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/2591062.2591065},
}

@Article{10.1145/142882.143755,
  author     = {M\"{u}ller, H. A. and Tilley, S. R. and Orgun, M. A. and Corrie, B. D. and Madhavji, N. H.},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {A Reverse Engineering Environment Based on Spatial and Visual Software Interconnection Models},
  year       = {1992},
  issn       = {0163-5948},
  month      = nov,
  number     = {5},
  pages      = {88–98},
  volume     = {17},
  abstract   = {Reverse engineering is the process of extracting system abstractions and design information out of existing software systems. This information can then be used for subsequent development, maintenance, re-engineering, or reuse purposes. This process involves the identification of software artifacts in a particular subject system, and the aggregation of these artifacts to form more abstract system representations. This paper describes a reverse engineering environment which uses the spatial and visual information inherent in graphical representations of software systems to form the basis of a software interconnection model. This information is displayed and manipulated by the reverse engineer using an interactive graph editor to build subsystem structures out of software building blocks. The spatial component constitutes information about how a software structure looks. The coexistence of these two representations is critical to the comprehensive appreciation of the generated data, and greatly benefits subsequent analysis, processing, and decision-making.},
  address    = {New York, NY, USA},
  doi        = {10.1145/142882.143755},
  file       = {:10.1145_142882.143755 - A Reverse Engineering Environment Based on Spatial and Visual Software Interconnection Models.pdf:PDF},
  issue_date = {Dec. 1992},
  keywords   = {skimmed},
  numpages   = {11},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/142882.143755},
}

@InProceedings{Alnusair2010,
  author     = {Alnusair, Awny and Zhao, Tian},
  booktitle  = {2010 {IEEE} {International} {Conference} on {Information} {Reuse} {Integration}},
  title      = {Component search and reuse: {An} ontology-based approach},
  year       = {2010},
  month      = aug,
  pages      = {258--261},
  abstract   = {In order to realize the full potential of software reuse, effective search techniques are indeed essential. In this paper, we propose a semantic-based approach for retrieving relevant components from a reuse repository. This approach utilizes an ontology model that includes a source-code ontology, a component ontology, and a domain-specific ontology. Due to the indexing and knowledge population mechanisms we used, our approach supports various kinds of search techniques. However, our experiments show evidence that only pure semantic search that exploits domain knowledge tends to improve precision.},
  doi        = {10.1109/IRI.2010.5558931},
  file       = {:C\:/Users/Wernsen/Downloads/10.1.1.1052.3934.pdf:PDF},
  keywords   = {Ontologies, Semantics, Software, Resource description framework, Libraries, Object oriented modeling, OWL, Component retrieval, Semantic inference, skimmed},
  readstatus = {skimmed},
  shorttitle = {Component search and reuse},
}

@InProceedings{Holt2000,
  author     = {Holt, R.C. and Winter, A. and Schurr, A.},
  booktitle  = {Proceedings {Seventh} {Working} {Conference} on {Reverse} {Engineering}},
  title      = {{GXL}: toward a standard exchange format},
  year       = {2000},
  month      = nov,
  note       = {ISSN: 1095-1350},
  pages      = {162--171},
  abstract   = {This paper describes ongoing work toward the development of a standard software exchange format (SEF), for exchanging information among tools that analyze computer programs. A particular exchange format called GXL (Graph Exchange Language) is proposed. GXL can be viewed as a merger of well known formats (e.g. GraX, PROGRES, RPA, RSF and TA) for exchanging typed, attributed, directed graphs. By using XML as the notation, GXL offers a scalable and adaptable means to facilitate interoperability of reengineering tools.},
  doi        = {10.1109/WCRE.2000.891463},
  file       = {:Holt2000 - GXL_ toward a Standard Exchange Format.pdf:PDF},
  issn       = {1095-1350},
  keywords   = {Software tools, XML, Information analysis, Software standards, Corporate acquisitions, Reverse engineering, Data mining, Algebra, Tree graphs, Computer industry, skimmed},
  readstatus = {skimmed},
  shorttitle = {{GXL}},
}

@Article{Ebert2002,
  author     = {Ebert, Jürgen and Kullbach, Bernt and Riediger, Volker and Winter, Andreas},
  journal    = {Electronic Notes in Theoretical Computer Science},
  title      = {{GUPRO} - {Generic} {Understanding} of {Programs} {An} {Overview}},
  year       = {2002},
  issn       = {1571-0661},
  month      = nov,
  number     = {2},
  pages      = {47--56},
  volume     = {72},
  abstract   = {GUPRO is an integrated workbench to support program understanding of heterogenous software systems on arbitrary levels of granularity. GUPRO can be adapted to specific needs by an appropriate conceptual model of the target software. GUPRO is based on graph-technology. It heavily relies on graph querying and graph algorithms. Source code is extracted into a graph repository which can be viewed by an integrated querying and browsing facility. For C-like languages GUPRO browsing includesa complete treatment of preprocessor facilities. This paper summarizes the work done on GUPRO during the last seven years.},
  doi        = {10.1016/S1571-0661(05)80528-6},
  file       = {:Ebert2002 - GUPRO Generic Understanding of Programs an Overview.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {{GraBaTs} 2002, {Graph}-{Based} {Tools} ({First} {International} {Conference} on {Graph} {Transformation})},
  url        = {https://www.sciencedirect.com/science/article/pii/S1571066105805286},
  urldate    = {2021-06-25},
}

@InProceedings{Klint08usingrscript,
  author     = {Paul Klint},
  booktitle  = {In Working Session on Query Technologies and Applications for Program Comprehension (QTAPC},
  title      = {Using Rscript for software analysis},
  year       = {2008},
  file       = {:Klint08usingrscript - Using Rscript for Software Analysis.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Lin2004,
  author     = {Lin, Yuan and Holt, Richard C.},
  journal    = {Electronic Notes in Theoretical Computer Science},
  title      = {Formalizing {Fact} {Extraction}},
  year       = {2004},
  issn       = {1571-0661},
  month      = may,
  pages      = {93--102},
  volume     = {94},
  abstract   = {Reverse engineering commonly uses fact extraction to transform source programs to factbases. These factbases are in turn used to determine particular views or aspects of the program, such as its architecture or its anomalous structures. Fact extraction is usually defined in an ad hoc manner, which is often incomplete or inconsistent. This paper takes the position that formal specification of fact extraction is beneficial to the reverse engineering community. A formal specification can serve as an unambiguous and reliable standard for people who use, write or verify a fact extractor. We explain how a formal specification for extracted facts can be derived from the source language grammar in such a way that the relationship between the code and its corresponding extracted facts is made clear. To support our position, we report our experience with formalizing a version of the Datrix Schema.},
  doi        = {10.1016/j.entcs.2004.01.001},
  file       = {:Lin2004 - Formalizing Fact Extraction.pdf:PDF},
  keywords   = {schema, formalization, fact extraction, software architecture, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Proceedings of the {International} {Workshop} on {Meta}-{Models} and {Schemas} for {Reverse} {Engineering} ({ateM} 2003)},
  url        = {https://www.sciencedirect.com/science/article/pii/S1571066104050108},
  urldate    = {2021-06-25},
}

@Article{Moor2003,
  author     = {de Moor, Oege and Lacey, David and Van Wyk, Eric},
  journal    = {Higher-Order and Symbolic Computation},
  title      = {Universal {Regular} {Path} {Queries}},
  year       = {2003},
  issn       = {1573-0557},
  month      = mar,
  number     = {1},
  pages      = {15--35},
  volume     = {16},
  abstract   = {Given are a directed edge-labelled graph G with a distinguished node n0, and a regular expression P which may contain variables. We wish to compute all substitutions F (of symbols for variables), together with all nodes n such that all paths n0 ? n are in F(P). We derive an algorithm for this problem using relational algebra, and show how it may be implemented in Prolog. The motivation for the problem derives from a declarative framework for specifying compiler optimisations.},
  doi        = {10.1023/A:1023063919574},
  file       = {:Moor2003 - Universal Regular Path Queries.pdf:PDF;:10.1.1.102.590.pdf:PDF},
  keywords   = {skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1023/A:1023063919574},
  urldate    = {2021-06-25},
}

@InProceedings{Blewitt2001,
  author     = {Blewitt, A. and Bundy, A. and Stark, I.},
  booktitle  = {Proceedings 16th {Annual} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE} 2001)},
  title      = {Automatic verification of {Java} design patterns},
  year       = {2001},
  month      = nov,
  note       = {ISSN: 1938-4300},
  pages      = {324--327},
  abstract   = {Design patterns are widely used by object oriented designers and developers for building complex systems in object oriented programming languages such as Java. However, systems evolve over time, increasing the chance that the pattern in its original form will be broken. We attempt to show that many design patterns (implemented in Java) can be verified automatically. Patterns are defined in terms of variants, mini-patterns, and artifacts in a pattern description language called SPINE. These specifications are then processed by Hedgehog, an automated proof tool that attempts to prove that Java source code meets these specifications.},
  doi        = {10.1109/ASE.2001.989821},
  file       = {:Blewitt2001 - Automatic Verification of Java Design Patterns.pdf:PDF},
  issn       = {1938-4300},
  keywords   = {Java, Bridges, Informatics, Books, Formal languages, Buildings, Computer languages, Production facilities, Memory management, Runtime, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Sefika1996,
  author     = {Sefika, M. and Sane, A. and Campbell, R.H.},
  booktitle  = {Proceedings of {IEEE} 18th {International} {Conference} on {Software} {Engineering}},
  title      = {Monitoring compliance of a software system with its high-level design models},
  year       = {1996},
  month      = mar,
  note       = {ISSN: 0270-5257},
  pages      = {387--396},
  abstract   = {As a complex software system evolves, its implementation tends to diverge from the intended or documented design models. Such undesirable deviation makes the system hard to understand, modify and maintain. This paper presents a hybrid computer-assisted approach for confirming that the implementation of a system maintains its expected design models and rules. Our approach closely integrates logic-based static analysis and dynamic visualization, providing multiple code views and perspectives. We show that the hybrid technique helps determine design-implementation congruence at various levels of abstraction: concrete rules like coding guidelines, architectural models like design patterns or connectors, and subjective design principles like low coupling and high cohesion. The utility of our approach has been demonstrated in the development of /spl mu/Choices, a new multimedia operating system which inherits many design decisions and guidelines learned from experience in the construction and maintenance of its predecessor, Choices.},
  doi        = {10.1109/ICSE.1996.493433},
  file       = {:Sefika1996 - Monitoring Compliance of a Software System with Its High Level Design Models.pdf:PDF},
  issn       = {0270-5257},
  keywords   = {Software systems, Guidelines, Operating systems, Computerized monitoring, Visualization, Concrete, Computer science, World Wide Web, Multimedia systems, Utility programs, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Jarzabek1994a,
  author     = {Jarzabek, S. and Shen, Han and Chan, Hock Chuan},
  booktitle  = {Proceedings of 1st {Asia}-{Pacific} {Software} {Engineering} {Conference}},
  title      = {A hybrid program knowledge base for static program analyzers},
  year       = {1994},
  month      = dec,
  pages      = {400--409},
  abstract   = {Static program analyzers (SPA) are interactive tools that enhance program understanding by answering queries about programs. An SPA extracts relevant information from input programs and stores it in a program knowledge base (PKB). In this paper, we present a hybrid PKB design model that integrates a relational database with attributed syntax trees. In the hybrid PKB, global properties of programs are stored in a relational database and detailed program structures are stored as attributed syntax trees. The hybrid PKB approach simplifies the structure of the PKB and provides a flexible mechanism for analysis of complex structured objects such as syntax trees and control/data flow graphs. The model reduces the size of the database, and hence program queries can be answered efficiently.{\textless}{\textgreater}},
  doi        = {10.1109/APSEC.1994.465240},
  file       = {:Jarzabek1994a - A Hybrid Program Knowledge Base for Static Program Analyzers.pdf:PDF},
  keywords   = {Relational databases, Tree graphs, Flow graphs, Programming profession, Database languages, Information systems, Computer science, Information analysis, Data mining, Reverse engineering, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Burson1990,
  author     = {Burson, S. and Kotik, G.B. and Markosian, L.Z.},
  booktitle  = {Proceedings., {Fourteenth} {Annual} {International} {Computer} {Software} and {Applications} {Conference}},
  title      = {A program transformation approach to automating software re-engineering},
  year       = {1990},
  month      = oct,
  pages      = {314--322},
  abstract   = {The authors describe a novel approach to software re-engineering that combines several technologies: object-oriented databases integrated with parser, for capturing the software to be re-engineered; specification and pattern languages for querying and analyzing a database of software; and transformation rules for automatically generating re-engineered code. The authors then describe REFINE, an environment for program representation, analysis, and transformation that provides the tools needed to implement the automation of software maintenance and re-engineering. The transformational approach is illustrated with examples taken from actual experience in re-engineering software in C, JCL and NATURAL. It is concluded that the ability to support automation in modifying large software systems by using rule-based program transformation is a key innovation of the present approach that distinguishes it from tools that focus only on automation of program analysis.{\textless}{\textgreater}},
  doi        = {10.1109/CMPSAC.1990.139375},
  file       = {:Burson1990 - A Program Transformation Approach to Automating Software Re Engineering.pdf:PDF},
  keywords   = {Object oriented databases, Software tools, Pattern matching, Data analysis, Pattern analysis, Computer aided software engineering, Database languages, Maintenance engineering, Software maintenance, Resource management, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/965660.965675,
  author     = {Janzen, Doug and De Volder, Kris},
  booktitle  = {Proceedings of the 2003 OOPSLA Workshop on Eclipse Technology EXchange},
  title      = {Programs as Information},
  year       = {2003},
  address    = {New York, NY, USA},
  pages      = {69–73},
  publisher  = {Association for Computing Machinery},
  series     = {eclipse '03},
  abstract   = {In any programming system the environment in which a program is constructed is tightly bound to the format of the data used to represent that program. As software development environments are becoming more sophisticated, there is a growing need to get richer representations of programs that allow programs to be treated at a much higher level of abstraction than as sequences of characters in source files.In this paper we explore the use of a database representation as a medium for representing and manipulating programs. We report on the work we did on two different Eclipse plugins exemplifying the potential advantages of such a representation. The first plugin explores the use of a database representation to store programs written in a traditional programming language (Java). The second plugin is part of a research effort to try to define a programming language directly in terms of a database representation.},
  doi        = {10.1145/965660.965675},
  file       = {:10.1145_965660.965675 - Programs As Information.pdf:PDF},
  isbn       = {9781450374705},
  keywords   = {skimmed},
  location   = {Anaheim, California},
  numpages   = {5},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/965660.965675},
}

@InProceedings{Research2334,
  author     = {IBM Research and Report},
  title      = {RC23343 (W0409-135) September 21, 2004 Computer Science},
  year       = {2334},
  file       = {:C\:/Users/Wernsen/Downloads/rc23343.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Anderson2001SoftwareIU,
  author     = {P. Anderson and T. Teitelbaum},
  title      = {Software Inspection Using CodeSurfer},
  year       = {2001},
  file       = {:Anderson2001SoftwareIU - Software Inspection Using CodeSurfer.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Kniesel2004,
  author     = {Kniesel, Günter and Koch, Helge},
  journal    = {Science of Computer Programming},
  title      = {Static composition of refactorings},
  year       = {2004},
  issn       = {0167-6423},
  month      = aug,
  number     = {1},
  pages      = {9--51},
  volume     = {52},
  abstract   = {The number of possible refactorings is unlimited, so no tool vendor will ever be able to provide custom refactorings for all specific user needs. Therefore, we propose a new kind of refactoring tools, which allow users to create, edit and compose required refactorings just like any other documents. The heart of such a refactoring editor is the ability to compose larger refactorings from existing ones. Computing the precondition of the composite refactoring from the preconditions of the composed refactorings is non-trivial since earlier transformations influence the truth of preconditions of later ones. The ability to calculate these effects without referring to a particular program to which the refactorings should be applied is called program-independent composition. It is the prerequisite for creating composite refactorings that are reusable on arbitrary programs. The main contribution of this paper is a formal model for automatic, program-independent composition of conditional program transformations. We show that conditional transformations, including refactorings, can be composed from a limited set of basic operations. Program-independent derivation of a precondition for the composite is based on the notion of “transformation description”, which can be seen as a simplified, yet equally powerful, variant of Roberts’ “postconditions” (Practical analysis for refactoring, Ph.D. Thesis (1999)). Our approach simplifies the implementation of refactoring tools—only the basic operations and the ability for composition must be hard coded in a tool. As a proof of concept, we sketch a transformation framework that implements our approach (jConditioner) and, based on the framework, an experimental refactoring tool (ConTraCT) that includes the editing capabilities that motivated our work.},
  doi        = {10.1016/j.scico.2004.03.002},
  file       = {:Kniesel2004 - Static Composition of Refactorings.pdf:PDF},
  keywords   = {Refactoring, Conditional program transformation, OR-sequence, AND-sequence, Composition, Derivation of precondition, Transformation description, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Special {Issue} on {Program} {Transformation}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0167642304000462},
  urldate    = {2021-06-28},
}

@InProceedings{Wuyts1998,
  author     = {Wuyts, R.},
  booktitle  = {Proceedings. {Technology} of {Object}-{Oriented} {Languages}. {TOOLS} 26 ({Cat}. {No}.{98EX176})},
  title      = {Declarative reasoning about the structure of object-oriented systems},
  year       = {1998},
  month      = aug,
  pages      = {112--124},
  abstract   = {The structure of object-oriented systems typically forms a complicated tangled web of interdependent classes. Understanding this implicit and hidden structure poses severe problems to developers and maintainers who want to use, extend or adapt those systems. This paper advocates the use of a logic meta-language to express and extract structural relationships in class-based object-oriented systems. As validation the logic meta-language SOUL was implemented and used to construct a declarative framework that allows reasoning about the structure of Smalltalk programs. The declarative framework's usefulness is illustrated by expressing different high-level structural relationships such as those described by design patterns.},
  doi        = {10.1109/TOOLS.1998.711007},
  file       = {:Wuyts1998 - Declarative Reasoning about the Structure of Object Oriented Systems.pdf:PDF},
  keywords   = {Documentation, Data mining, Object oriented programming, Electronic mail, World Wide Web, Process design, Logic programming, Software systems, Writing, Computer languages, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/2328876.2328882,
  author     = {Hills, Mark and Klint, Paul and Vinju, Jurgen J.},
  booktitle  = {Proceedings of the Fifth Workshop on Refactoring Tools},
  title      = {Scripting a Refactoring with Rascal and Eclipse},
  year       = {2012},
  address    = {New York, NY, USA},
  pages      = {40–49},
  publisher  = {Association for Computing Machinery},
  series     = {WRT '12},
  abstract   = {To facilitate experimentation with creating new, complex refactorings, we want to reuse existing transformation and analysis code as orchestrated parts of a larger refactoring: i.e., to script refactorings. The language we use to perform this scripting must be able to deal with the diversity of languages, tools, analyses, and transformations that arise in practice. To illustrate one solution to this problem, in this paper we describe, in detail, a specific refactoring script for switching from the Visitor design pattern to the Interpreter design pattern. This script, written in the meta-programming language Rascal, and targeting an interpreter written in Java, extracts facts from the interpreter code using the Eclipse JDT, performs the needed analysis in Rascal, and then transforms the interpreter code using a combination of Rascal code and existing JDT refactorings. Using this script we illustrate how a new, real and complex refactoring can be scripted in a few hundred lines of code and within a short timeframe. We believe the key to successfully building such refactorings is the ability to pair existing tools, focused on specific languages, with general-purpose meta-programming languages.},
  doi        = {10.1145/2328876.2328882},
  file       = {:C\:/Users/Wernsen/Downloads/WRT2012.pdf:PDF},
  isbn       = {9781450315005},
  keywords   = {design patterns, refactoring scripts, refactoring tools, program transformation, meta-programming, skimmed},
  location   = {Rapperswil, Switzerland},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/2328876.2328882},
}

@InProceedings{Antoniol2003,
  author     = {Antoniol, G. and Di Penta, M. and Merlo, E.},
  booktitle  = {11th {IEEE} {International} {Workshop} on {Program} {Comprehension}, 2003.},
  title      = {{YAAB} ({Yet} another {AST} browser): using {OCL} to navigate {ASTs}},
  year       = {2003},
  month      = may,
  note       = {ISSN: 1092-8138},
  pages      = {13--22},
  abstract   = {In the last decades several tools and environments defined and introduced languages for querying, navigating and transforming abstract syntax trees. These environments were meant to support software maintenance, reengineering and program comprehension activities. Instead of introducing a new language, this paper proposes to adopt the Object Constraint Language (OCL) to express queries over an object model representing the abstract syntax tree of the code to be analyzed. OCL is part of the UML lingua franca and thus several advantages can be readily obtained. Central to the idea is to shift the analysis paradigm from a tree-based to an object-oriented paradigm, and to provide a meta-model decoupling the query language from the target language. This paper presents the current status in implementing an OCL interpreter with the ability of querying an object model representing the abstract syntax tree, as well as some interesting applications, such as extracting software metrics or computing clones.},
  doi        = {10.1109/WPC.2003.1199185},
  file       = {:Antoniol2003 - YAAB (Yet Another AST Browser)_ Using OCL to Navigate ASTs.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Navigation, Software maintenance, Unified modeling language, Documentation, Software systems, Object oriented modeling, Reverse engineering, Computer languages, Software tools, Database languages, skimmed},
  readstatus = {skimmed},
  shorttitle = {{YAAB} ({Yet} another {AST} browser)},
}

@TechReport{mccracken2006design,
  author      = {McCracken, Michael O},
  institution = {Technical report, UCSD CSE},
  title       = {The design and implementation of the lens program information framework},
  year        = {2006},
  file        = {:mccracken2006design - The Design and Implementation of the Lens Program Information Framework.pdf:PDF},
  keywords    = {skimmed},
  readstatus  = {skimmed},
}

@Article{rajagolopan2002qjbrowser,
  author     = {Rajagolopan, R and Volder, KD},
  title      = {QJBrowser: A Query-Based Approach to Explore Crosscuting Concerns},
  year       = {2002},
  file       = {:C\:/Users/Wernsen/Downloads/qjbrowser.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Aguiar2004,
  author     = {Aguiar, Ademar and David, Gabriel and Badros, Greg},
  booktitle  = {{XML}: {Aplicações} e {Tecnologias} {Associadas} ({XATA} 2004},
  title      = {{JavaML} 2.0: {Enriching} the {Markup} {Language} for {Java} {Source} {Code}},
  year       = {2004},
  abstract   = {Abstract. Although the representation of source code in plain text format is convenient for manipulation by programmers, it is not an effective format for processing by software engineering tools at an abstraction level suitable for source code analysis, reverse-engineering, or refactoring. Textual source code files require language-specific parsing to uncover program structure, a task undertaken by all compilers but by only a few software engineering tools. JavaML is an alternative and complementary XML representation of Java source code that adds structural and semantic information into source code, and is easy to manipulate, query, and transform using general purpose XML tools and techniques. This paper presents an evolved version of JavaML, dubbed JavaML 2.0, and the enhancements made to the schema and respective converters: DTD and XML Schema support, cross-linking of all program symbols, and full preservation of original formatting and comments. The application of JavaML 2.0 is illustrated with concrete examples taken from the software documentation tool that motivated the enhancements. 1},
  file       = {:Aguiar2004 - JavaML 2.0_ Enriching the Markup Language for Java Source Code.pdf:PDF;:JavaML_20_Enriching_the_Markup_Language_for_Java_S.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
  shorttitle = {{JavaML} 2.0},
}

@InProceedings{Maruyama2005,
  author     = {Maruyama, K. and Yamamoto, S.},
  booktitle  = {13th {International} {Workshop} on {Program} {Comprehension} ({IWPC}'05)},
  title      = {Design and implementation of an extensible and modifiable refactoring tool},
  year       = {2005},
  month      = may,
  note       = {ISSN: 1092-8138},
  pages      = {195--204},
  abstract   = {Refactoring is an essential and useful practice in developing and maintaining object-oriented software since it improves the design of existing code without changing its external behavior. Therefore, several refactoring tools tend to be integrated into contemporary IDEs. However, these tools represent source code as an abstract syntax tree (AST) and thus their implementations are hard to extend and modify. This paper presents Jrbx, a refactoring tool that uses a fine-grained XML representation of source code and supports stylized manipulations of the representation. Moreover, Jrbx aggressively exploits control flow graphs (CFGs) and program dependence graphs (PDGs) for both precondition checking and change creation. The use of the XML, CFG, and PDG representations makes the implementation of Jrbx more understandable and reusable, and thus facilitates tool developers creating new refactorings and modifying existing ones.},
  doi        = {10.1109/WPC.2005.17},
  file       = {:Maruyama2005 - Design and Implementation of an Extensible and Modifiable Refactoring Tool.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {XML, Software maintenance, Programming profession, Computer science, Information systems, Tree graphs, Flow graphs, Registers, Information analysis, Conferences, skimmed},
  readstatus = {skimmed},
}

@Article{Holt2006,
  author     = {Holt, Richard C. and Schürr, Andy and Sim, Susan Elliott and Winter, Andreas},
  journal    = {Science of Computer Programming},
  title      = {{GXL}: {A} graph-based standard exchange format for reengineering},
  year       = {2006},
  issn       = {0167-6423},
  month      = apr,
  number     = {2},
  pages      = {149--170},
  volume     = {60},
  abstract   = {GXL (Graph eXchange Language) is an XML-based standard exchange format for sharing data between tools. Formally, GXL represents typed, attributed, directed, ordered graphs which are extended to represent hypergraphs and hierarchical graphs. This flexible data model can be used for object-relational data and a wide variety of graphs. An advantage of GXL is that it can be used to exchange instance graphs together with their corresponding schema information in a uniform format, i.e. using a common document type specification. This paper describes GXL and shows how GXL is used to provide interoperability of graph-based tools. GXL has been ratified by reengineering and graph transformation research communities and is being considered for adoption by other communities.},
  doi        = {10.1016/j.scico.2005.10.003},
  file       = {:Holt2006 - GXL_ a Graph Based Standard Exchange Format for Reengineering.pdf:PDF},
  keywords   = {Graph exchange language, Graph-based tools, Data interoperability, Reengineering, XML, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Special {Issue} on {Software} {Analysis}, {Evolution} and, {Re}-engineering},
  shorttitle = {{GXL}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0167642305001176},
  urldate    = {2021-06-29},
}

@InProceedings{Dsr1991,
  author     = {Dsr, David Rosenblum and Rosenblum, David S. and Wolf, Alexander L.},
  booktitle  = {In {USENIX} {C}++ {Conference} {Proceedings}},
  title      = {Representing {Semantically} {Analyzed} {C}++ {Code} with {Reprise}},
  year       = {1991},
  pages      = {119--134},
  abstract   = {A prominent stumbling block in the spread of the C++ programming language has been  a lack of programming and analysis tools to aid development and maintenance of C++  systems. One way to make the job of tool developers easier and to increase the quality  of the tools they create is to factor out the common components of tools and provide  the components as easily (re)used building blocks. Those building blocks include lexical,  syntactic, and semantic analyzers, tailored database derivers, code annotators and  instrumentors, and code generators. From these building blocks, tools such as structure  browsers, data-flow analyzers, program/specification verifiers, metrics collectors, compilers,  interpreters, and the like can be built more easily and cheaply. We believe that for  C++ programming and analysis tools the most primitive building blocks are centered  around a common representation of semantically analyzed C++ code.  In this paper we describe such a representation, called Repri...},
  file       = {:Dsr1991 - Representing Semantically Analyzed C++ Code with Reprise.pdf:PDF;:C\:/Users/Wernsen/Downloads/10.1.1.46.6923.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Brichau2007,
  author     = {Brichau, Johan and De Roover, Coen and Mens, Kim},
  booktitle  = {{XXVI} {International} {Conference} of the {Chilean} {Society} of {Computer} {Science} ({SCCC}'07)},
  title      = {Open {Unification} for {Program} {Query} {Languages}},
  year       = {2007},
  month      = nov,
  note       = {ISSN: 1522-4902},
  pages      = {92--101},
  abstract   = {Logic-based programming languages are increasingly applied as program query languages which allow developers to reason about the structure and behaviour of programs. To achieve this, the queried programs are reified as logic values such that logic quantification and unification can be used effectively. However, in many cases, standard logic unification is inappropriate for program entities, forcing developers to resort to overly complex queries. In this paper, we argue that such incidental complexity can be reduced significantly by customizing the unification algorithm. We present a practical implementation approach through inter-language reflection and open unification. These techniques are at the core of the logic program query language SOUL, through which we demonstrate custom unification schemes for reasoning over Smalltalk and Java programs. Queries written in this tailored version of SOUL can exploit advanced program matching strategies without increasing the incidental complexity of the queries.},
  doi        = {10.1109/SCCC.2007.16},
  file       = {:Brichau2007 - Open Unification for Program Query Languages.pdf:PDF},
  issn       = {1522-4902},
  keywords   = {Database languages, Logic programming, Problem-solving, Computer languages, Reflection, Java, Pattern matching, Computer science, Standards development, Testing, skimmed},
  readstatus = {skimmed},
}

@Article{Appeltauer2008,
  author     = {Appeltauer, Malte and Kniesel, Günter},
  journal    = {Electronic Notes in Theoretical Computer Science},
  title      = {Towards {Concrete} {Syntax} {Patterns} for {Logic}-based {Transformation} {Rules}},
  year       = {2008},
  issn       = {1571-0661},
  month      = nov,
  pages      = {113--132},
  volume     = {219},
  abstract   = {Logic meta-programming in Prolog is a powerful way to express program analysis and transformation. However, its use can be difficult and error-prone because it requires programmers to know the meta-level representation of the analysed language and to think and express their analyses in terms of this low-level representation. In addition, the backtracking-based evaluation strategy of Prolog may lead to subtle semantic problems when used to express transformations of a logic database. In this paper, we propose an alternative approach, GenTL, a generic transformation language that combines logic-based Conditional Transformations (CTs) and concrete syntax patterns. This combination addresses the above problems while still offering the full expressive power of logic meta-programming. Compared to approaches based on other formalisms, the design of GenTL offers advantages in terms of composability and easy transformation interference analysis.},
  doi        = {10.1016/j.entcs.2008.10.038},
  file       = {:Appeltauer2008 - Towards Concrete Syntax Patterns for Logic Based Transformation Rules.pdf:PDF},
  keywords   = {Generic transformation rules, concrete syntax patterns, program analysis, GenTL, conditional transformations, logic meta-programming, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Proceedings of the {Eighth} {International} {Workshop} on {Rule} {Based} {Programming} ({RULE} 2007)},
  url        = {https://www.sciencedirect.com/science/article/pii/S1571066108004325},
  urldate    = {2021-06-29},
}

@InProceedings{Falconer2007,
  author     = {Falconer, Henry and Kelly, Paul H. J. and Ingram, David M. and Mellor, Michael R. and Field, Tony and Beckmann, Olav},
  booktitle  = {Compiler {Construction}},
  title      = {A {Declarative} {Framework} for {Analysis} and {Optimization}},
  year       = {2007},
  address    = {Berlin, Heidelberg},
  editor     = {Krishnamurthi, Shriram and Odersky, Martin},
  pages      = {218--232},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {DeepWeaver-1 is a tool supporting cross-cutting program analysis and transformation components, called “weaves”. Like an aspect, a DeepWeaver weave consists of a query part, and a part which may modify code. DeepWeaver’s query language is based on Prolog, and provides access to data-flow and control-flow reachability analyses. DeepWeaver provides a declarative way to access the internal structure of methods, and supports cross-cutting weaves which operate on code blocks from different parts of the codebase simultaneously. DeepWeaver operates at the level of bytecode, but offers predicates to extract structured control flow constructs. This paper motivates the design, and demonstrates some of its power, using a sequence of examples including performance profiling and domain-specific performance optimisations for database access and remote method invocation.},
  doi        = {10.1007/978-3-540-71229-9_15},
  file       = {:Falconer2007 - A Declarative Framework for Analysis and Optimization.pdf:PDF},
  isbn       = {9783540712299},
  keywords   = {Query Language , Query Optimisation , Query Execution , Parameter List , Abstract Syntax Tree , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@InProceedings{Roover2011,
  author     = {Roover, Coen De},
  booktitle  = {2011 27th {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM})},
  title      = {A logic meta-programming foundation for example-driven pattern detection in object-oriented programs},
  year       = {2011},
  month      = sep,
  note       = {ISSN: 1063-6773},
  pages      = {556--561},
  abstract   = {This paper summarizes the doctoral dissertation in which we introduced an example-driven approach to pattern detection. This approach enables specifying pattern characteristics in a familiar language: through a code excerpt that corresponds to their prototypical implementation. Such excerpts are matched against the program under investigation according to various matching strategies that vary in leniency. Each match is quantified by the extent to which it exhibits the exemplified characteristics. The smaller this extent, the more likely the match is a false positive - thus establishing a ranking which facilitates assessing a large amount of matches. Unique to the matching process is that it incorporates whole-program analyses in its comparison of individual program elements. This way, we are able to recall implicit implementation variants (i.e., those implied by the semantics of the programming language) of a pattern of which only the prototypical implementation has been exemplified.},
  doi        = {10.1109/ICSM.2011.6080830},
  file       = {:Roover2011 - A Logic Meta Programming Foundation for Example Driven Pattern Detection in Object Oriented Programs.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {Observers, Syntactics, Semantics, Encoding, Compounds, Concrete, Dispatching, skimmed, relevant},
  readstatus = {skimmed},
  relevance  = {relevant},
}

@InProceedings{10.1145/1140335.1140363,
  author     = {Volanschi, Nic},
  booktitle  = {Proceedings of the 8th ACM SIGPLAN International Conference on Principles and Practice of Declarative Programming},
  title      = {Condate: A Proto-Language at the Confluence between Checking and Compiling},
  year       = {2006},
  address    = {New York, NY, USA},
  pages      = {225–236},
  publisher  = {Association for Computing Machinery},
  series     = {PPDP '06},
  abstract   = {Recent years have seen the advent of many different tools for program checking against user-defined properties. Despite this encouraging trend, checking technology is used still marginally today, and only on an occasional basis. Existing checkers are standalone tools, associated --- correctly or not --- with low efficiency, and duplicating much work already done in the compiler. We believe that, as a complement to more precise verifiers, the next generation of compilers should integrate some amount of user-defined checks that can be performed efficientlyCombining checking and compiling enables a pervasive propagation of checking technology and continuous use of checking throughout development. It also enables cross-fertilization between the two passes, resulting in increased expressiveness, precision, and even in improved complexity of the checking algorithm.We illustrate this integrated approach with a full-fledged checking compiler for C, extensible through Condate. Condate is a declarative language for expressing simple user-defined program properties to be checked in addition to normal compilation. Condate mixes in a very concise form syntactic, semantic, control flow, and data flow properties. These properties are defined as a new class of regular path expressions over the control-flow graph, checkable in linear time and covering many useful checks.We demonstrate the viability of the integrated approach based on Condate by applying it to successfully check some parts of the Linux kernel.},
  doi        = {10.1145/1140335.1140363},
  file       = {:C\:/Users/Wernsen/Downloads/10.1.1.219.1709.pdf:PDF},
  isbn       = {1595933883},
  keywords   = {customization, program checking, declarative languages, compilers, skimmed},
  location   = {Venice, Italy},
  numpages   = {12},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/1140335.1140363},
}

@InProceedings{Hajs2010ErlangSQ,
  author     = {Lilla Haj{\'o}s and M. T{\'o}th and L. L{\"o}vei},
  title      = {Erlang Semantic Query Language},
  year       = {2010},
  file       = {:C\:/Users/Wernsen/Downloads/ICAI2010-vol2-pp165-172.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Pinzger2008,
  author     = {Pinzger, Martin and Grafenhain, Katja and Knab, Patrick and Gall, Harald C.},
  booktitle  = {2008 16th {IEEE} {International} {Conference} on {Program} {Comprehension}},
  title      = {A {Tool} for {Visual} {Understanding} of {Source} {Code} {Dependencies}},
  year       = {2008},
  month      = jun,
  note       = {ISSN: 1092-8138},
  pages      = {254--259},
  abstract   = {Many program comprehension tools use graphs to visualize and analyze source code. The main issue is that existing approaches create graphs overloaded with too much information. Graphs contain hundreds of nodes and even more edges that cross each other. Understanding these graphs and using them for a given program comprehension task is tedious, and in the worst case developers stop using the tools. In this paper we present DA4Java, a graph-based approach for visualizing and analyzing static dependencies between Java source code entities. The main contribution of DA4Java is a set of features to incrementally compose graphs and remove irrelevant nodes and edges from graphs. This leads to graphs that contain significantly fewer nodes and edges and need less effort to understand.},
  doi        = {10.1109/ICPC.2008.23},
  file       = {:Pinzger2008 - A Tool for Visual Understanding of Source Code Dependencies.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Visualization, Java, Information filtering, Information filters, Packaging, Software systems, Informatics, Data mining, Costs, Software maintenance, Incremental Source Code Analysis, Program Comprehension, Software Visualization, Source Code Dependencies, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Witte2007,
  author     = {Witte, René and Zhang, Yonggang and Rilling, Jürgen},
  booktitle  = {The {Semantic} {Web}: {Research} and {Applications}},
  title      = {Empowering {Software} {Maintainers} with {Semantic} {Web} {Technologies}},
  year       = {2007},
  address    = {Berlin, Heidelberg},
  editor     = {Franconi, Enrico and Kifer, Michael and May, Wolfgang},
  pages      = {37--52},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Software maintainers routinely have to deal with a multitude of artifacts, like source code or documents, which often end up disconnected, due to their different representations and the size and complexity of legacy systems. One of the main challenges in software maintenance is to establish and maintain the semantic connections among all the different artifacts. In this paper, we show how Semantic Web technologies can deliver a unified representation to explore, query and reason about a multitude of software artifacts. A novel feature is the automatic integration of two important types of software maintenance artifacts, source code and documents, by populating their corresponding sub-ontologies through code analysis and text mining. We demonstrate how the resulting “Software Semantic Web” can support typical maintenance tasks through ontology queries and Description Logic reasoning, such as security analysis, architectural evolution, and traceability recovery between code and documents.},
  doi        = {10.1007/978-3-540-72667-8_5},
  file       = {:Witte2007 - Empowering Software Maintainers with Semantic Web Technologies.pdf:PDF},
  isbn       = {9783540726678},
  keywords   = {Software Maintenance , Ontology Population , Text Mining , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@InProceedings{Back2003MJA,
  author     = {Godmar Back and D. Engler},
  title      = {MJ - A System for Constructing Bug-Finding Analyses for Java},
  year       = {2003},
  file       = {:Back2003MJA - MJ a System for Constructing Bug Finding Analyses for Java.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Paul1992SCRUPLEAR,
  author    = {Santanu Paul},
  booktitle = {CASCON},
  title     = {SCRUPLE: a reengineer's tool for source code search},
  year      = {1992},
}

@InCollection{Menshchikov2019,
  author     = {Menshchikov, Maxim},
  title      = {Equid—{A} {Static} {Analysis} {Framework} for {Industrial} {Applications}},
  year       = {2019},
  isbn       = {9783030242886},
  month      = jun,
  pages      = {677--692},
  abstract   = {The rise of the software engineering industry sparkled the research on static analyzers in both academia and industry. Academic tools historically have an exhaustive feature set but don’t easily apply to industrial applications, and industrial verifiers are still very limited. The Equid project, which loosely stands for “Engine for performing queries on unified intermediate representations of program and domain models” is an attempt to fill the gap between theory and practice by building a language-agnostic analyzer in close contact with development and security community. In this introductory paper we set project goals, reveal motivation and describe code processing stages, such as preprocessing, translation to project’s own intermediate codes, virtual machine execution, constraint solving, all done to make static and interactive contract violation checks easier, more precisive yet informative. The project is compared to other analyzers. We believe that such a framework can draw attention to industrial uses clearly missed by verification communities and help shape a vision of universal static analyzer architectures.},
  doi        = {10.1007/978-3-030-24289-3_50},
  file       = {:Menshchikov2019 - Equid—A Static Analysis Framework for Industrial Applications.pdf:PDF;:10.1007_978-3-030-24289-3(1).pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Tappolet2010,
  author     = {Tappolet, Jonas and Kiefer, Christoph and Bernstein, Abraham},
  journal    = {Web Semantics: Science, Services and Agents on the World Wide Web},
  title      = {Semantic web enabled software analysis},
  year       = {2010},
  issn       = {1570-8268},
  month      = jul,
  number     = {2},
  pages      = {225--240},
  volume     = {8},
  abstract   = {One of the most important decisions researchers face when analyzing software systems is the choice of a proper data analysis/exchange format. In this paper, we present EvoOnt, a set of software ontologies and data exchange formats based on OWL. EvoOnt models software design, release history information, and bug-tracking meta-data. Since OWL describes the semantics of the data, EvoOnt (1) is easily extendible, (2) can be processed with many existing tools, and (3) allows to derive assertions through its inherent Description Logic reasoning capabilities. The contribution of this paper is that it introduces a novel software evolution ontology that vastly simplifies typical software evolution analysis tasks. In detail, we show the usefulness of EvoOnt by repeating selected software evolution and analysis experiments from the 2004–2007 Mining Software Repositories Workshops (MSR). We demonstrate that if the data used for analysis were available in EvoOnt then the analyses in 75\% of the papers at MSR could be reduced to one or at most two simple queries within off-the-shelf SPARQL tools. In addition, we present how the inherent capabilities of the Semantic Web have the potential of enabling new tasks that have not yet been addressed by software evolution researchers, e.g., due to the complexities of the data integration.},
  doi        = {10.1016/j.websem.2010.04.009},
  file       = {:Tappolet2010 - Semantic Web Enabled Software Analysis.pdf:PDF},
  keywords   = {Software comprehension framework, Software release similarity, Bug prediction, Software evolution, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Bridging the {Gap}—{Data} {Mining} and {Social} {Network} {Analysis} for {Integrating} {Semantic} {Web} and {Web} 2.0},
  url        = {https://www.sciencedirect.com/science/article/pii/S1570826810000338},
  urldate    = {2021-07-01},
}

@Article{dutko2011relational,
  author     = {Dutko, Adam M},
  title      = {The Relational Database: a New Static Analysis Tool?},
  year       = {2011},
  file       = {:C\:/Users/Wernsen/Downloads/csu1313678735.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{10.1145/996893.996868,
  author     = {Liu, Yanhong A. and Rothamel, Tom and Yu, Fuxiang and Stoller, Scott D. and Hu, Nanjun},
  journal    = {SIGPLAN Not.},
  title      = {Parametric Regular Path Queries},
  year       = {2004},
  issn       = {0362-1340},
  month      = jun,
  number     = {6},
  pages      = {219–230},
  volume     = {39},
  abstract   = {Regular path queries are a way of declaratively expressing queries on graphs as regular-expression-like patterns that are matched against paths in the graph. There are two kinds of queries: existential queries, which specify properties about individual paths, and universal queries, which specify properties about all paths. They provide a simple and convenient framework for expressing program analyses as queries on graph representations of programs, for expressing verification (model-checking) problems as queries on transition systems, for querying semi-structured data, etc. Parametric regular path queries extend the patterns with variables, called parameters, which significantly increase the expressiveness by allowing additional information along single or multiple paths to be captured and relate.This paper shows how a variety of program analysis and model-checking problems can be expressed easily and succinctly using parametric regular path queries. The paper describes the specification, design, analysis, and implementation of algorithms and data structures for efficiently solving existential and universal parametric regular path queries. Major contributions include the first complete algorithms and data structures for directly and efficiently solving existential and universal parametric regular path queries, detailed complexity analysis of the algorithms, detailed analytical and experimental performance comparison of variations of the algorithms and data structures, and investigation of efficiency tradeoffs between different formulations of queries.},
  address    = {New York, NY, USA},
  doi        = {10.1145/996893.996868},
  file       = {:C\:/Users/Wernsen/Downloads/ParamRPQ-PLDI04(1).pdf:PDF},
  issue_date = {May 2004},
  keywords   = {data tructures, algorithms, regular path queries, regular expressions, memoization, program analysis, optimization, graph query languages, precomputation, model checking, skimmed},
  numpages   = {12},
  publisher  = {Association for Computing Machinery},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/996893.996868},
}

@InProceedings{avgustinov_et_al:LIPIcs:2016:6096,
  author     = {Pavel Avgustinov and Oege de Moor and Michael Peyton Jones and Max Sch{\"a}fer},
  booktitle  = {30th European Conference on Object-Oriented Programming (ECOOP 2016)},
  title      = {{QL: Object-oriented Queries on Relational Data}},
  year       = {2016},
  address    = {Dagstuhl, Germany},
  editor     = {Shriram Krishnamurthi and Benjamin S. Lerner},
  pages      = {2:1--2:25},
  publisher  = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  series     = {Leibniz International Proceedings in Informatics (LIPIcs)},
  volume     = {56},
  annote     = {Keywords: Object orientation, Datalog, query languages, prescriptive typing},
  doi        = {10.4230/LIPIcs.ECOOP.2016.2},
  file       = {:avgustinov_et_al_LIPIcs_2016_6096 - QL_ Object Oriented Queries on Relational Data.pdf:PDF},
  isbn       = {978-3-95977-014-9},
  issn       = {1868-8969},
  keywords   = {skimmed},
  readstatus = {skimmed},
  url        = {http://drops.dagstuhl.de/opus/volltexte/2016/6096},
  urn        = {urn:nbn:de:0030-drops-60968},
}

@InProceedings{Volder1998TypeOL,
  author     = {K. Volder},
  title      = {Type Oriented Logic Meta Programming for Java},
  year       = {1998},
  file       = {:C\:/Users/Wernsen/Downloads/10.1.1.29.7082.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InCollection{Ebert2010,
  author     = {Ebert, Jürgen and Bildhauer, Daniel},
  publisher  = {Springer},
  title      = {Reverse {Engineering} {Using} {Graph} {Queries}},
  year       = {2010},
  address    = {Berlin, Heidelberg},
  editor     = {Engels, Gregor and Lewerentz, Claus and Schäfer, Wilhelm and Schürr, Andy and Westfechtel, Bernhard},
  isbn       = {9783642173226},
  pages      = {335--362},
  series     = {Lecture {Notes} in {Computer} {Science}},
  abstract   = {Software Reverse Engineering is the process of extracting (usually more abstract) information from software artifacts. Graph-based engineering tools work on fact repositories that keep all artifacts as graphs. Hence, information extraction can be viewed as querying this repository. This paper describes the graph query language GReQL and its use in reverse engineering tools.GReQL is an expression language based on set theory and predicate logics including regular path expressions (RPEs) as first class values. The GReQL evaluator is described in some detail with an emphasis on the efficient evaluation of RPEs for reachability and path-finding queries. Applications for reverse engineering Java software are added as sample use cases.},
  doi        = {10.1007/978-3-642-17322-6_15},
  file       = {:Ebert2010 - Reverse Engineering Using Graph Queries.pdf:PDF;:Reverse_Engineering_Using_Graph_Queries.pdf:PDF},
  keywords   = {Regular Expression , Query Language , Graph Element , Graph Query , Graph Schema , skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1007/978-3-642-17322-6_15},
  urldate    = {2021-07-01},
}

@Article{Seifert2008StaticSC,
  author     = {Mirko Seifert and Roland Samlaus},
  journal    = {Electron. Commun. Eur. Assoc. Softw. Sci. Technol.},
  title      = {Static Source Code Analysis using OCL},
  year       = {2008},
  volume     = {15},
  file       = {:Seifert2008StaticSC - Static Source Code Analysis Using OCL.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Article{Huang2005,
  author     = {Huang, Heyuan and Zhang, Shensheng and Cao, Jian and Duan, Yonghong},
  journal    = {Journal of Systems and Software},
  title      = {A practical pattern recovery approach based on both structural and behavioral analysis},
  year       = {2005},
  issn       = {0164-1212},
  month      = feb,
  number     = {1},
  pages      = {69--87},
  volume     = {75},
  abstract   = {While the merit of using design patterns is clear for forward engineering, we could also benefit from design pattern recovery in program understanding and reverse engineering. In this paper, we present a practical approach to enlarge the recoverable scope and improve precision ratio of pattern recovery. To specify both structural aspect and behavioral aspect of design patterns, we introduce traditional predicate logic combined with Allen's interval-based temporal logic as our theory foundation. The formal specifications could be conveniently converted into Prolog representations to support pattern recovery. To illustrate how to specify and recover design patterns in our approach, we take one example for each category of design patterns. Moreover, we give a taxonomy of design patterns based on the analysis in our approach to show its applicable scope. To validate our approach, we have developed a tool named PRAssistor and analyzed two well-known open source frameworks. The experiment results show that most of the patterns addressed in our taxonomy have been recovered. Besides larger recoverable scope, the recovery precision of our approach is much higher than others. Furthermore, we consider that our approach and tool could be promisingly extended to support “Debug at Design Level” and “Pattern-Driven Refactoring”.},
  doi        = {10.1016/j.jss.2003.11.018},
  file       = {:Huang2005 - A Practical Pattern Recovery Approach Based on Both Structural and Behavioral Analysis.pdf:PDF},
  keywords   = {Design pattern, Pattern recovery, Reverse engineering, skimmed},
  language   = {en},
  readstatus = {skimmed},
  series     = {Software {Engineering} {Education} and {Training}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0164121203003236},
  urldate    = {2021-07-01},
}

@Article{Strein2007,
  author     = {Strein, Dennis and Lincke, Rüdiger and Lundberg, Jonas and Löwe, Welf},
  journal    = {IEEE Transactions on Software Engineering},
  title      = {An {Extensible} {Meta}-{Model} for {Program} {Analysis}},
  year       = {2007},
  issn       = {1939-3520},
  month      = sep,
  number     = {9},
  pages      = {592--607},
  volume     = {33},
  abstract   = {Software maintenance tools for program analysis and refactoring rely on a metamodel capturing the relevant properties of programs. However, what is considered relevant may change when the tools are extended with new analyses, refactorings, and new programming languages. This paper proposes a language independent metamodel and an architecture to construct instances thereof, which is extensible for new analyses, refactorings, and new front-ends of programming languages. Due to the loose coupling between analysis, refactoring, and front-end components, new components can be added independently and reuse existing ones. Two maintenance tools implementing the metamodel and the architecture, VIZZANALYZER and X-DEVELOP, serve as proof of concept.},
  doi        = {10.1109/TSE.2007.70710},
  file       = {:Strein2007 - An Extensible Meta Model for Program Analysis.pdf:PDF},
  keywords   = {Information analysis, Independent component analysis, Data mining, Computer languages, Computer architecture, Software systems, Software maintenance, Costs, Software tools, Computer interfaces, skimmed},
  readstatus = {skimmed},
}

@Article{Badros2000,
  author     = {Badros, Greg J},
  journal    = {Computer Networks},
  title      = {{JavaML}: a markup language for {Java} source code},
  year       = {2000},
  issn       = {1389-1286},
  month      = jun,
  number     = {1},
  pages      = {159--177},
  volume     = {33},
  abstract   = {The classical plain-text representation of source code is convenient for programmers but requires parsing to uncover the deep structure of the program. While sophisticated software tools parse source code to gain access to the program's structure, many lightweight programming aids such as grep rely instead on only the lexical structure of source code. I describe a new XML application that provides an alternative representation of Java source code. This XML-based representation, called JavaML, is more natural for tools and permits easy specification of numerous software-engineering analyses by leveraging the abundance of XML tools and techniques. A robust converter built with the Jikes Java compiler framework translates from the classical Java source code representation to JavaML, and an XSLT stylesheet converts from JavaML back into the classical textual form.},
  doi        = {10.1016/S1389-1286(00)00037-2},
  file       = {:Badros2000 - JavaML_ a Markup Language for Java Source Code.pdf:PDF},
  keywords   = {Java, XML, Abstract syntax tree representation, Software-engineering analysis, Jikes compiler, skimmed},
  language   = {en},
  readstatus = {skimmed},
  shorttitle = {{JavaML}},
  url        = {https://www.sciencedirect.com/science/article/pii/S1389128600000372},
  urldate    = {2021-07-01},
}

@InProceedings{Heuzeroth2003,
  author     = {Heuzeroth, D. and Holl, T. and Hogstrom, G. and Lowe, W.},
  booktitle  = {11th {IEEE} {International} {Workshop} on {Program} {Comprehension}, 2003.},
  title      = {Automatic design pattern detection},
  year       = {2003},
  month      = may,
  note       = {ISSN: 1092-8138},
  pages      = {94--103},
  abstract   = {We detect design patterns in legacy code combining static and dynamic analyses. The analyses do not depend on coding or naming conventions. We classify potential pattern instances according to the evidence our analyses provide. We discuss our approach for the observer, composite, mediator, chain of responsibility and visitor patterns. Our Java analysis tool analyzes Java programs. We evaluate our approach by applying the tool on itself and on the Java SwingSetExample using the Swing library.},
  doi        = {10.1109/WPC.2003.1199193},
  file       = {:Heuzeroth2003 - Automatic Design Pattern Detection.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Connectors, Pattern analysis, Java, Computer architecture, Software systems, Software libraries, Software performance, Software tools, Performance analysis, Scattering, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Heuzeroth2003a,
  author     = {Heuzeroth, D. and Mandel, S. and Lowe, W.},
  booktitle  = {18th {IEEE} {International} {Conference} on {Automated} {Software} {Engineering}, 2003. {Proceedings}.},
  title      = {Generating design pattern detectors from pattern specifications},
  year       = {2003},
  month      = oct,
  note       = {ISSN: 1938-4300},
  pages      = {245--248},
  abstract   = {We present our approach to support program understanding by a tool that generates static and dynamic analysis algorithms from design pattern specifications to detect design patterns in legacy code. We therefore specify the static and dynamic aspects of patterns as predicates, and represent legacy code by predicates that encode its attributed abstract syntax trees. Given these representations, the static analysis is performed on the legacy code representation as a query derived from the specification of the static pattern aspects. It provides us with pattern candidates in the legacy code. The dynamic specification represents state sequences expected when using a pattern. We monitor the execution of the candidates and check their conformance to this expectation. We demonstrate our approach and evaluate our tool by detecting instances of the observer, composite and decorator patterns in Java code using Prolog to define predicates and queries.},
  doi        = {10.1109/ASE.2003.1240313},
  file       = {:Heuzeroth2003a - Generating Design Pattern Detectors from Pattern Specifications.pdf:PDF},
  issn       = {1938-4300},
  keywords   = {Detectors, Pattern analysis, Algorithm design and analysis, Performance analysis, Java, Computer architecture, Software systems, Protocols, Concrete, Heuristic algorithms, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Balanyi2003,
  author     = {Balanyi, Z. and Ferenc, R.},
  booktitle  = {International {Conference} on {Software} {Maintenance}, 2003. {ICSM} 2003. {Proceedings}.},
  title      = {Mining design patterns from {C}++ source code},
  year       = {2003},
  month      = sep,
  note       = {ISSN: 1063-6773},
  pages      = {305--314},
  abstract   = {Design patterns are micro architectures that have proved to be reliable, easy-to implement and robust. There is a need in science and industry for recognizing these patterns. We present a new method for discovering design patterns in the source code. This method provides a precise specification of how the patterns work by describing basic structural information like inheritance, composition, aggregation and association, and as an indispensable part, by defining call delegation, object creation and operation overriding. We introduce a new XML-based language, the Design Pattern Markup Language (DPML), which provides an easy way for the users to modify pattern descriptions to suit their needs, or even to define their own patterns or just classes in certain relations they wish to find. We tested our method on four open-source systems, and found it effective in discovering design pattern instances.},
  doi        = {10.1109/ICSM.2003.1235436},
  file       = {:Balanyi2003 - Mining Design Patterns from C++ Source Code.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {Pattern recognition, Software systems, Robustness, Unified modeling language, Software measurement, Artificial intelligence, Design methodology, Markup languages, System testing, Open source software, skimmed},
  readstatus = {skimmed},
}

@Article{Bellay1998,
  author     = {Bellay, Berndt and Gall, Harald},
  journal    = {Journal of Software Maintenance: Research and Practice},
  title      = {An evaluation of reverse engineering tool capabilities},
  year       = {1998},
  issn       = {1096-908X},
  number     = {5},
  pages      = {305--331},
  volume     = {10},
  abstract   = {Reverse engineering tools support software engineers in the process of analysing and understanding complex software systems during maintenance, re-engineering or re-architecturing. The functionality of such tools varies from editing and browsing capabilities to the generation of textual and graphical reports. There are several commercial reverse engineering tools on the market providing different capabilities and supporting specific source code languages. We evaluated four reverse engineering tools that analyse C source code: Refine/C, Imagix 4D, SNiFF+ and Rigi. We investigated the capabilities of these tools by applying them to a real-world embedded software system as a case study. We identified benefits and shortcomings of these tools and assessed their applicability for embedded software systems, their usability and their extensibility. © 1998 John Wiley \& Sons, Ltd.},
  doi        = {10.1002/(SICI)1096-908X(199809/10)10:5<305::AID-SMR175>3.0.CO;2-7},
  file       = {:Bellay1998 - An Evaluation of Reverse Engineering Tool Capabilities.pdf:PDF;:10.1.1.94.7956.pdf:PDF},
  keywords   = {tool evaluation, tool assessment criteria, tool case study, reverse engineering tools, call graphs, source code parsing, skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291096-908X%28199809/10%2910%3A5%3C305%3A%3AAID-SMR175%3E3.0.CO%3B2-7},
  urldate    = {2021-07-02},
}

@InProceedings{Armstrong1998,
  author     = {Armstrong, M.N. and Trudeau, C.},
  booktitle  = {Proceedings {Fifth} {Working} {Conference} on {Reverse} {Engineering} ({Cat}. {No}.{98TB100261})},
  title      = {Evaluating architectural extractors},
  year       = {1998},
  month      = oct,
  pages      = {30--39},
  abstract   = {One of the goals of reverse engineering a software system is to extract an architectural design from the source code. This paper compares a selection of tools available to perform this architectural recovery. The following tools are examined: Rigi (Muller, 1996), the Dali workbench (Kazman and Carriere, 1998), the Software Bookshelf (PBS) (Finnigan et al., 1997), CIA (Chen et al., 1990) and SNiFF+. This comparison is based on the abilities of the tools to perform data extraction, classification, and visualization. Of the tools evaluated, the Software Bookshelf and the Dali workbench were found to be the most suitable for architectural recovery.},
  doi        = {10.1109/WCRE.1998.723173},
  file       = {:Armstrong1998 - Evaluating Architectural Extractors.pdf:PDF},
  keywords   = {Data mining, Reverse engineering, Data visualization, Software tools, Software systems, Trademarks, Software engineering, Software prototyping, User interfaces, Relational databases, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/302405.302622,
  author     = {Keller, Rudolf K. and Schauer, Reinhard and Robitaille, S\'{e}bastien and Pag\'{e}, Patrick},
  booktitle  = {Proceedings of the 21st International Conference on Software Engineering},
  title      = {Pattern-Based Reverse-Engineering of Design Components},
  year       = {1999},
  address    = {New York, NY, USA},
  pages      = {226–235},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '99},
  doi        = {10.1145/302405.302622},
  file       = {:10.1145_302405.302622 - Pattern Based Reverse Engineering of Design Components.pdf:PDF},
  isbn       = {1581130740},
  keywords   = {design pattern, visualization, reverse-engineering, tool support, design recovery, object-oriented design, design component, skimmed},
  location   = {Los Angeles, California, USA},
  numpages   = {10},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/302405.302622},
}

@Article{Mendelzon1997,
  author     = {Mendelzon, Alberto and Sametinger, Johannes},
  journal    = {Software - Concepts and Tools},
  title      = {Reverse {Engineering} by {Visualizing} and {Querying}},
  year       = {1997},
  month      = dec,
  volume     = {16},
  abstract   = {. The automatic extraction of high-level structural information from code is important for both software maintenance and reuse. Instead of using specialpurpose tools, we explore the use of a general-purpose data visualization system called Hy+ for querying and visualizing information about object-oriented software systems. Hy+ supports visualization and visual querying of arbitrary graph-like databases. We store information about software systems in a database and use Hy+ for analyzing the source code and visualizing various relationships. In this paper we demonstrate the use of Hy+ for evaluating software metrics, verifying constraints, and identifying design patterns. Software metrics can be used to find components with low reusability or components that are hard to understand. Checking the source code against constraints can help bring design flaws to light, eliminate sources of errors, and guarantee consistent style. Identifying design patterns in a software system can reveal desig...},
  file       = {:Mendelzon1997 - Reverse Engineering by Visualizing and Querying.pdf:PDF;:Mendelzon1997 - Reverse Engineering by Visualizing and Querying (1).pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Ciupke1999,
  author     = {Ciupke, O.},
  booktitle  = {Proceedings of {Technology} of {Object}-{Oriented} {Languages} and {Systems} - {TOOLS} 30 ({Cat}. {No}.{PR00278})},
  title      = {Automatic detection of design problems in object-oriented reengineering},
  year       = {1999},
  month      = aug,
  pages      = {18--32},
  abstract   = {The evolution of software systems over many years often leads to unnecessarily complex and inflexible designs which in turn lead to a huge amount of effort for enhancements and maintenance. Thus, the reengineering of object oriented software becomes more and more important as the number, age and size of such legacy systems grow. A key issue during reengineering is the identification and location of design problems which prevent the efficient further development of a system. Up to now this problem area has not been sufficiently supported, either by methods, or by tools. We present a technique for analyzing legacy code, specifying frequent design problems as queries and locating the occurrences of these problems in a model derived from source code. We present our experiences with a tool set which we implemented to support this task by automatically analyzing a given system and detecting the specified problems. We applied our tools to check violations of a number of well known design rules in existing source code taken from several case studies, both from industrial and academic fields. These experiments showed that the task of problem detection in reengineering can be automated to a large degree, and that the technique presented can be efficiently applied to real world code.},
  doi        = {10.1109/TOOLS.1999.787532},
  file       = {:Ciupke1999 - Automatic Detection of Design Problems in Object Oriented Reengineering.pdf:PDF},
  keywords   = {Software systems, Object oriented modeling, Software tools, Data mining, Documentation, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Fahmy2001,
  author     = {Fahmy, Hoda and Holt, R.C. and Cordy, J.},
  title      = {Wins and {Losses} of {Algebraic} {Transformations} of {Software} {Architectures}},
  year       = {2001},
  month      = dec,
  pages      = {51--60},
  abstract   = {In order to understand, analyze and modify software, we commonly examine and manipulate its architecture. For example, we may want to examine the architecture at different levels of abstraction. We can view such manipulations as architectural transformations, and more specifically, as graph transformations. We evaluate relational algebra as a way of specifying and automating the architectural transformations. Specifically, we examine Grok, a relational calculator that is part of the PBS toolkit. We show that relational algebra is practical in that we are able to specify many of the transformations commonly occurring during software maintenance and, using a tool like Grok, we are able to manipulate, quite efficiently, large software graphs; this is a win. However, this approach is not well suited to express some types of transforms involving patterns of edges and nodes; this is a loss. By means of a set of examples, the paper makes clear when the approach wins and when it loses.},
  doi        = {10.1109/ASE.2001.989790},
  file       = {:Fahmy2001 - Wins and Losses of Algebraic Transformations of Software Architectures.pdf:PDF;:10.1.1.19.7678.pdf:PDF},
  isbn       = {9780769514260},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Behnke1998,
  author     = {Behnke, Ralf and Berghammer, Rudolf and Meyer, Erich and Schneider, Peter},
  booktitle  = {Fundamental {Approaches} to {Software} {Engineering}},
  title      = {{RELVIEW} — {A} system for calculating with relations and relational programming},
  year       = {1998},
  address    = {Berlin, Heidelberg},
  editor     = {Astesiano, Egidio},
  pages      = {318--321},
  publisher  = {Springer},
  series     = {Lecture {Notes} in {Computer} {Science}},
  doi        = {10.1007/BFb0053599},
  file       = {:Behnke1998 - RELVIEW — a System for Calculating with Relations and Relational Programming.pdf:PDF},
  isbn       = {9783540697237},
  keywords   = {Relational Algebra , Relational Domain , Graph Editor , Relational Program , Concrete Relation , skimmed},
  language   = {en},
  readstatus = {skimmed},
}

@InProceedings{Wu2002,
  author     = {Wu, Jingwei and Hassan, A.E. and Holt, R.C.},
  booktitle  = {Proceedings 10th {International} {Workshop} on {Program} {Comprehension}},
  title      = {Using graph patterns to extract scenarios},
  year       = {2002},
  month      = jun,
  note       = {ISSN: 1092-8138},
  pages      = {239--247},
  abstract   = {Scenario diagrams are useful for helping software developers to understand the interactions among the components of a software system. We present a semi-automatic approach to extracting scenarios from the implementation of a software system. In our approach, the source code of a software system is represented as a graph and scenarios are specified as graph patterns. A relational calculator, Grok, is extended to support graph pattern matching. Grok, as extended, is used in our analysis of the Nautilus open source file manager. Multiple scenarios are extracted and analyzed. These scenarios have helped us to analyze Nautilus's architecture.},
  doi        = {10.1109/WPC.2002.1021345},
  file       = {:Wu2002 - Using Graph Patterns to Extract Scenarios.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Software systems, Software architecture, Data mining, Computer architecture, Pattern matching, Utility programs, Computer science, Reverse engineering, Documentation, Systems engineering and theory, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Kullbach1999,
  author     = {Kullbach, B. and Winter, A.},
  booktitle  = {Proceedings of the {Third} {European} {Conference} on {Software} {Maintenance} and {Reengineering} ({Cat}. {No}. {PR00090})},
  title      = {Querying as an enabling technology in software reengineering},
  year       = {1999},
  month      = mar,
  pages      = {42--50},
  abstract   = {It is argued that different kinds of reengineering technologies can be based on querying. Several reengineering technologies are presented as being integrated into a technically oriented reengineering taxonomy. The usefulness of querying is pointed out with respect to these reengineering technologies. To impose querying as a base technology in reengineering, examples are given with respect to the EER/GRAL approach to conceptual modeling and implementation. This approach is presented, together with GReQL as its query part. The different reengineering technologies are finally reviewed in the context of the GReQL query facility.},
  doi        = {10.1109/CSMR.1999.756681},
  file       = {:Kullbach1999 - Querying As an Enabling Technology in Software Reengineering.pdf:PDF},
  keywords   = {Taxonomy, Software performance, Software systems, Writing, Electrical capacitance tomography, Q measurement, Application software, skimmed},
  readstatus = {skimmed},
}

@Misc{Stoerrle2009,
  author     = {Störrle, Harald},
  title      = {A logical model query interface *},
  year       = {2009},
  abstract   = {This paper presents the Logical Query Facility (LQF), a high level programming interface to query UML models. LQF is a Prolog library built on top of the Model Manipulation Toolkit (MoMaT, cf. [8]). It provides a set of versatile predicates that reflects the notions modelers use when reasoning about their models which makes it easy to formulate queries in a natural way. In order to demonstrate the capabilities of LQF in comparison to OCL, we have implemented it as a plug in to the popular MagicDraw UML CASE tool [3], and evaluated LQF with a benchmark suite of frequent model queries.},
  file       = {:Stoerrle2009 - A Logical Model Query Interface _.pdf:PDF;:paper2(1).pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Stoerrle2007,
  author     = {Störrle, Harald},
  title      = {A {PROLOG}-based {Approach} to {Representing} and {Querying} {Software} {Engineering} {Models}.},
  year       = {2007},
  month      = jan,
  pages      = {71--83},
  file       = {:Stoerrle2007 - A PROLOG Based Approach to Representing and Querying Software Engineering Models. (1).pdf:PDF;:Stoerrle2007 - A PROLOG Based Approach to Representing and Querying Software Engineering Models..pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Noguera2012,
  author     = {Noguera, Carlos and De Roover, Coen and Kellens, Andy and Jonckers, Viviane},
  booktitle  = {2012 20th {IEEE} {International} {Conference} on {Program} {Comprehension} ({ICPC})},
  title      = {Code querying by {UML}},
  year       = {2012},
  month      = jun,
  note       = {ISSN: 1092-8138},
  pages      = {229--238},
  abstract   = {The need to identify source code that exhibits particular characteristics is essential to program comprehension. In this paper we introduce Arabica, a tool for querying Java code using UML class and sequence diagrams. Our use of UML diagrams avoids the need for developers to familiarize themselves with yet another language. In contrast to tools that rely on dedicated query languages, Arabica encodes querying semantics in a dedicated, minimal UML profile. Stereotyped class and sequence diagrams, characterizing structural and behavioral properties respectively, are translated into logic program queries. Using examples from the JHotDraw framework, we illustrate the utility of Arabica in validating design invariants, finding design pattern implementations and exploring extension points. We present a pre/post-test quasi experiment as a preliminary assessment of our approach.},
  doi        = {10.1109/ICPC.2012.6240492},
  file       = {:Noguera2012 - Code Querying by UML.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Unified modeling language, Java, Semantics, Syntactics, Concrete, Database languages, Mice, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.5555/519621.853360,
  author     = {Sim, Susan Elliott and Clarke, Charles L. A. and Holt, Richard C. and Cox, Anthony M.},
  booktitle  = {Proceedings of the IEEE International Conference on Software Maintenance},
  title      = {Browsing and Searching Software Architectures},
  year       = {1999},
  address    = {USA},
  pages      = {381},
  publisher  = {IEEE Computer Society},
  series     = {ICSM '99},
  abstract   = {Software architecture visualization tools tend to support browsing, that is, exploration
by following concepts. If architectural diagrams are to be used during daily software
maintenance tasks, these tools also need to support specific fact-finding through
searching. Searching is essential to program comprehension and hypothesis testing.
Furthermore, searching allows users to reverse the abstractions in architectural diagrams
and access facts in the underlying program code. In this paper, we consider the problem
of searching and browsing software architectures using perspectives from information
retrieval and program comprehension. After analyzing our own user studies and results
from the literature, we propose a solution: the Searchable Bookshelf, an architecture
visualization tool that supports both navigation styles. We also present a prototype
of our tool which is an extension of an existing architecture visualization tool.},
  file       = {:10.5555_519621.853360 - Browsing and Searching Software Architectures.pdf:PDF},
  isbn       = {0769500161},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Robitaille2000,
  author     = {{Robitaille} and {Schauer} and {Keller}},
  booktitle  = {Proceedings 2000 {International} {Conference} on {Software} {Maintenance}},
  title      = {Bridging program comprehension tools by design navigation},
  year       = {2000},
  month      = oct,
  note       = {ISSN: 1063-6773},
  pages      = {22--32},
  abstract   = {Source code investigation is one of the most time-consuming activities during software maintenance and evolution, yet currently-available tool support suffers from several shortcomings. Browsing is typically limited to low-level elements, investigation is only supported as a one-way activity, and tools provide little help in getting an encompassing picture of the system under examination. In our research, we have developed tool support for design navigation that addresses these shortcomings. A design browser allows for flexible browsing of a system's design-level representation and for information exchange with a suite of program comprehension tools. The browser is complemented with a retriever supporting full-text and structural searching. In this paper, we detail these tools and their integration into a reverse engineering environment, present three case studies and put them into perspective.},
  doi        = {10.1109/ICSM.2000.882972},
  file       = {:C\:/Users/Wernsen/Downloads/10.1.1.30.2812.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {Reverse engineering, skimmed},
  readstatus = {skimmed},
}

@Book{HeuzerothHollLoewe2001_18272001,
  author     = {Heuzeroth, Dirk and Holl, Thomas and Loewe, Welf},
  title      = {Combining static and dynamic analyses to detect interaction patterns},
  year       = {2001},
  note       = {Karlsruhe 2001. (Interner Bericht. Fakultät für Informatik, Universität Karlsruhe. 2001,21.)},
  file       = {:HeuzerothHollLoewe2001_18272001 - Combining Static and Dynamic Analyses to Detect Interaction Patterns.pdf:PDF},
  keywords   = {skimmed},
  language   = {english},
  readstatus = {skimmed},
}

@InProceedings{Jarzabek1995a,
  author     = {Jarzabek, S. and Keam, Tan Poh},
  booktitle  = {Proceedings of 2nd {Working} {Conference} on {Reverse} {Engineering}},
  title      = {Design of a generic reverse engineering assistant tool},
  year       = {1995},
  month      = jul,
  pages      = {61--70},
  abstract   = {Reverse engineering is a knowledge-intensive process. We believe the involvement of a domain expert is critical in any but a trivial reverse engineering task. Our approach to reverse engineering assumes close cooperation between a domain expert and a knowledge-based reverse engineering assistant tool. Reverse engineering progresses in steps. At each step, a tool applies heuristic rules to extract design views, while a domain expert accepts/rejects decisions made by a tool and provides additional input to tune in the reverse engineering process. In our projects, we reverse engineer to enhance program understanding and to facilitate software reengineering. We apply reverse engineering to variety of sources, produce many types of design views, use many design view presentation methods and, finally, deal with a rich, evolving set of reverse engineering heuristics. Therefore, we designed a flexible reverse engineering tool that can be easily customized to a reverse engineering task in hand. In particular, our tool (1) can be customized to work with different source languages, (2) contains an end-user facility to specify reverse engineering heuristics, to inspect design views extracted from source programs and to specify filters to tune in the reverse engineering process, and (3) can export design views to other tools for further processing.},
  doi        = {10.1109/WCRE.1995.514694},
  file       = {:Jarzabek1995a - Design of a Generic Reverse Engineering Assistant Tool.pdf:PDF},
  keywords   = {Reverse engineering, Programming profession, Maintenance engineering, Software systems, Documentation, Design optimization, Guidelines, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Canfora1996,
  author     = {Canfora, G. and Mancini, L. and Tortorella, M.},
  booktitle  = {4th {Workshop} on {Program} {Comprehension} {WPC} '96},
  title      = {A workbench for program comprehension during software maintenance},
  year       = {1996},
  month      = mar,
  note       = {ISSN: 1092-8138},
  pages      = {30--39},
  abstract   = {The paper describes the research carried out into the process of program comprehension during software maintenance within the EUREKA project REM (Reverse Engineering and Maintenance). Tools to aid maintenance programmers to achieve and document an overall interpretation of the system being maintained, as well as a deep understanding of the fine details of the source code, are presented. The cognition model assumed exploits both the top down and the bottom up approaches: program comprehension is intended as an iterative process of guessing, constructing hypotheses and verifying them This process is supported by providing maintenance programmers with a flexible system for querying source code and testing hypotheses against the evidence in the code. Several facilities generate new documents at the design and specification level, thus allowing maintenance programmers to record the knowledge gained for future use.},
  doi        = {10.1109/WPC.1996.501118},
  file       = {:Canfora1996 - A Workbench for Program Comprehension during Software Maintenance.pdf:PDF},
  issn       = {1092-8138},
  keywords   = {Software maintenance, Programming profession, Documentation, Reverse engineering, Costs, Proposals, Switches, Cognition, Iterative methods, System testing, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Ducasse2000,
  author     = {Ducasse, Stephane and Lanza, Michele and Tichelaar, Sander},
  booktitle  = {In {Proceedings} of the {Second} {International} {Symposium} on {Constructing} {Software} {Engineering} {Tools} (coset 2000},
  title      = {{MOOSE}: an {Extensible} {Language}-{Independent} {Environment} for {Reengineering} {Object}-{Oriented} {Systems}},
  year       = {2000},
  abstract   = {Surprising as it may seem, many of the early adopters of the object-oriented paradigm already face a number of problems typically encountered in large-scale legacy systems. The reengineering of those systems often poses problems because of the considerable size and complexity of such systems. In the context of the FAMOOS project we have developed a language independent environment called Moose which can deal with that complexity. This paper describes the architecture of Moose, the tools which have been developed around it and the industrial experiences we have obtained.},
  file       = {:Ducasse2000 - MOOSE_ an Extensible Language Independent Environment for Reengineering Object Oriented Systems.pdf:PDF;:Duca00bMooseCoset(1).pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
  shorttitle = {{MOOSE}},
}

@InProceedings{10.5555/872023.872561,
  author     = {Albin-Amiot, Herv\'{e} and Cointe, Pierre and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Jussien, Narendra},
  booktitle  = {Proceedings of the 16th IEEE International Conference on Automated Software Engineering},
  title      = {Instantiating and Detecting Design Patterns: Putting Bits and Pieces Together},
  year       = {2001},
  address    = {USA},
  pages      = {166},
  publisher  = {IEEE Computer Society},
  series     = {ASE '01},
  abstract   = {Design patterns ease designing, understanding, andre-engineering software. Achieving
a well-designedpiece of software requires a deep nderstanding and agood practice of
design patterns. Understanding existing software relies on the ability to identify
architectural forms resulting of the implementation of designpatterns. Maintaining
software involves spotting placesthat can be improved by sing better design decisions,like
those advocated by design patterns. Nevertheless,there is a lack of tools automatizing
the use of designpatterns to achieve well-designed pieces of software, toidentify
recurrent architectural forms, and to maintainsoftware. In this paper, we present
a set of tools andtechniques to help OO software practitioners design,understand,
and re-engineer a piece of software,sing design-patterns. A first prototype tool,
PATTERNS-BOX, provides assistance in designing the architectureof a new piece of software,
while a second prototypetool, PTIDEJ, identifies design patterns used in an existing
one. These tools, in combination, support maintenance by highlighting defects in an
existing design,and by suggesting and applying corrections based onwidely-accepted
design patterns solutions.},
  file       = {:C\:/Users/Wernsen/Downloads/download(13).pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@InProceedings{Eichberg2004a,
  author     = {Eichberg, M. and Meizini, M. and Schafer, T. and Beringer, C. and Hamel, K.M.},
  booktitle  = {2004 {Australian} {Software} {Engineering} {Conference}. {Proceedings}.},
  title      = {Enforcing system-wide properties},
  year       = {2004},
  month      = apr,
  pages      = {158--167},
  abstract   = {Policy enforcement is a mechanism for ensuring that system components follow certain programming practices, comply with specified rules, and meet certain assumptions. Unfortunately, the most common mechanisms used today for policy enforcement are documentation, training, and code reviews. The fundamental problem is that these mechanisms are expensive, time-consuming, and still error-prone. To cope with this problem, we present IRC (Implementation Restriction Checker), an extensible framework for automatically enforcing system-wide policies or contracts. The framework is built on top of a platform for aspect-oriented programming at the level of Java byte-code instructions and is available as an eclipse plug-in as well as a standalone application. It includes a set of directly usable checkers and can be easily extended to implement new ones.},
  doi        = {10.1109/ASWEC.2004.1290468},
  file       = {:Eichberg2004a - Enforcing System Wide Properties.pdf:PDF},
  keywords   = {Java, Runtime, Testing, Best practices, Virtual machining, Containers, Computer science, Documentation, Contracts, Application software, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Mendonga2004,
  author     = {Mendonga, N.C. and Maia, P.H.M. and Fonseca, L.A. and Andrade, R.M.C.},
  booktitle  = {20th {IEEE} {International} {Conference} on {Software} {Maintenance}, 2004. {Proceedings}.},
  title      = {{RefaX}: a refactoring framework based on {XML}},
  year       = {2004},
  month      = sep,
  note       = {ISSN: 1063-6773},
  pages      = {147--156},
  abstract   = {Refactoring, i.e., the process of changing a software system to improve its internal quality and preserving its external behavior, is gaining increasing acceptance among software developers. Even though many refactoring tools are now available for a variety of programming languages, most of them are difficult to reuse, extend or even customize, mainly because they provide no uniform way of representing and manipulating source code information. This work presents a refactoring framework, called RefaX, which relies on open, XML-based source code models and processing standards to facilitate the development, customization and reuse of refactoring tools. In particular, RefaX makes it possible to develop refactoring tools that are independent of source code model, programming language and XML processing technology. The viability of the framework is illustrated through a refactoring prototype for Java.},
  doi        = {10.1109/ICSM.2004.1357799},
  file       = {:Mendonga2004 - RefaX_ a Refactoring Framework Based on XML.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {XML, Computer languages, Software maintenance, Java, Costs, Software systems, Software quality, Code standards, Programming, Data models, skimmed},
  readstatus = {skimmed},
  shorttitle = {{RefaX}},
}

@Article{Kiesel1995,
  author     = {Kiesel, Norbert and Schuerr, Andy and Westfechtel, Bernhard},
  journal    = {Information Systems},
  title      = {Gras, a graph-oriented (software) engineering database system},
  year       = {1995},
  issn       = {0306-4379},
  month      = mar,
  number     = {1},
  pages      = {21--51},
  volume     = {20},
  abstract   = {Modern software systems for application areas such as software engineering, CAD, or office automation are usually highly interactive and deal with rather complex object structures. For the realization of these systems a nonstandard database system is needed which is able to efficiently handle different types of coarse- and fine-grained objects (like documents and paragraphs), hierarchical and non-hierarchical relations between objects (like composition-links and cross-references), and finally attributes of rather different size (like chapter numbers and bitmaps). Furthermore, this database system should support incremental computation of derived data, undo/redo of data modifications, error recovery from system crashes, and version control mechanisms. In this paper, we describe the underlying data model and the functionality of GRAS, a database system which has been designed according to the requirements mentioned above. Furthermore, we motivate our central design decisions concerning its realization, and report on experiences and applications.},
  doi        = {10.1016/0306-4379(95)00002-L},
  file       = {:Kiesel1995 - Gras, a Graph Oriented (software) Engineering Database System.pdf:PDF},
  keywords   = {attributed graphs, graph rewriting systems, distribution, version control, derived data, software engineering environments, skimmed},
  language   = {en},
  readstatus = {skimmed},
  url        = {https://www.sciencedirect.com/science/article/pii/030643799500002L},
  urldate    = {2021-07-13},
}

@Article{Paakki2000,
  author     = {Paakki, Jukka and Karhinen, Anssi and Gustafsson, Juha and Nenonen, Lilli and Verkamo, A.},
  journal    = {Proceedings of the International Conference on Software: Theory and Practice (16th IFIP World Computer Congress)},
  title      = {Software {Metrics} by {Architectural} {Pattern} {Mining}},
  year       = {2000},
  month      = jan,
  abstract   = {A software architecture is the key artifact in software design, describing the main elements of a software system and their interrelationships. We present a method for automatically analyzing the quality of an architecture by searching for architectural and design patterns from it. In addition to approximating the quality of the design, the extracted patterns can also be used for predicting the quality of the actual system. The method is demonstrated by an industrial case over a complex telephone exchange software.},
  file       = {:Paakki2000 - Software Metrics by Architectural Pattern Mining.pdf:PDF;:ifip2000.pdf:PDF},
  keywords   = {skimmed},
  readstatus = {skimmed},
}

@Book{Kazman1998,
  author     = {Kazman, Rick and Carriere, S.Jeromy},
  title      = {View extraction and view fusion in architectural understanding},
  year       = {1998},
  isbn       = {9780818683770},
  month      = jul,
  abstract   = {When performing architectural analysis on legacy software systems, it is frequently necessary to extract the architecture of the system, because it has not been documented, or because its documentation is out of date. However, architectural information does not exist directly in the artifacts that we can extract. The architecture exists in abstractions; compositions of extracted information. Thus, extracted artifacts must be able to be flexibly aggregated and combined. We call this process “view refinement and fusion”. This paper presents a workbench for architectural extraction called Dali, and shows how Dali supports flexible extraction and fusion of architectural information. Its use is described through two extended examples of architectural reconstruction},
  doi        = {10.1109/ICSR.1998.685754},
  file       = {:Kazman1998 - View Extraction and View Fusion in Architectural Understanding.pdf:PDF;:C\:/Users/Wernsen/Downloads/download(14).pdf:PDF},
  journal    = {International Conference on Software Reuse},
  keywords   = {skimmed},
  pages      = {299},
  readstatus = {skimmed},
}

@InProceedings{Prete2010,
  author     = {Prete, Kyle and Rachatasumrit, Napol and Sudan, Nikita and Kim, Miryung},
  booktitle  = {2010 {IEEE} {International} {Conference} on {Software} {Maintenance}},
  title      = {Template-based reconstruction of complex refactorings},
  year       = {2010},
  month      = sep,
  note       = {ISSN: 1063-6773},
  pages      = {1--10},
  abstract   = {Knowing which types of refactoring occurred between two program versions can help programmers better understand code changes. Our survey of refactoring identification techniques found that existing techniques cannot easily identify complex refactorings, such as an replace conditional with polymorphism refactoring, which consist of a set of atomic refactorings. This paper presents REF-FINDER that identifies complex refactorings between two program versions using a template-based refactoring reconstruction approach - REF-FINDER expresses each refactoring type in terms of template logic rules and uses a logic programming engine to infer concrete refactoring instances. It currently supports sixty three refactoring types from Fowler's catalog, showing the most comprehensive coverage among existing techniques. The evaluation using code examples from Fowler's catalog and open source project histories shows that REF-FINDER identifies refactorings with an overall precision of 0.79 and recall of 0.95.},
  doi        = {10.1109/ICSM.2010.5609577},
  file       = {:Prete2010 - Template Based Reconstruction of Complex Refactorings.pdf:PDF},
  issn       = {1063-6773},
  keywords   = {Concrete, Databases, Catalogs, Cloning, Feature extraction, Software, Algorithm design and analysis, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Tourwe2003,
  author     = {Tourwe, T. and Mens, T.},
  booktitle  = {Seventh {European} {Conference} {onSoftware} {Maintenance} and {Reengineering}, 2003. {Proceedings}.},
  title      = {Identifying refactoring opportunities using logic meta programming},
  year       = {2003},
  month      = mar,
  note       = {ISSN: 1534-5351},
  pages      = {91--100},
  abstract   = {We show how automated support can be provided for identifying refactoring opportunities, e.g., when an application's design should be refactored and which refactoring(s) in particular should be applied. Such support is achieved by using the technique of logic meta programming to detect so-called bad smells and by defining a framework that uses this information to propose adequate refactorings. We report on some initial but promising experiments that were applied using the proposed techniques.},
  doi        = {10.1109/CSMR.2003.1192416},
  issn       = {1534-5351},
  keywords   = {Logic programming, Guidelines, Degradation, Automatic programming, Aging, Computer languages, Phase detection, Software performance, Application software, Inspection, skimmed},
  readstatus = {skimmed},
}

@InProceedings{Mamas2000,
  author     = {Mamas, E. and Kontogiannis, K.},
  booktitle  = {Proceedings {Seventh} {Working} {Conference} on {Reverse} {Engineering}},
  title      = {Towards portable source code representations using {XML}},
  year       = {2000},
  month      = nov,
  note       = {ISSN: 1095-1350},
  pages      = {172--182},
  abstract   = {One of the most important issues in source code analysis and software re-engineering is the representation of software code text at an abstraction level and form suitable for algorithmic processing. However, source code representation schemes must be compact, accessible by well defined application programming interfaces (APIs) and above all portable to different operating platforms and various CASE tools. This paper proposes a program representation technique that is based on language domain modes and the XML markup language. In this context, source code is represented as XML DOM trees that offer a higher level of openness and portability than custom-made tool specific abstract syntax trees. The DOM trees can be exchanged between tools in textual or binary form. Similarly the domain model allows for language entities to be associated with analysis services offered by various CASE tools, leading to an integrated software maintenance environment.},
  doi        = {10.1109/WCRE.2000.891464},
  file       = {:C\:/Users/Wernsen/Downloads/10.1.1.88.6173(1).pdf:PDF},
  issn       = {1095-1350},
  keywords   = {XML, Software maintenance, Computer aided software engineering, Software systems, Software algorithms, Information management, Project management, Application software, Markup languages, Pressing, skimmed},
  readstatus = {skimmed},
}

@InProceedings{10.1145/2591062.2591065,
  author     = {Deering, Tom and Kothari, Suresh and Sauceda, Jeremias and Mathews, Jon},
  booktitle  = {Companion Proceedings of the 36th International Conference on Software Engineering},
  title      = {Atlas: A New Way to Explore Software, Build Analysis Tools},
  year       = {2014},
  address    = {New York, NY, USA},
  pages      = {588–591},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE Companion 2014},
  abstract   = {Atlas is a new software analysis platform from EnSoft Corp. Atlas decouples the domain-specific
analysis goal from its underlying mechanism by splitting analysis into two distinct
phases. In the first phase, polynomial-time static analyzers index the software AST,
building a rich graph database. In the second phase, users can explore the graph directly
or run custom analysis scripts written using a convenient API. These features make
Atlas ideal for both interaction and automation. In this paper, we describe the motivation,
design, and use of Atlas. We present validation case studies, including the verification
of safe synchronization of the Linux kernel, and the detection of malware in Android
applications. Our ICSE 2014 demo explores the comprehension and malware detection
use cases. Video: http://youtu.be/cZOWlJ-IO0k},
  doi        = {10.1145/2591062.2591065},
  file       = {:2591062.2591065(1).pdf:PDF},
  isbn       = {9781450327688},
  keywords   = {Human-in-the-loop, Static analysis, Analysis platform, skimmed},
  location   = {Hyderabad, India},
  numpages   = {4},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/2591062.2591065},
}

@Comment{jabref-meta: databaseType:bibtex;}
